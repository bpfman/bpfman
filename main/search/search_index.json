{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Formerly know as <code>bpfd</code></p>"},{"location":"#bpfman-an-ebpf-manager","title":"bpfman: An eBPF Manager","text":"<p>bpfman operates as an eBPF manager, focusing on simplifying the deployment and administration of eBPF programs. Its notable features encompass:</p> <ul> <li>System Overview: Provides insights into how eBPF is utilized in your system.</li> <li>eBPF Program Loader: Includes a built-in program loader that supports program cooperation for XDP and TC programs, as well as deployment of eBPF programs from OCI images.</li> <li>eBPF Filesystem Management: Manages the eBPF filesystem, facilitating the deployment of eBPF applications without requiring additional privileges.</li> </ul> <p>Our program loader and eBPF filesystem manager ensure the secure deployment of eBPF applications. Furthermore, bpfman includes a Kubernetes operator, extending these capabilities to Kubernetes. This allows users to confidently deploy eBPF through custom resource definitions across nodes in a cluster.</p>"},{"location":"#why-ebpf","title":"Why eBPF?","text":"<p>eBPF is a powerful general-purpose framework that allows running sandboxed programs in the kernel. It can be used for many purposes, including networking, monitoring, tracing and security.</p>"},{"location":"#why-ebpf-in-kubernetes","title":"Why eBPF in Kubernetes?","text":"<p>Demand is increasing from both Kubernetes developers and users. Examples of eBPF in Kubernetes include:</p> <ul> <li>Cilium and Calico   CNIs</li> <li>Pixie: Open source observability</li> <li>KubeArmor: Container-aware runtime security   enforcement system</li> <li>Blixt: Gateway API L4 conformance   implementation</li> <li>NetObserv: Open source operator for network   observability</li> </ul>"},{"location":"#challenges-for-ebpf-in-kubernetes","title":"Challenges for eBPF in Kubernetes","text":"<ul> <li>Requires privileged pods.</li> <li>eBPF-enabled apps require at least CAP_BPF permissions and potentially     more depending on the type of program that is being attached.</li> <li>Since the Linux capabilities are very broad it is challenging to constrain     a pod to the minimum set of privileges required. This can allow them to do     damage (either unintentionally or intentionally).</li> <li>Handling multiple eBPF programs on the same eBPF hooks.</li> <li>Not all eBPF hooks are designed to support multiple programs.</li> <li>Some software using eBPF assumes exclusive use of an eBPF hook and can     unintentionally eject existing programs when being attached. This can     result in silent failures and non-deterministic failures.</li> <li>Debugging problems with deployments is hard.</li> <li>The cluster administrator may not be aware that eBPF programs are being     used in a cluster.</li> <li>It is possible for some eBPF programs to interfere with others in     unpredictable ways.</li> <li>SSH access or a privileged pod is necessary to determine the state of eBPF     programs on each node in the cluster.</li> <li>Lifecycle management of eBPF programs.</li> <li>While there are libraries for the basic loading and unloading of eBPF     programs, a lot of code is often needed around them for lifecycle     management.</li> <li>Deployment on Kubernetes is not simple.</li> <li>It is an involved process that requires first writing a daemon that loads     your eBPF bytecode and deploying it using a DaemonSet.</li> <li>This requires careful design and intricate knowledge of the eBPF program     lifecycle to ensure your program stays loaded and that you can easily     tolerate pod restarts and upgrades.</li> <li>In eBPF enabled K8s deployments today, the eBPF Program is often embedded     into the userspace binary that loads and interacts with it. This means     there's no easy way to have fine-grained versioning control of the     bpfProgram in relation to it's accompanying userspace counterpart.</li> </ul>"},{"location":"#what-is-bpfman","title":"What is bpfman?","text":"<p>bpfman is a software stack that aims to make it easy to load, unload, modify and monitor eBPF programs whether on a single host, or in a Kubernetes cluster. bpfman includes the following core components:</p> <ul> <li>bpfman: A system daemon that supports loading, unloading, modifying and   monitoring of eBPF programs exposed over a gRPC API.</li> <li>eBPF CRDS: bpfman provides a set of CRDs (<code>XdpProgram</code>, <code>TcProgram</code>, etc.) that   provide a way to express intent to load eBPF programs as well as a bpfman   generated CRD (<code>BpfProgram</code>) used to represent the runtime state of loaded   programs.</li> <li>bpfman-agent: The agent runs in a container in the bpfman daemonset and ensures   that the requested eBPF programs for a given node are in the desired state.</li> <li>bpfman-operator: An operator, built using Operator   SDK, that manages the installation and   lifecycle of bpfman-agent and the CRDs in a Kubernetes cluster.</li> </ul> <p>bpfman is developed in Rust and built on top of Aya, a Rust eBPF library.</p> <p>The benefits of this solution include the following:</p> <ul> <li>Security</li> <li>Improved security because only the bpfman daemon, which can be tightly     controlled, has the privileges needed to load eBPF programs, while access     to the API can be controlled via standard RBAC methods. Within bpfman, only     a single thread keeps these capabilities while the other threads (serving     RPCs) do not.</li> <li>Gives the administrators control over who can load programs.</li> <li>Allows administrators to define rules for the ordering of networking eBPF     programs. (ROADMAP)</li> <li>Visibility/Debuggability</li> <li>Improved visibility into what eBPF programs are running on a system, which     enhances the debuggability for developers, administrators, and customer     support.</li> <li>The greatest benefit is achieved when all apps use bpfman, but even if they     don't, bpfman can provide visibility into all the eBPF programs loaded on     the nodes in a cluster.</li> <li>Multi-program Support</li> <li>Support for the coexistence of multiple eBPF programs from multiple users.</li> <li>Uses the libxdp multiprog     protocol     to allow multiple XDP programs on single interface</li> <li>This same protocol is also supported for TC programs to provide a common     multi-program user experience across both TC and XDP.</li> <li>Productivity</li> <li>Simplifies the deployment and lifecycle management of eBPF programs in a     Kubernetes cluster.</li> <li>developers can stop worrying about program lifecycle (loading, attaching,     pin management, etc.) and use existing eBPF libraries to interact with     their program maps using well defined pin points which are managed by     bpfman.</li> <li>Developers can still use Cilium/libbpf/Aya/etc libraries for eBPF     development, and load/unload with bpfman.</li> <li>Provides eBPF Bytecode Image Specifications that allows fine-grained     separate versioning control for userspace and kernelspace programs. This     also allows for signing these container images to verify bytecode     ownership.</li> </ul> <p>For more details, please see the following:</p> <ul> <li>bpfman Overview for an overview of bpfman.</li> <li>Deploying Example eBPF Programs On Local Host   for some examples of running <code>bpfman</code> on local host and using the CLI to install   eBPF programs on the host.</li> <li>Deploying Example eBPF Programs On Kubernetes   for some examples of deploying eBPF programs through <code>bpfman</code> in a Kubernetes deployment.</li> <li>Setup and Building bpfman for instructions   on setting up your development environment and building bpfman.</li> <li>Example eBPF Programs for some examples of   eBPF programs written in Go, interacting with <code>bpfman</code>.</li> <li>Deploying the bpfman-operator for   details on launching bpfman in a Kubernetes cluster.</li> <li>Meet the Community for details on community   meeting details.</li> </ul>"},{"location":"blog/","title":"Bpfman Blog","text":""},{"location":"blog/2023/11/25/a-new-logo-using-generative-ai-of-course/","title":"A New Logo: Using Generative AI, of course","text":"<p>Since we renamed the project to <code>bpfman</code> we are in need of a new logo. Given that the tech buzz around Generative AI is infectious, we decided to explore using generative AI to create our new logo. What we found was that it was a great way to generate ideas, but a human (me) was still needed to create the final design.</p>"},{"location":"blog/2023/11/25/a-new-logo-using-generative-ai-of-course/#the-brief","title":"The Brief","text":"<p>I have a love of open source projects with animal mascots, so bpfman should be no different. The \"bee\" is used a lot for eBPF related projects. One such example is Crabby, the crab/bee hybrid, that I created for the Aya project.</p> <p>The logo should be cute and playful, but not too childish. As a nod to Podman, we'd like to use the same typeface and split color-scheme as they do, replacing purple with yellow.</p> <p>One bee is not enough! Since we're an eBPF manager, we need a more bees!</p> <p>via GIPHY</p> <p>And since those bees are bee-ing (sorry) managed, they should be organized. Maybe in a pyramid shape?</p>"},{"location":"blog/2023/11/25/a-new-logo-using-generative-ai-of-course/#the-process","title":"The Process","text":"<p>We used Bing Image Creator, which is backed by DALL-E 3.</p> <p>Initially we tried to use the following prompt:</p> <p>Logo for open source software project called \"bpfman\". \"bpf\" should be yellow and \"man\" should be black or grey. an illustration of some organized bees above the text. cute. playful</p> <p>Our AI overlords came up with:</p> <p></p> <p>Not bad, but not quite what we were looking for. It's clear that as smart as AI is, it struggles with text, so whatever we need will need some manual post-processing. There are bees, if you squint a bit, but they're not very organized. Let's refine our prompt a bit:</p> <p>Logo for open source software project called \"bpfman\" as one word. The \"bpf\" should be yellow and \"man\" should be black or grey. an illustration of some organized bees above the text. cute. playful.</p> <p></p> <p>That... is worse.</p> <p>Let's try again:</p> <p>Logo for a project called \"bpfman\". In the text \"bpfman\", \"bpf\" should be yellow and \"man\" should be black or grey. add an illustration of some organized bees above the text. cute and playful style.</p> <p></p> <p>The bottom left one is pretty good! So I shared it with the rest of the maintainers to see what they thought.</p> <p>At this point the feedback that I got was the bees were too cute! We're a manager, and managers are serious business, so we need serious bees.</p> <p>Prompting the AI for the whole logo was far too ambitious, so I decided I would just use the AI to generate the bees and then I would add the text myself.</p> <p>I tried a few different prompts, but the one that worked best was:</p> <p>3 bees guarding a hive. stern expressions. simple vector style.</p> <p></p> <p>The bottom right was exactly what I had in mind! With a little bit of post-processing, I ended up with this:</p> <p></p> <p>Now it was time to solicit some feedback.</p>"},{"location":"blog/2023/11/25/a-new-logo-using-generative-ai-of-course/#gathering-feedback","title":"Gathering Feedback","text":"<p>After showing the logo to a few others, we decided that the bees were infact too stern. At this point we had a few options, like reverting back to our cute bees, however, this section in the [Bing Image Creator Terms of Service] was pointed out to me:</p> <p>Use of Creations. Subject to your compliance with this Agreement, the Microsoft Services Agreement, and our Content Policy, you may use Creations outside of the Online Services for any legal personal, non-commercial purpose.</p> <p>This means that we can't use the AI generated images for our logo.</p>"},{"location":"blog/2023/11/25/a-new-logo-using-generative-ai-of-course/#was-it-all-for-nothing","title":"Was it all for nothing?","text":"<p>Was it all for nothing? No! We learnt a lot from this process.</p> <p>Generative AI is great for generating ideas. Some of the logo compositions produced were great!</p> <p>It was also very useful to adjust the prompt based on feedback from team members so we could incorporate their ideas into the design.</p> <p>We also learnt that the AI is not great at text, so we should avoid using it for that.</p> <p>And finally, we learnt that we can't use the AI generated images for our logo. Well, not with the generator we used anyway.</p>"},{"location":"blog/2023/11/25/a-new-logo-using-generative-ai-of-course/#the-semi-final-design-process","title":"The (Semi) Final Design Process","text":"<p>I started from scratch, taking inspiration from the AI generated images. The bees were drawn first and composed around a hive - as our AI overlords suggested. I then added the text, and colours, but it still felt like it was missing something.</p> <p>What if we added a force field around the hive? That might be cool! And so, I added a force field around the hive and played around with the colours until I was happy.</p> <p>Here's what we ended up with:</p> <p></p> <p>We consulted a few more people and got some feedback. The general consensus was that the logo was too busy... However, the reception to the force field was that the favicon I'd mocked would work better as the logo.</p>"},{"location":"blog/2023/11/25/a-new-logo-using-generative-ai-of-course/#the-final-design","title":"The Final Design","text":"<p>Here's the final design:</p> <p></p> <p>Pretty cool, right? Even if I do say so myself.</p> <p>Our mascot is a queen bee, because she's the manager of the hive.</p> <p>The force field, is now no longer a force field - It's a pheramone cloud that represents the Queen Mandibular Pheromone (QMP) that the queen bee produces to keep the hive organized.</p>"},{"location":"blog/2023/11/25/a-new-logo-using-generative-ai-of-course/#conclusion","title":"Conclusion","text":"<p>I'm really happy with the result! I'm not a designer, so I'm sure there are things that could be improved, but I think it's a good start.</p> <p>What do you think? Join us on Slack and let us know!</p>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/","title":"bpfman's Integration with the AF_XDP Device Plugin and CNI for Kubernetes","text":"<p>AF_XDP is an address/socket family that is optimized for high performance packet processing. It takes advantage of XDP (an in Kernel fastpath), which essentially runs an eBPF program as early as possible on a network driver's receive path, and redirects the packet to an AF_XDP socket.</p> <p></p> <p>AF_XDP sockets (XSKs) are created in Userspace and have a 1:1 mapping with netdev queues. An XSKMAP is an eBPF map of AF_XDP sockets for a particular netdev. It's a simple key:value map where the key is the netdev's queue-id and the value is the AF_XDP socket that's attached to that queue. The eBPF program (at the XDP hook) will leverage the XSKMAP and the XDP_REDIRECT action to redirect packets to an AF_XDP socket. In the image below the XDP program is redirecting an incoming packet to the XSK attached to Queue 2.</p> <p>NOTE: If no XSK is attached to a queue, the XDP program will simply pass the packet to the Kernel Network Stack.</p> <pre><code>+---------------------------------------------------+\n|     XSK A      |     XSK B       |      XSK C     |&lt;---+  Userspace\n=========================================================|==========\n|    Queue 0     |     Queue 1     |     Queue 2    |    |  Kernel space\n+---------------------------------------------------+    |\n|                  Netdev eth0                      |    |\n+---------------------------------------------------+    |\n|                            +=============+        |    |\n|                            | key |  xsk  |        |    |\n|  +---------+               +=============+        |    |\n|  |         |               |  0  | xsk A |        |    |\n|  |         |               +-------------+        |    |\n|  |         |               |  1  | xsk B |        |    |\n|  | BPF     |               +-------------+        |    |\n|  | prog    |-- redirect --&gt;|  2  | xsk C |-------------+\n|  | (XDP    |               +-------------+        |\n|  |  HOOK)  |                   xskmap             |\n|  |         |                                      |\n|  +---------+                                      |\n|                                                   |\n+---------------------------------------------------+\n</code></pre> <p>The AF_XDP Device Plugin and CNI project provides the Kubernetes components to provision, advertise and manage AF_XDP networking devices for Kubernetes pods. These networking devices are typically used as a Secondary networking interface for a pod. A key goal of this project is to enable pods to run without any special privileges, without it pods that wish to use AF_XDP will need to run with elevated privileges in order to manage the eBPF program on the interface. The infrastructure will have little to no control over what these pods can load. Therefore it's ideal to leverage a central/infrastructure centric eBPF program management approach.  This blog will discuss the eBPF program management journey for the AF_XDP Device Plugin and CNI.</p>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#what-does-the-af_xdp-device-plugin-and-cni-do","title":"What does the AF_XDP Device Plugin and CNI do?","text":"<p>For pods to create and use AF_XDP sockets on their interfaces, they can either:</p> <ol> <li>Create the AF_XDP socket on an interface already plumbed to the Pod (via SR-IOV    Device Plugin and the Host CNI) --&gt; But this requires CAP_BPF or CAP_SYS_ADMIN    privileges in order to load the BPF program on the netdev.</li> </ol> <p>OR</p> <ol> <li> <p>Use the AF_XDP Device Plugin (DP) and CNI in order to support a Pod without the    aforementioned root like privileges.</p> <p>NOTE: Prior to kernel 5.19, all BPF sys calls required CAP_BPF, which are used to access maps shared between the BPF program and the userspace program. In kernel 5.19, a change went in that only requires CAP_BPF for map creation (BPF_MAP_CREATE) and loading programs (BPF_PROG_LOAD).</p> <p>In this scenario, the <code>AF_XDP DP</code>, will advertise resource pools (of netdevs) to <code>Kubelet</code>. When a Pod requests a resource from these pools, <code>Kubelet</code> will <code>Allocate()</code> one of these devices through the <code>AF_XDP DP</code>. The <code>AF_XDP DP</code> will load the eBPF program (to redirect packets to an AF_XDP socket) on the allocated device.</p> <p>The default behaviour of the <code>AF_XDP DP</code> (unless otherwise configured) is to take note of the XSKMAP File Descriptor (FD) for that netdev. It will also mount a Unix Domain Socket (UDS), as a hostpath mount, in the Pod. This UDS will be used by the AF_XDP application to perform a handshake with the <code>AF_XDP DP</code> to retrieve the XSKMAP FD. The application needs the XSKMAP FD to \"attach\" AF_XDP sockets it creates to the netdev queues.</p> <p>NOTE: Newer versions of the <code>AF_XDP DP</code> support eBPF map pinning which eliminate the need to perform this (non trivial) handshake with AF_XDP pods. It now mounts the pinned XSKMAP into the Pod using a hostpath mount. The downside of this approach is that the <code>AF_XDP DP</code> now needs to manage several eBPF File Systems (BPFFS), one per pod.</p> <p>The <code>AF_XDP CNI</code> (like any CNI) has the task of moving the netdev (with the loaded eBPF program) into the Pod namespace. It also does a few other important things:</p> <ul> <li>It does not rename the netdev (to allow the DP to avoid IF_INDEX clashes as it manages      the AF_XDP resource pools).</li> <li>The CNI is also capable of configuring hardware filters on the NIC.</li> <li>Finally, the CNI also unloads the eBPF program from the netdev and clear any hardware     filters when the Pod is terminated.</li> </ul> <p>NOTE 1: The <code>AF_XDP CNI</code> manages the unloading of the eBPF program due to the <code>AF_XDP DP</code> not being aware of when a pod terminates (it's only invoked by <code>Kubelet</code> during pod creation).</p> <p>NOTE 2: Prior to bpfman integration, the CNI was extended to signal the AF_XDP DP on pod termination (via gRPC) in an effort to support eBPF map pinning directly in the AF_XDP DP. The AF_XDP DP was managing BPFFS(es) for map pinning and needed to be signalled to clean them up.</p> </li> </ol>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#bpfman-integration","title":"bpfman Integration","text":"<p>Prior to bpfman integration the AF_XDP Device Plugin and CNI managed the eBPF program for redirecting incoming packets to AF_XDP sockets, its associated map (XSKMAP), and/or several BPFFS.</p>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#integration-benefits","title":"Integration benefits","text":"<p>So what are the benefits of bpfman integration for the AF_XDP DP and CNI?</p> <ul> <li> <p>Removes code for loading and managing eBPF from the AF_XDP DP and CNI codebase.</p> </li> <li> <p>This presented a difficulty particularly when trying to find/update appropriate     base container images to use for the AF_XDP device plugin. Different images     supported different versions of eBPF management libraries (i.e libbpf or libxdp) which     forced multiple changes around the loading and attaching of the base eBPF program.</p> </li> <li> <p>Additionally the CNI runs as a binary on the Kubernetes node so we would need to     statically compile libbpf/libxdp as part of the CNI.</p> </li> <li> <p>More diverse XDP program support through bpfman's eBPF Bytecode Image Specification. Not   only do the AF_XDP eBPF programs no longer need to be stored in the Device Plugin   itself, but it's now configurable on a per pool basis.</p> </li> <li> <p>No longer required to leverage Hostpath volume mounts to mount the AF_XDP maps inside   a Pod. But rather take advantage of the bpfman CSI support to ensure that maps are   pinned in the context of the Pod itself and not in a BPFFS on the host (then shared   to the Pod).</p> </li> </ul>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#af_xdp-device-plugin-ebpf-programmap-management","title":"AF_XDP Device Plugin eBPF program/map management","text":"<p>The role of the <code>AF_XDP DP</code> in eBPF program/map management prior to bpfman integration:</p> <ul> <li> <p>Loads the default AF_XDP BPF prog onto the netdev at Pod creation and manages info regarding the XSKMAP for that netdev.</p> </li> <li> <p>Mounts a UDS as a hostpath volume in the Pod OR creates a BPFFS per netdev and pins the   XSKMAP to it, then mounts this BPFFS as a hostpath volume in the Pod.</p> </li> <li> <p>Shares the XSKMAP file descriptor via UDS (involves a handshake with the Pod).</p> </li> </ul> <p>The role of the <code>AF_XDP DP</code> in eBPF program/map management after bpfman integration:</p> <ul> <li> <p>Uses bpfman's client APIs to load the BPF prog.</p> </li> <li> <p>Shares the XSKMAP (that bpfman pinned ) with the Pod as a hostpath volume.</p> </li> </ul>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#af_xdp-cni-ebpf-programmap-management","title":"AF_XDP CNI eBPF program/map management","text":"<p>The role of the <code>AF_XDP CNI</code> in eBPF program/map management prior to bpfman integration:</p> <ul> <li>Unloads the eBPF program when a device is returned to the Host network namespace.</li> </ul> <p>The role of the <code>AF_XDP CNI</code> in eBPF program/map management after bpfman integration:</p> <ul> <li>Uses gRPC to signal to the Device Plugin to request bpfman to unload the eBPF program   using the client APIs.</li> </ul>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#is-there-a-working-example","title":"Is there a working example?","text":"<p>The bpfman integration with the AF_XDP Device Plugin and CNI was demo'ed as part of a series of demos that show the migration of a DPDK application to AF_XDP (without) any application modification. The demo can be watched below:</p> <p></p>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#af_xdp-dp-and-cnis-integration-with-bpfman-in-images","title":"AF_XDP DP and CNI's integration with bpfman in images","text":"<p>The following sections will present the evolution of the AF_XDP DP and CNI from independent eBPF program management to leveraging bpfman to manage eBPF programs on their behalf.</p>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#af_xdp-dp-and-cni-managing-ebpf-programs-independently","title":"AF_XDP DP and CNI managing eBPF programs independently","text":"<p>The following diagram details how the AF_XDP DP and CNI worked prior to bpfman integration.</p> <p></p> <ol> <li> <p>Setup Subfunctions on the network devices (if the are supported/being used).</p> </li> <li> <p>Create an AF_XDP DP and CNI configuration file to setup the device resource pools and    deploy the DP and CNI.</p> </li> <li> <p>When the AF_XDP DP runs it will discover the netdevs on the host and create the resource pools.</p> </li> <li> <p>The AF_XDP DP registers the resource pools with Kubelet.</p> </li> <li> <p>When a pod (that requests an AF_XDP resource) is started, Kubelet will send an <code>Allocate()</code>    request to the AF_XDP DP. The AF_XDP DP loads the eBPF program on the interface and mounts the    UDS in the pod and sets some environment variables in the pod using the Downward API.</p> </li> </ol> <p>NOTE: In the case where eBPF map pinning is used rather than the UDS, the AF_XDP    DP will create a BPFFS where it pins the XSKMAP and mounts the BPFFS as a hostpath volume    in the pod.</p> <ol> <li> <p>The AF_XDP DP signals success to the Kubelet so that the device is added to the pod.</p> </li> <li> <p>Kubelet triggers multus, which in turn triggers the AF_XDP CNI. The CNI does the relevant network    configuration and moves the netdev into the pod network namespace.</p> </li> <li> <p>The application in the pod start and initiates a handshake with the AF_XDP DP over the mounted UDS    to retrieve the XSKMAP FD.</p> </li> </ol>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#af_xdp-dp-and-cni-integrated-with-bpfman-no-csi","title":"AF_XDP DP and CNI integrated with bpfman (no csi)","text":"<p>The following diagram details how the AF_XDP DP and CNI worked after bpfman integration.</p> <p></p> <p>The main difference here is that when the <code>Allocate()</code> request comes in from Kubelet, the AF_XDP DP uses the bpfman client API to load the eBPF program on the relevant netdev. It takes note of where bpfman pins the XSKMAP and mounts this directory as a hostpath volume in the pod.</p>"},{"location":"blog/2024/02/27/bpfmans-integration-with-the-af_xdp-device-plugin-and-cni-for-kubernetes/#af_xdp-dp-and-cni-integrated-with-bpfman-with-csi","title":"AF_XDP DP and CNI integrated with bpfman (with csi)","text":"<p>The following diagram details how the AF_XDP DP and CNI will work with bpfman leveraging the new CSI implementation.</p> <p></p> <p>The pod will include a volume definition as follows:</p> <pre><code>   volumes:\n   - name: bpf-maps\n     csi:\n       driver: csi.bpfman.dev\n       volumeAttributes:\n         csi.bpfman.dev/thru-annotations: true\n</code></pre> <p>The idea here is when the <code>Allocate()</code> request comes in from Kubelet, the AF_XDP DP uses the bpfman client API to load the eBPF program on the relevant netdev. The AF_XDP DP will annotate the pod with the XdpProgram name, map and mountpath. When the bpfman CSI plugin is triggered by Kubelet, it will retrieve the information it needs from the pod annotations in order to pin the map inside the Pod.</p>"},{"location":"blog/2023/11/23/bpfd-becomes-bpfman/","title":"bpfd becomes bpfman","text":"<p>Bpfd is now bpfman! We've renamed the project to better reflect the direction we're taking. We're still the same project, just with a new name.</p>"},{"location":"blog/2023/11/23/bpfd-becomes-bpfman/#why-the-name-change","title":"Why the name change?","text":"<p>We've been using the name <code>bpfd</code> for a while now, but we were not the first to use it. There were projects before us that used the name <code>bpfd</code>, but since most were inactive, originally we didn't see this as an issue.</p> <p>More recently though the folks at Meta have started using the name <code>systemd-bpfd</code> for their proposed addition to systemd.</p> <p>In addition, we've been thinking about the future of the project, and particularly about security and whether it's wise to keep something with <code>CAP_BPF</code> capabilities running as a daemon - even if we've been very careful. This is similar to the issues faced by docker which eventually lead to the creation of podman.</p> <p>This issue led us down the path of redesigning the project to be daemonless. We'll be implementing these changes in the coming months and plan to perform our first release as <code>bpfman</code> in Q1 of 2024.</p> <p>The 'd' in <code>bpfd</code> stood for daemon, so with our new design and the confusion surrounding the name <code>bpfd</code> we though it was time for a change.</p> <p>Since we're a BPF manager, we're now bpfman! It's also a nice homage to podman, which we're big fans of.</p>"},{"location":"blog/2023/11/23/bpfd-becomes-bpfman/#what-does-this-mean-for-me","title":"What does this mean for me?","text":"<p>If you're a developer of <code>bpfman</code> you will need to update your Git remotes to point at our new organization and repository name. Github will redirect these for a while, but we recommend updating your remotes as soon as possible.</p> <p>If you're a user of <code>bpfd</code> or the <code>bpfd-operator</code> then version 0.3.1 will be the last release under the <code>bpfd</code> name. We will continue to support you as best we can, but we recommend upgrading to <code>bpfman</code> as soon as our first release is available.</p>"},{"location":"blog/2023/11/23/bpfd-becomes-bpfman/#whats-next","title":"What's next?","text":"<p>We've hinted at some of the changes we're planning, and of course, our roadmap is always available in Github. It's worth mentioning that we're also planning to expand our release packages to include RPMs and DEBs, making it even easier to install <code>bpfman</code> on your favorite Linux distribution.</p>"},{"location":"blog/2023/11/23/bpfd-becomes-bpfman/#thanks","title":"Thanks!","text":"<p>We'd like to thank everyone who has contributed to <code>bpfd</code> over the years. We're excited about the future of <code>bpfman</code> and we hope you are too! Please bear with us as we make this transition, and if you have any questions or concerns, please reach out to us on Slack. We're in the '#bpfd' channel, but we'll be changing that to '#bpfman' soon.</p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/","title":"Technical Challenges for Attaching eBPF Programs in Containers","text":"<p>We recently added support for attaching uprobes inside containers. The purpose of this blog is to give a brief overview of the feature, to document the technical challenges encountered, and describe our solutions for those challenges. In particular, how to attach an eBPF program inside of a container, and how to find the host Process ID (PID) on the node for the container?</p> <p>The solutions seem relatively straightforward now that they are done, but we found limited information elsewhere, so we thought it would be helpful to document them here.</p> <p>The uprobe implementation will be used as the example in this blog, but the concepts can (and will eventually) be applied to other program types.</p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#introduction","title":"Introduction","text":"<p>A \"uprobe\" (user probe) is a type of eBPF program that can be attached to a specific location in a user-space application. This allows developers and system administrators to dynamically instrument a user-space binary to inspect its behavior, measure performance, or debug issues without modifying the application's source code or binary. When the program execution reaches the location to which the uprobe is attached, the eBPF program associated with the uprobe is executed.</p> <p>bpfman support for uprobes has existed for some time.  We recently extended this support to allow users to attach uprobes inside of containers both in the general case of a container running on a Linux server and also for containers running in a Kubernetes cluster.</p> <p>The following is a bpfman command line example for loading a uprobe inside a container:</p> <pre><code>bpfman load image --image-url quay.io/bpfman-bytecode/uprobe:latest uprobe --fn-name \"malloc\" --target \"libc\" --container-pid 102745\n</code></pre> <p>The above command instructs bpfman to attach a uprobe to the <code>malloc</code> function in the <code>libc</code> library for the container with PID 102745. The main addition here is the ability to specify a <code>container-pid</code>, which is the PID of the container as it is known to the host server.</p> <p>The term \"target\" as used in the above bpfman command (and the CRD below) describes the library or executable that we want to attach the uprobe to.  The fn-name (the name of the function within that target) and/or an explicit \"offset\" can be used to identify a specific offset from the beginning of the target.  We also use the term \"target\" more generally to describe the intended location of the uprobe.</p> <p>For Kubernetes, the CRD has been extended to include a \"container selector\" to describe one or more containers as shown in the following example.</p> <pre><code>apiVersion: bpfman.io/v1alpha1\nkind: UprobeProgram\nmetadata:\n  labels:\n    app.kubernetes.io/name: uprobeprogram\n  name: uprobe-example-containers\nspec:\n  # Select all nodes\n  nodeselector: {}\n  bpffunctionname: my_uprobe\n  func_name: malloc\n  # offset: 0 # optional offset w/in function\n  target: libc\n  retprobe: false\n  # pid: 0 # optional pid to execute uprobe for\n  bytecode:\n    image:\n      url: quay.io/bpfman-bytecode/uprobe:latest\n  containers:      &lt;=== New section for specifying containers to attach uprobe to\n    namespace: bpfman\n    pods:\n      matchLabels:\n        name: bpfman-daemon\n    containernames:\n      - bpfman\n      - bpfman-agent\n</code></pre> <p>In the Kubernetes case, the container selector (<code>containers</code>) is used to identify one or more containers in which to attach the uprobe. If <code>containers</code> identifies any containers on a given node, the bpfman agent on that node will determine their host PIDs and make the calls to bpfman to attach the uprobes.</p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#attaching-uprobes-in-containers","title":"Attaching uprobes in containers","text":"<p>A Linux \"mount namespace\" is a feature that isolates the mount points seen by a group of processes. This means that processes in different mount namespaces can have different views of the filesystem.  A container typically has its own mount namespace that is isolated both from those of other containers and its parent. Because of this, files that are visible in one container are likely not visible to other containers or even to the parent host (at least not directly). To attach a uprobe to a file in a container, we need to have access to that container's mount namespace so we can see the file to which the uprobe needs to be attached.</p> <p>From a high level, attaching a uprobe to an executable or library in a container is relatively straight forward. <code>bpfman</code> needs to change to the mount namespace of the container, attach the uprobe to the target in that container, and then return to our own mount namespace so that we can save the needed state and continue processing other requests.</p> <p>The main challenges are:</p> <ol> <li>Changing to the mount namespace of the target container.</li> <li>Returning to the bpfman mount namespace.</li> <li><code>setns</code> (at least for the mount namespace) can't be called from a    multi-threaded application, and bpfman is currently multithreaded.</li> <li>How to find the right PID for the target container.</li> </ol>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#the-mount-namespace","title":"The Mount Namespace","text":"<p>To enter the container namespace, <code>bpfman</code> uses the sched::setns function from the Rust nix crate. The <code>setns</code> function requires the file descriptor for the mount namespace of the target container.</p> <p>For a given container PID, the namespace file needed by the <code>setns</code> function can be found in the <code>/proc/&lt;PID&gt;/ns/</code> directory. An example listing for the PID 102745 directory is shown below:</p> <pre><code>sudo ls -l /proc/102745/ns/\ntotal 0\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 cgroup -&gt; 'cgroup:[4026531835]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 ipc -&gt; 'ipc:[4026532858]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 mnt -&gt; 'mnt:[4026532856]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:07 net -&gt; 'net:[4026532860]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 pid -&gt; 'pid:[4026532859]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 pid_for_children -&gt; 'pid:[4026532859]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 time -&gt; 'time:[4026531834]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 time_for_children -&gt; 'time:[4026531834]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 user -&gt; 'user:[4026531837]'\nlrwxrwxrwx 1 root root 0 Feb 15 12:10 uts -&gt; 'uts:[4026532857]'\n</code></pre> <p>In this case, the mount namespace file is <code>/proc/102745/ns/mnt</code>. </p> <p>NOTE: How to find the PID and the relationship between parent and child PIDs is described in the \"Finding The PID\" section below.</p> <p>When running directly on a Linux server, <code>bpfman</code> has access to the host <code>/proc</code> directory and can access the mount namespace file for any PID.  However, on Kubernetes, <code>bpfman</code> runs in a container, so it doesn't have access to the namespace files of other containers or the <code>/proc</code> directory of the host by default. Therefore, in the Kubernetes implementation, <code>/proc</code> is mounted in the <code>bpfman</code> container so it has access to the ns directories of other containers. </p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#returning-to-the-bpfman-mount-namespace","title":"Returning to the <code>bpfman</code> Mount Namespace","text":"<p>After <code>bpfman</code> does a <code>setns</code> to the target container mount namespace, it has access to the target binary in that container.  However, it only has access to that container's view of the filesystem, and in most cases, this does not include access to bpfman's filesystem or the host filesystem.  As a result, bpfman loses the ability to access its own mount namespace file.</p> <p>However, before calling setns, <code>bpfman</code> has access to it's own mount namespace file.  Therefore, to avoid getting stranded in a different mount namespace, <code>bpfman</code> also opens its own mount namespace file prior to calling <code>setns</code> so it already has the file descriptor that will allow it to call <code>setns</code> to return to its own mount namespace.</p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#running-setns-from-a-multi-threaded-process","title":"Running <code>setns</code> From a Multi-threaded Process","text":"<p>Calling <code>setns</code> to a mount namespace doesn't work from a multi-threaded process.</p> <p>To work around this issue, the logic was moved to a standalone single-threaded executable called bpfman-ns that does the job of entering the namespace, attaching the uprobe, and then returning to the bpfman namespace to save the needed info.</p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#finding-the-pid","title":"Finding the PID","text":""},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#finding-a-host-container-pid-on-a-linux-server","title":"Finding a Host Container PID on a Linux Server","text":"<p>This section provides an overview of PID namespaces and shows several ways to find the host PID for a container.</p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#tldr","title":"tl;dr","text":"<p>If you used Podman or Docker to run your container, and you gave the container a unique name, the following commands can be used to find the host PID of a container.</p> <pre><code>podman inspect -f '{{.State.Pid}}' &lt;CONTAINER_NAME&gt;\n</code></pre> <p>or, similarly,</p> <pre><code>docker inspect -f '{{.State.Pid}}'  &lt;CONTAINER_NAME&gt;\n</code></pre>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#overview-of-pid-namespaces-and-container-host-pids","title":"Overview of PID namespaces and Container Host PIDs","text":"<p>Each container has a PID namespace. Each PID namespace (other than the root) is contained within a parent PID namespace. In general, this relationship is hierarchical and PID namespaces can be nested within other PID namespaces. In this section, we will just cover the case of a root PID namepsace on a Linux server that has containers with PID namespaces that are direct children of the root. The multi-level case is described in the section on Nested Containers with kind below.</p> <p>The PID namespaces can be listed using the <code>lsns -t pid</code> command. Before we start any containers, we just have the one root pid namespace as shown below.</p> <pre><code>sudo lsns -t pid\n        NS TYPE NPROCS PID USER COMMAND\n4026531836 pid     325   1 root /usr/lib/systemd/systemd rhgb --switched-root --system --deserialize 30\n</code></pre> <p>Now lets start a container with the following command in a new shell:</p> <pre><code>podman run -it --name=container_1 fedora:latest /bin/bash\n</code></pre> <p>NOTE: In this section, we are using <code>podman</code> to run containers. However, all of the same commands can also be used with <code>docker</code>.</p> <p>Now back on the host we have:</p> <pre><code>sudo lsns -t pid\n        NS TYPE NPROCS    PID USER      COMMAND\n4026531836 pid     337      1 root      /usr/lib/systemd/systemd rhgb --switched-root --system --deserialize 30\n4026532948 pid       1 150342 user_abcd /bin/bash\n</code></pre> <p>We can see that the host PID for the container we just started is 150342.</p> <p>Now let's start another container in a new shell with the same command (except with a different name), and run the <code>lsns</code> command again on the host.</p> <pre><code>podman run -it --name=container_2 fedora:latest /bin/bash\n</code></pre> <p>On the host:</p> <pre><code>sudo lsns -t pid\n        NS TYPE NPROCS    PID USER      COMMAND\n4026531836 pid     339      1 root      /usr/lib/systemd/systemd rhgb --switched-root --system --deserialize 30\n4026532948 pid       1 150342 user_abcd /bin/bash\n4026533041 pid       1 150545 user_abcd /bin/bash\n</code></pre> <p>We now have 3 pid namespaces -- one for root and two for the containers. Since we already know that the first container had PID 150342 we can conclude that the second container has PID 150545. However, what would we do if we didn't already know the PID for one of the containers?  </p> <p>If the container we were interested in was running a unique command, we could use that to disambiguate. However, in this case, both are running the same <code>/bin/bash</code> command.</p> <p>If something unique is running inside of the container, we can use the <code>ps -e -o pidns,pid,args</code> command to get some info.</p> <p>For example, run <code>sleep 1111</code> in <code>container_1</code>, then</p> <pre><code>sudo ps -e -o pidns,pid,args | grep 'sleep 1111'\n4026532948  150778 sleep 1111\n4026531836  151002 grep --color=auto sleep 1111\n</code></pre> <p>This tells us that the <code>sleep 1111</code> command is running in PID namespace 4026532948. And,</p> <pre><code>sudo lsns -t pid | grep 4026532948\n4026532948 pid       2 150342 user_abcd /bin/bash\n</code></pre> <p>Tells us that the container's host PID is 150342.</p> <p>Alternatively, we could run <code>lsns</code> inside of <code>container_1</code>.</p> <pre><code>dnf install -y util-linux\nlsns -t pid\n        NS TYPE NPROCS PID USER COMMAND\n4026532948 pid       2   1 root /bin/bash\n</code></pre> <p>This tells us a few interesting things. </p> <ol> <li>Inside the container, the PID is 1,</li> <li>We can't see any of the other PID namespaces inside the container.</li> <li>The container PID namespace is 4026532948.</li> </ol> <p>With the container PID namespace, we can run the <code>lsns -t pid | grep 4026532948</code> command as we did above to find the container's host PID</p> <p>Finally, the container runtime knows the pid mapping. As mentioned at the beginning of this section, if the unique name of the container is known, the following command can be used to get the host PID.</p> <pre><code>podman inspect -f '{{.State.Pid}}' container_1\n150342\n</code></pre>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#how-bpfman-agent-finds-the-pid-on-kubernetes","title":"How bpfman Agent Finds the PID on Kubernetes","text":"<p>When running on Kubernetes, the \"containers\" field in the UprobeProgram CRD can be used to identify one or more containers using the following information:</p> <ul> <li>Namespace</li> <li>Pod Label</li> <li>Container Name</li> </ul> <p>If the container selector matches any containers on a given node, the <code>bpfman-agent</code> determines the host PID for those containers and then calls <code>bpfman</code> to attach the uprobe in the container with the given PID.</p> <p>From what we can tell, there is no way to find the host PID for a container running in a Kubernetes pod from the Kubernetes interface. However, the container runtime does know this mapping. </p> <p>The <code>bpfman-agent</code> implementation uses multiple steps to find the set of PIDs on a given node (if any) for the containers that are identified by the container selector.</p> <ol> <li>It uses the Kubernetes interface to get a list of pods on the local node that    match the container selector.</li> <li>It uses use crictl with the names of the pods found to get the pod IDs</li> <li>It uses <code>crictl</code> with the pod ID to find the containers in those pods and    then checks whether any match the container selector.</li> <li>Finally, it uses <code>crictl</code> with the pod IDs found to get the host PIDs for the    containers.</li> </ol> <p>As an example, the bpfman.io_v1alpha1_uprobe_uprobeprogram_containers.yaml file can be used with the <code>kubectl apply -f</code> command to install uprobes on two of the containers in the <code>bpfman-agent</code> pod. The bpfman code does this programmatically, but we will step through the process of finding the host PIDs for the two containers here using cli commands to demonstrate how it works.</p> <p>We will use a kind deployment with bpfman for this demo. See Deploy Locally via KIND for instructions on how to get this running.</p> <p>The container selector in the above yaml file is the following.</p> <pre><code>  containers:\n    namespace: bpfman\n    pods:\n      matchLabels:\n        name: bpfman-daemon\n    containernames:\n      - bpfman\n      - bpfman-agent\n</code></pre> <p><code>bpfman</code> accesses the Kubernetes API and uses <code>crictl</code> from the <code>bpfman-agent</code> container. However, the <code>bpfman-agent</code> container doesn't have a shell by default, so we will run the examples from the <code>bpfman-deployment-control-plane</code> node, which will yield the same results. <code>bpfman-deployment-control-plane</code> is a docker container in our kind cluster, so enter the container.</p> <p><pre><code>docker exec -it c84cae77f800 /bin/bash\n</code></pre> Install <code>crictl</code>.</p> <pre><code>apt update\napt install wget\nVERSION=\"v1.28.0\"\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz\ntar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin\nrm -f crictl-$VERSION-linux-amd64.tar.gz\n</code></pre> <p>First use <code>kubectl</code> to get the list of pods that match our container selector.</p> <pre><code>kubectl get pods -n bpfman -l name=bpfman-daemon\nNAME                  READY   STATUS    RESTARTS   AGE\nbpfman-daemon-cv9fm   3/3     Running   0          6m54s\n</code></pre> <p>NOTE: The bpfman code also filters on the local node, but we only have one node in this deployment, so we'll ignore that here.</p> <p>Now, use <code>crictl</code> with the name of the pod found to get the pod ID.</p> <pre><code>crictl pods --name bpfman-daemon-cv9fm\nPOD ID              CREATED             STATE               NAME                  NAMESPACE           ATTEMPT             RUNTIME\ne359900d3eca5       46 minutes ago      Ready               bpfman-daemon-cv9fm   bpfman              0                   (default)\n</code></pre> <p>Now, use the pod ID to get the list of containers in the pod.</p> <pre><code>crictl ps --pod e359900d3eca5\nCONTAINER           IMAGE               CREATED             STATE               NAME                    ATTEMPT             POD ID              POD\n5eb3b4e5b45f8       50013f94a28d1       48 minutes ago      Running             node-driver-registrar   0                   e359900d3eca5       bpfman-daemon-cv9fm\n629172270a384       e507ecf33b1f8       48 minutes ago      Running             bpfman-agent            0                   e359900d3eca5       bpfman-daemon-cv9fm\n6d2420b80ddf0       86a517196f329       48 minutes ago      Running             bpfman                  0                   e359900d3eca5       bpfman-daemon-cv9fm\n</code></pre> <p>Now use the container IDs for the containers identified in the container selector to get the PIDs of the containers.</p> <pre><code># Get PIDs for bpfman-agent container\ncrictl inspect 629172270a384 | grep pid\n    \"pid\": 2158,\n            \"pid\": 1\n            \"type\": \"pid\"\n\n# Get PIDs for bpfman container\ncrictl inspect 6d2420b80ddf0 | grep pid\n    \"pid\": 2108,\n            \"pid\": 1\n            \"type\": \"pid\"\n</code></pre> <p>From the above output, we can tell that the host PID for the <code>bpfman-agent</code> container is 2158, and the host PID for the <code>bpfman</code> container is 2108. So, now <code>bpfman-agent</code> would have the information needed to call <code>bpfman</code> with a request to install a uprobe in the containers.</p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#nested-containers-with-kind","title":"Nested Containers with kind","text":"<p>kind is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. The kind cluster we used for the previous section had a single node.</p> <pre><code>$ kubectl get nodes\nNAME                              STATUS   ROLES           AGE   VERSION\nbpfman-deployment-control-plane   Ready    control-plane   24h   v1.27.3\n</code></pre> <p>We can see the container for that node on the base server from Docker as follows.</p> <pre><code>docker ps\nCONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS        PORTS                       NAMES\nc84cae77f800   kindest/node:v1.27.3   \"/usr/local/bin/entr\u2026\"   25 hours ago   Up 25 hours   127.0.0.1:36795-&gt;6443/tcp   bpfman-deployment-control-plane\n</code></pre> <p>Our cluster has a number of pods as shown below.</p> <pre><code>kubectl get pods -A\nNAMESPACE            NAME                                                      READY   STATUS    RESTARTS   AGE\nbpfman               bpfman-daemon-cv9fm                                       3/3     Running   0          24h\nbpfman               bpfman-operator-7f67bc7c57-bpw9v                          2/2     Running   0          24h\nkube-system          coredns-5d78c9869d-7tw9b                                  1/1     Running   0          24h\nkube-system          coredns-5d78c9869d-wxwfn                                  1/1     Running   0          24h\nkube-system          etcd-bpfman-deployment-control-plane                      1/1     Running   0          24h\nkube-system          kindnet-lbzw4                                             1/1     Running   0          24h\nkube-system          kube-apiserver-bpfman-deployment-control-plane            1/1     Running   0          24h\nkube-system          kube-controller-manager-bpfman-deployment-control-plane   1/1     Running   0          24h\nkube-system          kube-proxy-sz8v9                                          1/1     Running   0          24h\nkube-system          kube-scheduler-bpfman-deployment-control-plane            1/1     Running   0          24h\nlocal-path-storage   local-path-provisioner-6bc4bddd6b-22glj                   1/1     Running   0          24h\n</code></pre> <p>Using the <code>lsns</code> command in the node's docker container, we can see that it has a number of PID namespaces (1 for each container that is running in the pods in the cluster), and all of these containers are nested inside of the docker \"node\" container shown above.</p> <p><pre><code>lsns -t pid\n        NS TYPE NPROCS   PID USER  COMMAND\n# Note: 12 rows have been deleted below to save space\n4026532861 pid      17     1 root  /sbin/init\n4026532963 pid       1   509 root  kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-addre\n4026532965 pid       1   535 root  kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfi\n4026532967 pid       1   606 root  kube-apiserver --advertise-address=172.18.0.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt\n4026532969 pid       1   670 root  etcd --advertise-client-urls=https://172.18.0.2:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib\n4026532972 pid       1  1558 root  local-path-provisioner --debug start --helper-image docker.io/kindest/local-path-helper:v20230510-486859a6 --config /etc/config/config.json\n4026533071 pid       1   957 root  /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=bpfman-deployment-control-plane\n4026533073 pid       1  1047 root  /bin/kindnetd\n4026533229 pid       1  1382 root  /coredns -conf /etc/coredns/Corefile\n4026533312 pid       1  1896 65532 /usr/local/bin/kube-rbac-proxy --secure-listen-address=0.0.0.0:8443 --upstream=http://127.0.0.1:8174/ --logtostderr=true --v=0\n4026533314 pid       1  1943 65532 /bpfman-operator --health-probe-bind-address=:8175 --metrics-bind-address=127.0.0.1:8174 --leader-elect\n4026533319 pid       1  2108 root  ./bpfman system service --timeout=0 --csi-support\n4026533321 pid       1  2158 root  /bpfman-agent --health-probe-bind-address=:8175 --metrics-bind-address=127.0.0.1:8174\n4026533323 pid       1  2243 root  /csi-node-driver-registrar --v=5 --csi-address=/csi/csi.sock --kubelet-registration-path=/var/lib/kubelet/plugins/csi-bpfman/csi.sock\n</code></pre> We can see the bpfman containers we were looking at earlier in the output above. Let's take a deeper look at the <code>bpfman-agent</code> container that has a PID of 2158 on the Kubernetes node container and a PID namespace of 4026533321. If we go back to the base server, we can find the container's PID there.</p> <pre><code>sudo lsns -t pid | grep 4026533321\n4026533321 pid       1 222225 root  /bpfman-agent --health-probe-bind-address=:8175 --metrics-bind-address=127.0.0.1:8174\n</code></pre> <p>This command tells us that the PID of our <code>bpfman-agent</code> is 222225 on the base server. The information for this PID is contained in <code>/proc/222225</code>.  The following command will show the PID mappings for that one container at each level.</p> <pre><code>sudo grep NSpid /proc/222225/status\nNSpid:  222225  2158    1\n</code></pre> <p>The output above tells us that the PIDs for the <code>bpfman-agent</code> container are 222225 on the base server, 2158 in the Docker \"node\" container, and 1 inside the container itself.</p>"},{"location":"blog/2024/02/26/technical-challenges-for-attaching-ebpf-programs-in-containers/#moving-forward","title":"Moving Forward","text":"<p>As always, there is more work to do. The highest priority goals are to support additional eBPF program types and to use the Container Runtime Interface directly.</p> <p>We chose uprobes first because we had a user with a specific need. However, there are use cases for other eBPF program types.</p> <p>We used <code>crictl</code> in this first implementation because it already exists, supports multiple container runtimes, handles the corner cases, and is maintained. This allowed us to focus on the bpfman implementation and get the feature done more quickly. However, it would be better to access the container runtime interface directly rather than using an external executable.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/","title":"bpfman: A Novel Way to Manage eBPF","text":"<p>In today's cloud ecosystem, there's a demand for low-level system access to enable high-performance observability, security, and networking functionality for applications. Historically these features have been implemented in user space, however, the ability to program such functionality into the kernel itself can provide many benefits including (but not limited to) performance. Regardless, many Linux users still opt away from in-tree or kernel module development due to the slow rate of iteration and ensuing large management burden.  eBPF has emerged as a technology in the Linux Kernel looking to change all that.</p> <p>eBPF is a simple and efficient way to dynamically load programs into the kernel at runtime, with safety and performance provided by the kernel itself using a Just-In-Time (JIT) compiler and verification process. There are a wide variety of program types one can create with eBPF, which include everything from networking applications to security systems.</p> <p>However, eBPF is still a fairly nascent technology and it's not all kittens and rainbows. The process of developing, testing, deploying, and maintaining eBPF programs is not a road well traveled yet, and the story gets even more complicated when you want to deploy your programs in a multi-node system, such as a Kubernetes cluster. It was these kinds of problems that motivated the creation of bpfman, a system daemon for loading and managing eBPF programs in both traditional systems and Kubernetes clusters. In this blog post, we'll discuss the problems bpfman can help solve, and how to deploy and use it.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#current-challenges-with-developing-and-deploying-ebpf-programs","title":"Current Challenges with Developing and Deploying eBPF Programs","text":"<p>While some organizations have had success developing, deploying, and maintaining production software which includes eBPF programs, the barrier to entry is still very high.</p> <p>Following the basic eBPF development workflow, which often involves many hours trying to interpret and fix mind-bending eBPF verifier errors, the process of deploying a program in testing and staging environments often results in a lot of custom program loading and management functionality specific to the application. When moving to production systems in environments like Kubernetes clusters the operational considerations continue to compound.</p> <p>Security is another significant challenge, which we will cover in more depth in a follow-on blog.  However, at a high level, applications that use eBPF typically load their own eBPF programs, which requires at least CAP_BPF.  Many BPF programs and attach points require additional capabilities from CAP_SYS_PTRACE, CAP_NET_ADMIN and even including CAP_SYS_ADMIN.  These privileges include capabilities that aren\u2019t strictly necessary for eBPF and are too coarsely grained to be useful.  Since the processes that load eBPF are usually long-lived and often don\u2019t drop privileges it leaves a wide attack surface.</p> <p>While it doesn't solve all the ergonomic and maintenance problems associated with adopting eBPF, bpfman does try to address several of these issues -- particularly as it pertains to security and the lifecycle management of eBPF programs. In the coming sections, we will go into more depth about what eBPF does, and how it can help reduce the costs associated with deploying and managing eBPF-powered workloads.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#bpfman-overview","title":"bpfman Overview","text":"<p>The bpfman project provides a software stack that makes it easy to manage the full lifecycle of eBPF programs.  In particular, it can load, unload, modify, and monitor eBPF programs on a single host, or across a full Kubernetes cluster. The key components of bpfman include the bpfman daemon itself which can run independently on any Linux box, an accompanying Kubernetes Operator designed to bring first-class support to clusters via Custom Resource Definitions (CRDs), and eBPF program packaging.</p> <p>These components will be covered in more detail in the following sections.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#bpfman-daemon","title":"bpfman Daemon","text":"<p>The bpfman daemon works directly with the operating system to manage eBPF programs.  It loads, updates, and unloads eBPF programs, pins maps, and provides visibility into the eBPF programs loaded on a system.  Currently, bpfman fully supports XDP, TC, Tracepoint, uProbe, and kProbe eBPF programs. In addition, bpfman can display information about all types of eBPF programs loaded on a system whether they were loaded by bpfman or some other mechanism. bpfman is developed in the Rust programming language and uses Aya, an eBPF library which is also developed in Rust.</p> <p>When used on an individual server, bpfman runs as a system daemon, and applications communicate with it using a gRPC API.  bpfman can also be used via a command line which in turn uses the gRPC API. The following is an example of using bpfman to load and attach an xdp program.</p> <pre><code>bpfman load-from-image -g GLOBAL_u8=01 -i quay.io/bpfman-bytecode/xdp_pass:latest xdp -i eth0 -p 100\n</code></pre> <p>This architecture is depicted in the following diagram.</p> <p></p> <p>Using bpfman in this manner significantly improves security because the API is secured using mTLS, and only bpfman needs the privileges required to load and manage eBPF programs and maps.</p> <p>Writing eBPF code is tough enough as it is.  Typically, an eBPF-based application would need to also implement support for the lifecycle management of the required eBPF programs.  bpfman does that for you and allows you to focus on developing your application.</p> <p>Another key functional advantage that bpfman offers over libbpf or the Cilium ebpf-go library is support for multiple XDP programs. Standard XDP only allows a single XDP program on a given interface, while bpfman supports loading multiple XDP programs on each interface using the multi-prog protocol defined in libxdp. This allows the user to add, delete, update, prioritize, and re-prioritize the multiple programs on each interface. There is also support to configure whether the flow of execution should terminate and return or continue to the next program in the list based on the return value.</p> <p>While TC natively supports multiple programs on each attach point, it lacks the controls and flexibility enabled by the multi-prog protocol. bpfman therefore also supports the same XDP multi-prog solution for TC programs which has the added benefit of a consistent user experience for both XDP and TC programs.</p> <p>eBPF programs are also difficult to debug on a system.  The visibility provided by bpfman can be a key tool in understanding what is deployed and how they may interact.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#bpfman-kubernetes-support","title":"bpfman Kubernetes Support","text":"<p>The benefits of bpfman are brought to Kubernetes by the bpfman operator.  The bpfman operator is developed in Go using the Operator SDK framework, so it should be familiar to most Kubernetes application developers. The bpfman operator deploys a daemonset, containing both bpfman and the bpfman agent processes on each node. Rather than making requests directly to bpfman with the gRPC API or CLI as described above, Kubernetes applications use bpfman custom resource definitions (CRDs) to make requests to bpfman to load and attach eBPF programs.  bpfman uses two types of CRDs; Program CRDs for each eBPF program type (referred to as *Program CRDs, where * = Xdp, Tc, etc.) created by the application to express the desired state of an eBPF program on the cluster, and per node BpfProgram CRDs created by the bpfman agent to report the current state of the eBPF program on each node.</p> <p>Using XDP as an example, the application can request that an XDP program be loaded on multiple nodes using the XdpProgram CRD, which includes the necessary information such as the bytecode image to load, interface to attach it to, and priority.  An XdpProgram CRD that would do the same thing as the CLI command shown above on every node in a cluster is shown below.</p> <pre><code>apiVersion: bpfman.io/v1alpha1\nkind: XdpProgram\nmetadata:\n  labels:\n    app.kubernetes.io/name: xdpprogram\n  name: xdp-pass-all-nodes\nspec:\n  name: pass\n  # Select all nodes\n  nodeselector: {}\n  interfaceselector:\n    primarynodeinterface: true\n  priority: 0\n  bytecode:\n    image:\n      url: quay.io/bpfman-bytecode/xdp_pass:latest\n  globaldata:\n    GLOBAL_u8:\n      - 0x01\n</code></pre> <p>The bpfman agent on each node watches for the *Program CRDs, and makes calls to the local instance of bpfman as necessary to ensure that the state on the local node reflects the state requested in the *Program CRD.  The bpfman agent on each node in turn creates and updates a BpfProgram object for the *Program CRD that reflects the state of the program on that node and reports the eBPF map information for the program.  The following is the BpfProgram CRD on one node for the above XdpProgram CRD.</p> <pre><code>kubectl get bpfprograms.bpfman.io xdp-pass-all-nodes-bpfman-deployment-control-plane-eth0 -o yaml\n</code></pre> <pre><code>apiVersion: bpfman.io/v1alpha1\nkind: BpfProgram\nmetadata:\n  annotations:\n    bpfman.io.xdpprogramcontroller/interface: eth0\n  creationTimestamp: \"2023-08-29T22:08:12Z\"\n  finalizers:\n  - bpfman.io.xdpprogramcontroller/finalizer\n  generation: 1\n  labels:\n    bpfman.io/ownedByProgram: xdp-pass-all-nodes\n    kubernetes.io/hostname: bpfman-deployment-control-plane\n  name: xdp-pass-all-nodes-bpfman-deployment-control-plane-eth0\n  ownerReferences:\n  - apiVersion: bpfman.io/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: XdpProgram\n    name: xdp-pass-all-nodes\n    uid: 838dc2f8-a348-427e-9dc4-f6a6ea621930\n  resourceVersion: \"2690\"\n  uid: 5a622961-e5b0-44fe-98af-30756b2d0b62\nspec:\n  type: xdp\nstatus:\n  conditions:\n  - lastTransitionTime: \"2023-08-29T22:08:14Z\"\n    message: Successfully loaded bpfProgram\n    reason: bpfmanLoaded\n    status: \"True\"\n    type: Loaded\n</code></pre> <p>Finally, the bpfman operator watches for updates to the BpfProgram objects and reports the global state of each eBPF program.  If the program was successfully loaded on every selected node, it will report success, otherwise, it will identify the node(s) that had a problem.  The following is the XdpProgram CRD as updated by the operator.</p> <pre><code>kubectl get xdpprograms.bpfman.io xdp-pass-all-nodes -o yaml\n</code></pre> <pre><code>apiVersion: bpfman.io/v1alpha1\nkind: XdpProgram\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"bpfman.io/v1alpha1\",\"kind\":\"XdpProgram\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/name\":\"xdpprogram\"},\"name\":\"xdp-pass-all-nodes\"},\"spec\":{\"bytecode\":{\"image\":{\"url\":\"quay.io/bpfman-bytecode/xdp_pass:latest\"}},\"globaldata\":{\"GLOBAL_u8\":[1]},\"interfaceselector\":{\"primarynodeinterface\":true},\"nodeselector\":{},\"priority\":0,\"bpffunctionname\":\"pass\"}}\n  creationTimestamp: \"2023-08-29T22:08:12Z\"\n  finalizers:\n  - bpfman.io.operator/finalizer\n  generation: 2\n  labels:\n    app.kubernetes.io/name: xdpprogram\n  name: xdp-pass-all-nodes\n  resourceVersion: \"2685\"\n  uid: 838dc2f8-a348-427e-9dc4-f6a6ea621930\nspec:\n  bytecode:\n    image:\n      imagepullpolicy: IfNotPresent\n      url: quay.io/bpfman-bytecode/xdp_pass:latest\n  globaldata:\n    GLOBAL_u8: 0x01\n  interfaceselector:\n    primarynodeinterface: true\n  mapownerselector: {}\n  nodeselector: {}\n  priority: 0\n  proceedon:\n  - pass\n  - dispatcher_return\n  name: pass\nstatus:\n  conditions:\n  - lastTransitionTime: \"2023-08-29T22:08:12Z\"\n    message: Waiting for Program Object to be reconciled to all nodes\n    reason: ProgramsNotYetLoaded\n    status: \"True\"\n    type: NotYetLoaded\n  - lastTransitionTime: \"2023-08-29T22:08:12Z\"\n    message: bpfProgramReconciliation Succeeded on all nodes\n    reason: ReconcileSuccess\n    status: \"True\"\n    type: ReconcileSuccess\n</code></pre> <p>More details about this process can be seen here</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#ebpf-program-packaging","title":"eBPF program packaging","text":"<p>The eBPF Bytecode Image specification was created as part of the bpfman project to define a way to package eBPF bytecode as OCI container images.  Its use was illustrated in the CLI and <code>XdpProgram</code> CRD examples above in which the XDP program was loaded from <code>quay.io/bpfman-bytecode/xdp_pass:latest</code>. The initial motivation for this image spec was to facilitate the deployment of eBPF programs in container orchestration systems such as Kubernetes, where it is necessary to provide a portable way to distribute bytecode to all nodes that need it. However, bytecode images have proven useful on standalone Linux systems as well.  When coupled with BPF CO-RE (Compile Once \u2013 Run Everywhere), portability is further enhanced in that applications can use the same bytecode images across different kernel versions without the need to recompile them for each version. Another benefit of bytecode containers is image signing.  There is currently no way to sign and validate raw eBPF bytecode.  However, the bytecode containers can be signed and validated by bpfman using sigstore to improve supply chain security.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#key-benefits-of-bpfman","title":"Key benefits of bpfman","text":"<p>This section reviews some of the key benefits of bpfman.  These benefits mostly apply to both standalone and Kubernetes deployments, but we will focus on the benefits for Kubernetes here.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#security","title":"Security","text":"<p>Probably the most compelling benefit of using bpfman is enhanced security. When using bpfman, only the bpfman daemon, which can be tightly controlled, needs the privileges required to load eBPF programs, while access to the API can be controlled via standard RBAC methods on a per-application and per-CRD basis. Additionally, the signing and validating of bytecode images enables supply chain security.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#visibility-and-debuggability","title":"Visibility and Debuggability","text":"<p>eBPF programs can interact with each other in unexpected ways.  The multi-program support described above helps control these interactions by providing a common mechanism to prioritize and control the flow between the programs.  However, there can still be problems, and there may be eBPF programs running on nodes that were loaded by other mechanisms that you don\u2019t even know about.  bpfman helps here too by reporting all of the eBPF programs running on all of the nodes in a Kubernetes cluster.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#productivity","title":"Productivity","text":"<p>As described above, managing the lifecycle of eBPF programs is something that each application currently needs to do on its own.  It is even more complicated to manage the lifecycle of eBPF programs across a Kubernetes cluster.  bpfman does this for you so you don't have to.  eBPF bytecode images help here as well by simplifying the distribution of eBPF bytecode to multiple nodes in a cluster, and also allowing separate fine-grained versioning control for user space and kernel space code.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#demonstration","title":"Demonstration","text":"<p>This demonstration is adapted from the instructions documented by Andrew Stoycos here.</p> <p>These instructions use kind and bpfman release v0.2.1. It should also be possible to run this demo on other environments such as minikube or an actual cluster.</p> <p>Another option is to build the code yourself and use make run-on-kind</p> <p>to create the cluster as is described in the given links.  Then, start with step 5.</p>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#run-the-demo","title":"Run the demo","text":"<p>1. Create Kind Cluster</p> <pre><code>kind create cluster --name=test-bpfman\n</code></pre> <p>2. Deploy Cert manager</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml\n</code></pre> <p>3. Deploy bpfman Crds</p> <pre><code>kubectl apply -f  https://github.com/bpfman/bpfman/releases/download/v0.2.1/bpfman-crds-install-v0.2.1.yaml\n</code></pre> <p>4. Deploy bpfman-operator</p> <pre><code>kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v0.2.1/bpfman-operator-install-v0.2.1.yaml\n</code></pre> <p>5. Verify the deployment</p> <pre><code>kubectl get pods -A\n</code></pre> <pre><code>NAMESPACE            NAME                                              READY   STATUS    RESTARTS   AGE\nbpfman                 bpfman-daemon-nkzpf                                 2/2     Running   0          28s\nbpfman                 bpfman-operator-77d697fdd4-clrf7                    2/2     Running   0          33s\ncert-manager         cert-manager-99bb69456-x8n84                      1/1     Running   0          57s\ncert-manager         cert-manager-cainjector-ffb4747bb-pt4hr           1/1     Running   0          57s\ncert-manager         cert-manager-webhook-545bd5d7d8-z5brw             1/1     Running   0          57s\nkube-system          coredns-565d847f94-gjjft                          1/1     Running   0          61s\nkube-system          coredns-565d847f94-mf2cq                          1/1     Running   0          61s\nkube-system          etcd-test-bpfman-control-plane                      1/1     Running   0          76s\nkube-system          kindnet-lv6f9                                     1/1     Running   0          61s\nkube-system          kube-apiserver-test-bpfman-control-plane            1/1     Running   0          76s\nkube-system          kube-controller-manager-test-bpfman-control-plane   1/1     Running   0          77s\nkube-system          kube-proxy-dtmvb                                  1/1     Running   0          61s\nkube-system          kube-scheduler-test-bpfman-control-plane            1/1     Running   0          78s\nlocal-path-storage   local-path-provisioner-684f458cdd-8gxxv           1/1     Running   0          61s\n</code></pre> <p>Note that we have the bpfman-operator, bpf-daemon and cert-manager pods running.</p> <p>6. Deploy the XDP counter program and user space application</p> <pre><code>kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v0.2.1/go-xdp-counter-install-v0.2.1.yaml\n</code></pre> <p>7. Confirm that the programs are loaded</p> <p>Userspace program:</p> <pre><code>kubectl get pods -n go-xdp-counter\n</code></pre> <pre><code>NAME                      READY   STATUS              RESTARTS   AGE\ngo-xdp-counter-ds-9lpgp   0/1     ContainerCreating   0          5s\n</code></pre> <p>XDP program:</p> <pre><code>kubectl get xdpprograms.bpfman.io -o wide\n</code></pre> <pre><code>NAME                     BPFFUNCTIONNAME   NODESELECTOR   PRIORITY   INTERFACESELECTOR               PROCEEDON\ngo-xdp-counter-example   stats             {}             55         {\"primarynodeinterface\":true}   [\"pass\",\"dispatcher_return\"]\n</code></pre> <p>8. Confirm that the counter program is counting packets.</p> <p>Notes:</p> <ul> <li>The counters are updated every 5 seconds, and stats are being collected for the pod's primary node interface, which may not have a lot of traffic. However, running the <code>kubectl</code> command below generates traffic on that interface, so run the command a few times and give it a few seconds in between to confirm whether the counters are incrementing.</li> <li>Replace \"go-xdp-counter-ds-9lpgp\" with the go-xdp-counter pod name for your deployment.</li> </ul> <pre><code>kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail\n</code></pre> <pre><code>2023/09/05 16:58:21 1204 packets received\n2023/09/05 16:58:21 13741238 bytes received\n\n2023/09/05 16:58:24 1220 packets received\n2023/09/05 16:58:24 13744258 bytes received\n\n2023/09/05 16:58:27 1253 packets received\n2023/09/05 16:58:27 13750364 bytes received\n</code></pre> <p>9. Deploy the <code>xdp-pass-all-nodes</code> program with <code>priority</code> set to 50 and   <code>proceedon</code> set to <code>drop</code> as shown below</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: bpfman.io/v1alpha1\nkind: XdpProgram\nmetadata:\n  labels:\n    app.kubernetes.io/name: xdpprogram\n  name: xdp-pass-all-nodes\nspec:\n  name: pass\n  nodeselector: {}\n  interfaceselector:\n    primarynodeinterface: true\n  priority: 50\n  proceedon:\n    - drop\n  bytecode:\n    image:\n      url: quay.io/bpfman-bytecode/xdp_pass:latest\nEOF\n</code></pre> <p>10. Verify both XDP programs are loaded.</p> <pre><code>kubectl get xdpprograms.bpfman.io -o wide\n</code></pre> <pre><code>NAME                     BPFFUNCTIONNAME   NODESELECTOR   PRIORITY   INTERFACESELECTOR               PROCEEDON\ngo-xdp-counter-example   stats             {}             55         {\"primarynodeinterface\":true}   [\"pass\",\"dispatcher_return\"]\nxdp-pass-all-nodes       pass              {}             50         {\"primarynodeinterface\":true}   [\"drop\"]\n</code></pre> <p>The priority setting determines the order in which programs attached to the same interface are executed by the dispatcher with a lower number being a higher priority.  The <code>go-xdp-counter-example</code> program was loaded at priority 55, so the <code>xdp-pass-all-nodes</code> program will execute before the <code>go-xdp-counter-example</code> program.</p> <p>The proceedon setting tells the dispatcher whether to \"proceed\" to execute the next lower priority program attached to the same interface depending on the program's return value.  When we set proceedon to drop, execution will proceed only if the program returns <code>XDP_DROP</code>.  However, the <code>xdp-pass-all-nodes</code> program only returns <code>XDP_PASS</code>, so execution will terminate after it runs.</p> <p>Therefore, by loading the <code>xdp-pass-all-nodes</code> program in this way, we should have effectively stopped the <code>go-xdp-counter-example</code> program from running.  Let's confirm that.</p> <p>11. Verify that packet counts are not being updated anymore</p> <p>Run the following command several times</p> <pre><code>kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail\n</code></pre> <pre><code>2023/09/05 17:10:27 1395 packets received\n2023/09/05 17:10:27 13799730 bytes received\n\n2023/09/05 17:10:30 1395 packets received\n2023/09/05 17:10:30 13799730 bytes received\n\n2023/09/05 17:10:33 1395 packets received\n2023/09/05 17:10:33 13799730 bytes received\n</code></pre> <p>12. Now, change the priority of the xdp-pass program to 60</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: bpfman.io/v1alpha1\nkind: XdpProgram\nmetadata:\n  labels:\n    app.kubernetes.io/name: xdpprogram\n  name: xdp-pass-all-nodes\nspec:\n  name: pass\n  # Select all nodes\n  nodeselector: {}\n  interfaceselector:\n    primarynodeinterface: true\n  priority: 60\n  proceedon:\n    - drop\n  bytecode:\n    image:\n      url: quay.io/bpfman-bytecode/xdp_pass:latest\nEOF\n</code></pre> <p>13. Confirm that packets are being counted again</p> <p>Run the following command several times</p> <pre><code>kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail\n</code></pre> <pre><code>2023/09/05 17:12:21 1435 packets received\n2023/09/05 17:12:21 13806214 bytes received\n\n2023/09/05 17:12:24 1505 packets received\n2023/09/05 17:12:24 13815359 bytes received\n\n2023/09/05 17:12:27 1558 packets received\n2023/09/05 17:12:27 13823065 bytes received\n</code></pre> <p>We can see that the counters are incrementing again.</p> <p>14. Clean everything up</p> <p>Delete the programs</p> <pre><code>kubectl delete xdpprogram xdp-pass-all-nodes\nkubectl delete -f https://github.com/bpfman/bpfman/releases/download/v0.2.0/go-xdp-counter-install-v0.2.0.yaml\n</code></pre> <p>And/or, delete the whole kind cluster</p> <pre><code>kind delete clusters test-bpfman\n</code></pre>"},{"location":"blog/2023/09/07/bpfman-a-novel-way-to-manage-ebpf/#joining-the-bpfman-community","title":"Joining the bpfman community","text":"<p>If you're interested in bpfman and want to get involved, you can connect with the community in multiple ways. If you have some simple questions or need some help feel free to start a discussion. If you find an issue, or you want to request a new feature, please create an issue. If you want something a little more synchronous, the project maintains a <code>#bpfman</code> channel on Kubernetes Slack and we have a weekly community meeting where everyone can join and bring topics to discuss about the project. We hope to see you there!</p>"},{"location":"blog/2024/01/15/bpfmans-shift-towards-a-daemonless-design-and-using-sled-a-high-performance-embedded-database/","title":"bpfman's Shift Towards a Daemonless Design and Using Sled: a High Performance Embedded Database","text":"<p>As part of issue #860 the community has steadily been converting all of the internal state management to go through a sled database instance which is part of the larger effort to make bpfman completely damonless.</p> <p>This article will go over the reasons behind the change and dive into some of the details of the actual implementation.</p>"},{"location":"blog/2024/01/15/bpfmans-shift-towards-a-daemonless-design-and-using-sled-a-high-performance-embedded-database/#why","title":"Why?","text":"<p>State management in bpfman has always been a headache, not because there's a huge amount of disparate data but there's multiple representations of the same data. Additionally the delicate filesystem interactions and layout previously used to ensure persistence across restarts often led to issues.</p> <p>Understanding the existing flow of data in bpfman can help make this a bit clearer:</p> <p></p> <p>With this design there was a lot of data wrangling required to convert the tonic generated rust bindings for the protocol buffer API into data structures that were useful for bpfman. Specifically, data would arrive via GRPC server as specified in <code>bpfman.v1.rs</code> where rust types are inferred from the protobuf definition. In <code>rpc.rs</code> data was then converted to an internal set of structures defined in <code>command.rs</code>.  Prior to pull request #683 there was an explosion of types, with each bpfman command having it's own set of internal structures and enums.  Now, most of the data for a program that bpfman needs internally for all commands to manage an eBPF program is stored in the <code>ProgramData</code> structure, which we'll take a deeper look at a bit later. Additionally, there is extra complexity for XDP and TC program types which rely on an eBPF dispatcher program to provide multi-program support on a single network interface, however this article will try to instead focus on the simpler examples.</p> <p>The tree of data stored by bpfman is quite complex and this is made even more complicated since bpfman has to be persistent across restarts. To support this, raw data was often flushed to disk in the form of JSON files (all types in <code>command.rs</code> needed to implement serde's <code>Serialize</code> and <code>Deserialize</code>). Specific significance would also be encoded to bpfman's directory structure, i.e all program related information was encoded in <code>/run/bpfd/programs/&lt;ID&gt;</code>. The extra infrastructure and failure modes introduced by this process was a constant headache, pushing the community to find a better solution.</p>"},{"location":"blog/2024/01/15/bpfmans-shift-towards-a-daemonless-design-and-using-sled-a-high-performance-embedded-database/#why-sled","title":"Why Sled?","text":"<p>Sled is an open source project described in github as \"the champagne of beta embedded databases\". The \"reasons\" for choosing an embedded database from the project website are pretty much spot on:</p> <pre><code>Embedded databases are useful in several cases:\n\n- you want to store data on disk, without facing the complexity of files\n- you want to be simple, without operating an external database\n- you want to be fast, without paying network costs\n- using disk storage as a building block in your system\n</code></pre> <p>As discussed in the previous section, persistence across restarts, is one of bpfman's core design constraints, and with sled we almost get it for free! Additionally due to the pervasive nature of data management to bpfman's core workflow the data-store needed to be kept as simple and light weight as possible, ruling out heavier production-ready external database systems such as MySQL or Redis.</p> <p>Now this mostly focused on why embedded dbs in general, but why did we choose sled...well because it's written in :crab: Rust :crab: of course! Apart from the obvious we took a small dive into the project before rewriting everything by transitioning the OCI bytecode image library to use the db rather than the filesystem.  Overall the experience was extremely positive due to the following:</p> <ul> <li>No more dealing directly with the filesystem, the sled instance is flushed to   the fs automatically every 500 ms by default and for good measure we manually   flush it before shutting down.</li> <li>The API is extremely simple, traditional get and insert operations function   as expected.</li> <li>Error handling with <code>sled:Error</code> is relatively simple and easy to map explicitly   to a <code>bpfmanError</code></li> <li>The db \"tree\" concept makes it easy to have separate key-spaces within the same   instance.</li> </ul>"},{"location":"blog/2024/01/15/bpfmans-shift-towards-a-daemonless-design-and-using-sled-a-high-performance-embedded-database/#transitioning-to-sled","title":"Transitioning to Sled","text":"<p>Using the new embedded database started with the creation of a sled instance which could be easily shared across all of the modules in bpfman. To do this we utilized a globally available [<code>lazy_static</code>] variable called <code>ROOT_DB</code> in <code>main.rs</code>:</p> <pre><code>#[cfg(not(test))]\nlazy_static! {\n    pub static ref ROOT_DB: Db = Config::default()\n        .path(STDIR_DB)\n        .open()\n        .expect(\"Unable to open root database\");\n}\n\n#[cfg(test)]\nlazy_static! {\n    pub static ref ROOT_DB: Db = Config::default()\n        .temporary(true)\n        .open()\n        .expect(\"Unable to open temporary root database\");\n}\n</code></pre> <p>This block creates OR opens the filesystem backed database at <code>/var/lib/bpfman/db</code> database only when the <code>ROOT_DB</code> variable is first accessed, and also allows for the creation of a temporary db instance if running in unit tests. With this setup all of the modules within bpfman can now easily access the database instance by simply using it i.e <code>use crate::ROOT_DB</code>.</p> <p>Next the existing bpfman structures needed to be flattened in order to work with the db, the central <code>ProgramData</code> can be used to demonstrate how this was completed. Prior to the recent sled conversion that structure looked like:</p> <pre><code>/// ProgramInfo stores information about bpf programs that are loaded and managed\n/// by bpfd.\n#[derive(Debug, Serialize, Deserialize, Clone, Default)]\npub(crate) struct ProgramData {\n    // known at load time, set by user\n    name: String,\n    location: Location,\n    metadata: HashMap&lt;String, String&gt;,\n    global_data: HashMap&lt;String, Vec&lt;u8&gt;&gt;,\n    map_owner_id: Option&lt;u32&gt;,\n\n    // populated after load\n    kernel_info: Option&lt;KernelProgramInfo&gt;,\n    map_pin_path: Option&lt;PathBuf&gt;,\n    maps_used_by: Option&lt;Vec&lt;u32&gt;&gt;,\n\n    // program_bytes is used to temporarily cache the raw program data during\n    // the loading process.  It MUST be cleared following a load so that there\n    // is not a long lived copy of the program data living on the heap.\n    #[serde(skip_serializing, skip_deserializing)]\n    program_bytes: Vec&lt;u8&gt;,\n}\n</code></pre> <p>This worked well enough, but as mentioned before the process of flushing the data to disk involved manual serialization to JSON, which needed to occur at a specific point in time (following program load) which made disaster recovery almost impossible and could sometimes result in lost or partially reconstructed state.</p> <p>With sled the first idea was to completely flatten ALL of bpfman's data into a single key-space, so that <code>program.name</code> now simply turns into a <code>db.get(\"program_&lt;ID&gt;_name\")</code>, however removing all of the core structures would have resulted in a complex diff which would have been hard to review and merge.  Therefore a more staged approach was taken, the <code>ProgramData</code> structure was kept around, and now looks like:</p> <pre><code>/// ProgramInfo stores information about bpf programs that are loaded and managed\n/// by bpfman.\n#[derive(Debug, Clone)]\npub(crate) struct ProgramData {\n    // Prior to load this will be a temporary Tree with a random ID, following\n    // load it will be replaced with the main program database tree.\n    db_tree: sled::Tree,\n\n    // populated after load, randomly generated prior to load.\n    id: u32,\n\n    // program_bytes is used to temporarily cache the raw program data during\n    // the loading process.  It MUST be cleared following a load so that there\n    // is not a long lived copy of the program data living on the heap.\n    program_bytes: Vec&lt;u8&gt;,\n}\n</code></pre> <p>All of the fields are now removed in favor of a private reference to the unique [<code>sled::Tree</code>] instance for this <code>ProgramData</code> which is named using the unique kernel id for the program. Each <code>sled::Tree</code> represents a single logical key-space / namespace / bucket which allows key generation to be kept simple, i.e <code>db.get(\"program_&lt;ID&gt;_name\")</code> now can be <code>db_tree_prog_0000.get(\"program_name)</code>. Additionally getters and setters are now built for each existing field so that access to the db can be controlled and the serialization/deserialization process can be hidden from the caller:</p> <pre><code>...\npub(crate) fn set_name(&amp;mut self, name: &amp;str) -&gt; Result&lt;(), BpfmanError&gt; {\n    self.insert(\"name\", name.as_bytes())\n}\n\npub(crate) fn get_name(&amp;self) -&gt; Result&lt;String, BpfmanError&gt; {\n    self.get(\"name\").map(|v| bytes_to_string(&amp;v))\n}\n...\n</code></pre> <p>Therefore, <code>ProgramData</code> is now less of a container for program data and more of a wrapper for accessing program data.  The getters/setters act as a bridge between standard Rust types and the raw bytes stored in the database, i.e the [<code>sled::IVec</code> type].</p> <p>Once this was completed for all the relevant fields on all the relevant types, see pull request #874, the data bpfman needed for it's managed eBPF programs was now automatically synced to disk :partying_face:</p>"},{"location":"blog/2024/01/15/bpfmans-shift-towards-a-daemonless-design-and-using-sled-a-high-performance-embedded-database/#tradeoffs","title":"Tradeoffs","text":"<p>All design changes come with some tradeoffs: for bpfman's conversion to using sled the main negative ended up being with the complexity introduced with the [<code>sled::IVec</code> type]. It is basically just a thread-safe reference-counting pointer to a raw byte slice, and the only type raw database operations can be performed with. Previously when using <code>serde_json</code> all serialization/deserialization was automatically handled, however with sled the conversion is manual handled internally. Therefore, instead of a library handling the conversion of a rust string (<code>std::string::String</code>) to raw bytes <code>&amp;[u8]</code> bpfman has to handle it internally, using [<code>std::string::String::as_bytes</code>] and <code>bpfman::utils::bytes_to_string</code>:</p> <pre><code>pub(crate) fn bytes_to_string(bytes: &amp;[u8]) -&gt; String {\n    String::from_utf8(bytes.to_vec()).expect(\"failed to convert &amp;[u8] to string\")\n}\n</code></pre> <p>For strings, conversion was simple enough, but when working with more complex rust data types like <code>HashMaps</code> and <code>Vectors</code> this became a bit more of an issue. For <code>Vectors</code>, we simply flatten the structure into a group of key/values with indexes encoded into the key:</p> <pre><code>    pub(crate) fn set_kernel_map_ids(&amp;mut self, map_ids: Vec&lt;u32&gt;) -&gt; Result&lt;(), BpfmanError&gt; {\n        let map_ids = map_ids.iter().map(|i| i.to_ne_bytes()).collect::&lt;Vec&lt;_&gt;&gt;();\n\n        map_ids.iter().enumerate().try_for_each(|(i, v)| {\n            sled_insert(&amp;self.db_tree, format!(\"kernel_map_ids_{i}\").as_str(), v)\n        })\n    }\n</code></pre> <p>The sled <code>scan_prefix(&lt;K&gt;)</code> api then allows for easy fetching and rebuilding of the vector:</p> <pre><code>    pub(crate) fn get_kernel_map_ids(&amp;self) -&gt; Result&lt;Vec&lt;u32&gt;, BpfmanError&gt; {\n        self.db_tree\n            .scan_prefix(\"kernel_map_ids_\".as_bytes())\n            .map(|n| n.map(|(_, v)| bytes_to_u32(v.to_vec())))\n            .map(|n| {\n                n.map_err(|e| {\n                    BpfmanError::DatabaseError(\"Failed to get map ids\".to_string(), e.to_string())\n                })\n            })\n            .collect()\n    }\n</code></pre> <p>For <code>HashMaps</code>, we follow a similar paradigm, except the map key is encoded in the database key:</p> <pre><code>    pub(crate) fn set_metadata(\n        &amp;mut self,\n        data: HashMap&lt;String, String&gt;,\n    ) -&gt; Result&lt;(), BpfmanError&gt; {\n        data.iter().try_for_each(|(k, v)| {\n            sled_insert(\n                &amp;self.db_tree,\n                format!(\"metadata_{k}\").as_str(),\n                v.as_bytes(),\n            )\n        })\n    }\n\n    pub(crate) fn get_metadata(&amp;self) -&gt; Result&lt;HashMap&lt;String, String&gt;, BpfmanError&gt; {\n    self.db_tree\n        .scan_prefix(\"metadata_\")\n        .map(|n| {\n            n.map(|(k, v)| {\n                (\n                    bytes_to_string(&amp;k)\n                        .strip_prefix(\"metadata_\")\n                        .unwrap()\n                        .to_string(),\n                    bytes_to_string(&amp;v).to_string(),\n                )\n            })\n        })\n        .map(|n| {\n            n.map_err(|e| {\n                BpfmanError::DatabaseError(\"Failed to get metadata\".to_string(), e.to_string())\n            })\n        })\n        .collect()\n    }\n</code></pre> <p>The same result could be achieved by creating individual database trees for each <code>Vector</code>/<code>HashMap</code> instance, however our goal was to keep the layout as flat as possible. Although this resulted in some extra complexity within the data layer, the overall benefits still outweighed the extra code once the conversion was complete.</p>"},{"location":"blog/2024/01/15/bpfmans-shift-towards-a-daemonless-design-and-using-sled-a-high-performance-embedded-database/#moving-forward-and-getting-involved","title":"Moving forward and Getting Involved","text":"<p>Once the conversion to sled is fully complete, see issue #860, the project will be able to completely transition to becoming a library without having to worry about data and state management.</p> <p>If you are interested in in memory databases, eBPF, Rust, or any of the technologies discussed today please don't hesitate to reach out at kubernetes slack on channel <code>#bpfman</code> or join one of the community meetings to get involved.</p>"},{"location":"blog/2024/01/04/community-meeting-january-4-2024/","title":"Community Meeting: January 4, 2024","text":""},{"location":"blog/2024/01/04/community-meeting-january-4-2024/#welcome-to-2024","title":"Welcome to 2024!","text":"<p>Welcome to the first <code>bpfman</code> Community Meeting of 2024. We are happy to start off a new year and excited for all the changes in store for <code>bpfman</code> in 2024!</p> <p>Below were some of the discussion points from this weeks Community Meeting.</p> <ul> <li>bpfman-csi Needs To Become Its Own Binary</li> <li>Kubernetes Support For Attaching uprobes In Containers</li> <li>Building The Community</li> </ul>"},{"location":"blog/2024/01/04/community-meeting-january-4-2024/#bpfman-csi-needs-to-become-its-own-binary","title":"bpfman-csi Needs To Become Its Own Binary","text":"<p>Some of the next work items for <code>bpfman</code> revolve around removing the async code from the code base, make <code>bpfman-core</code> a rust library, and removing all the gRPC logic. Dave (@dave-tucker) is currently investigating this. One area to help out is to take the <code>bpfman-csi</code> thread and making it it's own binary. This may require making <code>bpfman</code> a bin and lib crate (which is fine, just needs a lib.rs and to be very careful about what we\u2019re exporting). Andrew (@astoycos) is starting to take a look at this.</p>"},{"location":"blog/2024/01/04/community-meeting-january-4-2024/#kubernetes-support-for-attaching-uprobes-in-containers","title":"Kubernetes Support For Attaching uprobes In Containers","text":"<p>Base support for attaching uprobes in containers is currently merged. Andre (@anfredette) pushed PR#875 for the integration with Kubernetes. The hard problems are solved, like getting the Container PID, but the current PR has some shortcuts to get the functionality working before the holiday break. So the PR#875 is not ready for review, but Dave (@dave-tucker) and Andre (@anfredette) may have a quick review to verify the design principles.</p>"},{"location":"blog/2024/01/04/community-meeting-january-4-2024/#building-the-community","title":"Building The Community","text":"<p>Short discussion on building the Community. In a previous meeting, Dave (@dave-tucker) suggested capturing the meeting minutes in blogs. By placing in a blog, they become searchable from search engines. Billy (@billy99) re-raised this topic and volunteered to start capturing the content. In future meetings, we may use the transcript feature from Google Meet to capture the content and try generating the blog via ChatGTP.</p>"},{"location":"blog/2024/01/04/community-meeting-january-4-2024/#light-hearted-moments-and-casual-conversations","title":"Light-hearted Moments and Casual Conversations","text":"<p>Amidst the technical discussions, the community members took a moment to share some light-hearted moments and casual conversations. Topics ranged from the challenges of post-holiday credit card bills to the complexities of managing family schedules during exam week. The discussion touched on the quirks of public school rules and the unique challenges of parenting during exam periods.</p> <p>The meeting ended on a friendly note, with plans for further collaboration and individual tasks assigned for the upcoming days. Participants expressed their commitment to pushing updates and improvements, with a promise to reconvene in the near future.</p>"},{"location":"blog/2024/01/04/community-meeting-january-4-2024/#attendees","title":"Attendees","text":"<ul> <li>Andre Fredette (Red Hat)</li> <li>Andrew Stoycos (Red Hat)</li> <li>Billy McFall (Red Hat)</li> <li>Dave Tucker (Red Hat)</li> </ul>"},{"location":"blog/2024/01/04/community-meeting-january-4-2024/#bpfman-community-info","title":"bpfman Community Info","text":"<p>A friendly reminder that the Community Meetings are every Thursday 10am-11am Eastern US Time and all are welcome!</p> <p>Google Meet joining info:</p> <ul> <li>Google Meet</li> <li>Or dial: (US) +1 984-221-0859 PIN: 613 588 790#</li> <li>Agenda Document</li> </ul>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/","title":"Community Meeting: January 11 and 18, 2024","text":""},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#hit-the-ground-running","title":"Hit the Ground Running","text":"<p>Another set of <code>bpfman</code> Community Meetings for 2024. There is a lot going on with <code>bpfman</code> in Q1 of 2024. Spending a lot of time making <code>bpfman</code> daemonless. I bailed for a ski trip after the Jan 11 meeting, so the notes didn't get written up. So this summary will include two weeks of meetings.</p> <p>Below were some of the discussion points from the last two weeks Community Meetings.</p> <ul> <li>Manpage/CLI TAB Completion Questions (Jan 11)</li> <li>Kubernetes Support for Attaching uprobes in Containers (Jan 11)</li> <li>netify Preview in Github Removed (Jan 11)</li> <li>RPM Builds and Socket Activation (Jan 18)</li> <li>KubeCon EU Discussion (Jan 18)</li> </ul>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#january-11-2024","title":"January 11, 2024","text":""},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#manpagecli-tab-completion-questions-jan-11","title":"Manpage/CLI TAB Completion Questions (Jan 11)","text":"<p>The <code>bpfman</code> CLI now has TAB Completion and man pages. However, a couple nits need to be cleaned up Issue#913 and Billy (@billy99) wanted to clarify a few issues encountered. The current implementation for both features  is using an environment variable to set the destination directory for the generated files. Other features don't work this way and there was a discussion on the proper location for the generated files. The decision was to use <code>.output/.</code>.</p> <p>There was another discussion around <code>clap</code> (Rust CLI crate) and passing variables to <code>clap</code> from the Cargo.toml file. In the CLI code, <code>#[command(author, version, about, long_about = None)]</code> implies to pull the values from the Config.toml file, but we aren\u2019t setting any of those variables. Also, for <code>cargo xtask build-man-page</code> and <code>cargo xtask build-completion</code> they pull from the xtask Cargo.toml file. The decision was to set the variables implicitly in code and not pull from Cargo.toml.</p>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#kubernetes-support-for-attaching-uprobes-in-containers-jan-11","title":"Kubernetes Support for Attaching uprobes in Containers (Jan 11)","text":"<p>Andre (@anfredette) is working on a feature to enable attaching uprobes in other Containers. Currently, <code>bpfman</code> only supports attaching uprobes within the <code>bpfman</code> container. There was a discussion on proper way to format a query to the KubeAPI server to match on NodeName on a Pod list. The discussion included so code walk through. Andrew (@astoycos) found a possible solution client-go:Issue#410 and Dave (@dave-tucker) suggested kubernetes-api:podspec-v1-core.</p>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#netify-preview-in-github-removed-jan-11","title":"netify Preview in Github Removed (Jan 11)","text":"<p>Lastly, there was a discussion on the <code>netify</code> preview being removed from github and a reminder why. Dave (@dave-tucker) explained that with the docs release history now in place, \"current\" is from a branch and it is not easy to preview. So for now, document developers need to run mkdocs locally (See generate-documention).</p>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#attendees-jan-11","title":"Attendees (Jan 11)","text":"<ul> <li>Andre Fredette (Red Hat)</li> <li>Andrew Stoycos (Red Hat)</li> <li>Billy McFall (Red Hat)</li> <li>Dave Tucker (Red Hat)</li> <li>Shane Utt (Kong)</li> </ul>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#january-18-2024","title":"January 18, 2024","text":""},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#rpm-builds-and-socket-activation-jan-18","title":"RPM Builds and Socket Activation (Jan 18)","text":"<p>RPM Builds for <code>bpfman</code> went in fairly recently and Billy (@billy99) had some questions around their implementation. RPM and Socket Activation were developed and merged around the same time and the RPM builds are not installing socket activation properly. Just verifying that RPMs should be installing the <code>bpfman.socket</code> file. And they should. There were also some questions on how to build RPMs locally. Verified that <code>packit build locally</code> is the way forward.</p> <p>Note: Socket activation was added to RPM Builds along with documentation on building and using RPMs in PR#922</p>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#kubecon-eu-discussion-jan-18","title":"KubeCon EU Discussion (Jan 18)","text":"<p>With KubeCon EU just around the corner (March 19-22, 2024 in Paris), discussion around bpfman talks and who was attending. Dave (@dave-tucker) is probably attending and Shane (@shaneutt) might attend. So if you are planning on attending KubeCon EU and are interested in <code>bpfman</code> or just eBPF, keep an eye out for these guys for some lively discussions!</p>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#attendees-jan-18","title":"Attendees (Jan 18)","text":"<ul> <li>Billy McFall (Red Hat)</li> <li>Dave Tucker (Red Hat)</li> <li>Shane Utt (Kong)</li> </ul>"},{"location":"blog/2024/01/19/community-meeting-january-11-and-18-2024/#bpfman-community-info","title":"bpfman Community Info","text":"<p>A friendly reminder that the Community Meetings are every Thursday 10am-11am Eastern US Time and all are welcome!</p> <p>Google Meet joining info:</p> <ul> <li>Google Meet</li> <li>Or dial: (US) +1 984-221-0859 PIN: 613 588 790#</li> <li>Agenda Document</li> </ul>"},{"location":"design/daemonless/","title":"Daemonless bpfd","text":""},{"location":"design/daemonless/#introduction","title":"Introduction","text":"<p>The bpfd daemon is a userspace daemon that runs on the host and responds to gRPC API requests over a unix socket, to load, unload and list the eBPF programs on a host.</p> <p>The rationale behind running as a daemon was because something needs to be listening on the unix socket for API requests, and that we also maintain some state in-memory about the programs that have been loaded. However, since this daemon requires root privileges to load and unload eBPF programs it is a security risk for this to be a long-running - even with the mitigations we have in place to drop privileges and run as a non-root user. This risk is equivalent to that of something like Docker.</p> <p>This document describes the design of a daemonless bpfd, which is a bpfd that runs only runs when required, for example, to load or unload an eBPF program.</p>"},{"location":"design/daemonless/#design","title":"Design","text":"<p>The daemonless bpfd is a single binary that collects some of the functionality from both bpfd and bpfctl.</p> <p>:note: Daemonless, not rootless. Since CAP_BPF (and more) is required to load and unload eBPF programs, we will still need to run as root. But at least we can run as root for a shorter period of time.</p>"},{"location":"design/daemonless/#command-bpfd-system-service","title":"Command: bpfd system service","text":"<p>This command will run the bpfd gRPC API server - for one or more of the gRPC API services we support.</p> <p>It will listen on a unix socket (or tcp socket) for API requests - provided via a positional argument, defaulting to <code>unix:///var/run/bpfd.sock</code>. It will shutdown after a timeout of inactivity - provided by a <code>--timeout</code> flag defaulting to 5 seconds.</p> <p>It will support being run as a systemd service, via socket activation, which will allow it to be started on demand when a request is made to the unix socket. When in this mode it will not create the unix socket itself, but will instead use LISTEN_FDS to determine the file descriptor of the unix socket to use.</p> <p>Usage in local development (or packaged in a container) is still possible by running as follows:</p> <pre><code>sudo bpfd --timeout=0 unix:///var/run/bpfd.sock\n</code></pre> <p>:note: The bpfd user and group will be deprecated. We will also remove some of the unit-file complexity (i.e directories) and handle this in bpfd itself.</p>"},{"location":"design/daemonless/#command-bpfd-load-file","title":"Command: bpfd load file","text":"<p>As the name suggests, this command will load an eBPF program from a file. This was formerly <code>bpfctl load-from-file</code>.</p>"},{"location":"design/daemonless/#command-bpfd-load-image","title":"Command: bpfd load image","text":"<p>As the name suggests, this command will load an eBPF program from a container image. This was formerly <code>bpfctl load-from-image</code>.</p>"},{"location":"design/daemonless/#command-bpfd-unload","title":"Command: bpfd unload","text":"<p>This command will unload an eBPF program. This was formerly <code>bpfctl unload</code>.</p>"},{"location":"design/daemonless/#command-bpfd-list","title":"Command: bpfd list","text":"<p>This command will list the eBPF programs that are currently loaded. This was formerly <code>bpfctl list</code>.</p>"},{"location":"design/daemonless/#command-bpfd-pull","title":"Command: bpfd pull","text":"<p>This command will pull the bpfd container image from a registry. This was formerly <code>bpfctl pull</code>.</p>"},{"location":"design/daemonless/#command-bpfd-images","title":"Command: bpfd images","text":"<p>This command will list the bpfd container images that are available. This command didn't exist, but makes sense to add.</p>"},{"location":"design/daemonless/#command-bpfd-version","title":"Command: bpfd version","text":"<p>This command will print the version of bpfd. This command didn't exist, but makes sense to add.</p>"},{"location":"design/daemonless/#state-management","title":"State Management","text":"<p>This is perhaps the most significant change from how bpfd currently works.</p> <p>Currently bpfd maintains state in-memory about the programs that have been loaded (by bpfd, and the kernel). Some of this state is flushed to disk, so if bpfd is restarted it can reconstruct it.</p> <p>Flushing to disk and state reconstruction is cumbersome at present and having to move all state management out of in-memory stores is a forcing function to improve this. We will replace the existing state management with sled, which gives us a familiar API to work with while also being fast, reliable and persistent.</p>"},{"location":"design/daemonless/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"<p>While adding metrics and monitoring is not a goal of this design, it should nevertheless be a consideration. In order to provide metrics to Prometheus or OpenTelemetry we will require an additional exporter process.</p> <p>We can either:</p> <ol> <li>Use the bpfd socket and retrieve metrics via the gRPC API</li> <li>Place state access + metrics gathering functions in a library, such that    they could be used directly by the exporter process without requiring the    bpfd socket.</li> </ol> <p>The latter would be more inline with how podman-prometheus-exporter works. The benefit here is that, the metrics exporter process can be long running with less privileges - whereas if it were to hit the API over the socket it would effectively negate the point of being daemonless in the first place since collection will likley occur more frequently than the timeout on the socket.</p>"},{"location":"design/daemonless/#benefits","title":"Benefits","text":"<p>The benefits of this design are:</p> <ul> <li>No long-running daemon with root privileges</li> <li>No need to run as a non-root user, this is important since the number of   capabilities required is only getting larger.</li> <li>We only need to ship a single binary.</li> <li>We can use systemd socket activation to start bpfd on demand + timeout   after a period of inactivity.</li> <li>Forcs us to fix state management, since we can never rely on in-memory state.</li> <li>Bpfd becomes more modular - if we wish to add programs for runtime enforcement,   metrics, or any other purpose then it's design is decoupled from that of   bpfd. It could be another binary, or a subcommand on the CLI etc...</li> </ul>"},{"location":"design/daemonless/#drawbacks","title":"Drawbacks","text":"<p>None yet.</p>"},{"location":"design/daemonless/#backwards-compatibility","title":"Backwards Compatibility","text":"<ul> <li>The <code>bpfctl</code> command will be removed and all functionality folded into <code>bpfd</code></li> <li>The <code>bpfd</code> command will be renamed to <code>bpfd system service</code></li> </ul>"},{"location":"developer-guide/api-spec/","title":"API Specification","text":"<p>Packages:</p> <ul> <li> bpfman.io/v1alpha1 </li> </ul>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1","title":"bpfman.io/v1alpha1","text":"<p> <p>Package v1alpha1 contains API Schema definitions for the bpfman.io API group.</p> </p> <p>Resource Types:</p> <ul><li> BpfProgram </li><li> FentryProgram </li><li> FexitProgram </li><li> KprobeProgram </li><li> TcProgram </li><li> TracepointProgram </li><li> UprobeProgram </li><li> XdpProgram </li></ul>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.BpfProgram","title":"BpfProgram","text":"<p> <p>BpfProgram is the Schema for the Bpfprograms API</p> </p> Field Description <code>apiVersion</code> string <code> bpfman.io/v1alpha1 </code> <code>kind</code> string  <code>BpfProgram</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta   Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  BpfProgramSpec  <code>type</code>  string  (Optional) <p>Type specifies the bpf program type</p> <code>status</code>  BpfProgramStatus  (Optional)"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.FentryProgram","title":"FentryProgram","text":"<p> <p>FentryProgram is the Schema for the FentryPrograms API</p> </p> Field Description <code>apiVersion</code> string <code> bpfman.io/v1alpha1 </code> <code>kind</code> string  <code>FentryProgram</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta   Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  FentryProgramSpec  <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>func_name</code>  string  <p>Function to attach the fentry to.</p> <code>status</code>  FentryProgramStatus  (Optional)"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.FexitProgram","title":"FexitProgram","text":"<p> <p>FexitProgram is the Schema for the FexitPrograms API</p> </p> Field Description <code>apiVersion</code> string <code> bpfman.io/v1alpha1 </code> <code>kind</code> string  <code>FexitProgram</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta   Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  FexitProgramSpec  <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>func_name</code>  string  <p>Function to attach the fexit to.</p> <code>status</code>  FexitProgramStatus  (Optional)"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.KprobeProgram","title":"KprobeProgram","text":"<p> <p>KprobeProgram is the Schema for the KprobePrograms API</p> </p> Field Description <code>apiVersion</code> string <code> bpfman.io/v1alpha1 </code> <code>kind</code> string  <code>KprobeProgram</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta   Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  KprobeProgramSpec  <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>func_name</code>  string  <p>Functions to attach the kprobe to.</p> <code>offset</code>  uint64  (Optional) <p>Offset added to the address of the function for kprobe. Not allowed for kretprobes.</p> <code>retprobe</code>  bool  (Optional) <p>Whether the program is a kretprobe.  Default is false</p> <code>status</code>  KprobeProgramStatus  (Optional)"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.TcProgram","title":"TcProgram","text":"<p> <p>TcProgram is the Schema for the TcProgram API</p> </p> Field Description <code>apiVersion</code> string <code> bpfman.io/v1alpha1 </code> <code>kind</code> string  <code>TcProgram</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta   Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  TcProgramSpec  <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>interfaceselector</code>  InterfaceSelector  <p>Selector to determine the network interface (or interfaces)</p> <code>priority</code>  int32  <p>Priority specifies the priority of the tc program in relation to other programs of the same type with the same attach point. It is a value from 0 to 1000 where lower values have higher precedence.</p> <code>direction</code>  string  <p>Direction specifies the direction of traffic the tc program should attach to for a given network device.</p> <code>proceedon</code>  []TcProceedOnValue  (Optional) <p>ProceedOn allows the user to call other tc programs in chain on this exit code. Multiple values are supported by repeating the parameter.</p> <code>status</code>  TcProgramStatus  (Optional)"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.TracepointProgram","title":"TracepointProgram","text":"<p> <p>TracepointProgram is the Schema for the TracepointPrograms API</p> </p> Field Description <code>apiVersion</code> string <code> bpfman.io/v1alpha1 </code> <code>kind</code> string  <code>TracepointProgram</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta   Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  TracepointProgramSpec  <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>names</code>  []string  <p>Names refers to the names of kernel tracepoints to attach the bpf program to.</p> <code>status</code>  TracepointProgramStatus  (Optional)"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.UprobeProgram","title":"UprobeProgram","text":"<p> <p>UprobeProgram is the Schema for the UprobePrograms API</p> </p> Field Description <code>apiVersion</code> string <code> bpfman.io/v1alpha1 </code> <code>kind</code> string  <code>UprobeProgram</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta   Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  UprobeProgramSpec  <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>func_name</code>  string  (Optional) <p>Function to attach the uprobe to.</p> <code>offset</code>  uint64  (Optional) <p>Offset added to the address of the function for uprobe.</p> <code>target</code>  string  <p>Library name or the absolute path to a binary or library.</p> <code>retprobe</code>  bool  (Optional) <p>Whether the program is a uretprobe.  Default is false</p> <code>pid</code>  int32  (Optional) <p>Only execute uprobe for given process identification number (PID). If PID is not provided, uprobe executes for all PIDs.</p> <code>containers</code>  ContainerSelector  (Optional) <p>Containers identifes the set of containers in which to attach the uprobe. If Containers is not specified, the uprobe will be attached in the bpfman-agent container.  The ContainerSelector is very flexible and even allows the selection of all containers in a cluster.  If an attempt is made to attach uprobes to too many containers, it can have a negative impact on on the cluster.</p> <code>status</code>  UprobeProgramStatus  (Optional)"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.XdpProgram","title":"XdpProgram","text":"<p> <p>XdpProgram is the Schema for the XdpPrograms API</p> </p> Field Description <code>apiVersion</code> string <code> bpfman.io/v1alpha1 </code> <code>kind</code> string  <code>XdpProgram</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta   Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  XdpProgramSpec  <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>interfaceselector</code>  InterfaceSelector  <p>Selector to determine the network interface (or interfaces)</p> <code>priority</code>  int32  <p>Priority specifies the priority of the bpf program in relation to other programs of the same type with the same attach point. It is a value from 0 to 1000 where lower values have higher precedence.</p> <code>proceedon</code>  []XdpProceedOnValue  <code>status</code>  XdpProgramStatus  (Optional)"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.BpfProgramCommon","title":"BpfProgramCommon","text":"<p> (Appears on: FentryProgramSpec,  FexitProgramSpec,  KprobeProgramSpec,  TcProgramSpec,  TracepointProgramSpec,  UprobeProgramSpec,  XdpProgramSpec) </p> <p> <p>BpfProgramCommon defines the common attributes for all BPF programs</p> </p> Field Description <code>bpffunctionname</code>  string  <p>BpfFunctionName is the name of the function that is the entry point for the BPF program</p> <code>nodeselector</code>  Kubernetes meta/v1.LabelSelector  <p>NodeSelector allows the user to specify which nodes to deploy the bpf program to.  This field must be specified, to select all nodes use standard metav1.LabelSelector semantics and make it empty.</p> <code>bytecode</code>  BytecodeSelector  <p>Bytecode configures where the bpf program\u2019s bytecode should be loaded from.</p> <code>globaldata</code>  map[string][]byte  (Optional) <p>GlobalData allows the user to to set global variables when the program is loaded with an array of raw bytes. This is a very low level primitive. The caller is responsible for formatting the byte string appropriately considering such things as size, endianness, alignment and packing of data structures.</p> <code>mapownerselector</code>  Kubernetes meta/v1.LabelSelector  (Optional) <p>MapOwnerSelector is used to select the loaded eBPF program this eBPF program will share a map with. The value is a label applied to the BpfProgram to select. The selector must resolve to exactly one instance of a BpfProgram on a given node or the eBPF program will not load.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.BpfProgramConditionType","title":"BpfProgramConditionType (<code>string</code> alias)","text":"<p> <p>BpfProgramConditionType is a condition type to indicate the status of a BPF program at the individual node level.</p> </p> Value Description <p>\"BytecodeSelectorError\"</p> <p>BpfProgCondByteCodeError indicates that an error occured when trying to process the bytecode selector.</p> <p>\"Loaded\"</p> <p>BpfProgCondLoaded indicates that the eBPF program was successfully loaded into the kernel on a specific node.</p> <p>\"MapOwnerNotFound\"</p> <p>BpfProgCondMapOwnerNotFound indicates that the eBPF program sharing a map with another eBPF program and that program does not exist.</p> <p>\"MapOwnerNotLoaded\"</p> <p>BpfProgCondMapOwnerNotLoaded indicates that the eBPF program sharing a map with another eBPF program and that program is not loaded.</p> <p>\"NoContainersOnNode\"</p> <p>BpfProgCondNoContainersOnNode indicates that there are no containers on the node that match the container selector.</p> <p>\"None\"</p> <p>None of the above conditions apply</p> <p>\"NotLoaded\"</p> <p>BpfProgCondNotLoaded indicates that the eBPF program has not yet been loaded into the kernel on a specific node.</p> <p>\"NotSelected\"</p> <p>BpfProgCondNotSelected indicates that the eBPF program is not scheduled to be loaded on a specific node.</p> <p>\"NotUnLoaded\"</p> <p>BpfProgCondUnloaded indicates that in the midst of trying to remove the eBPF program from the kernel on the node, that program has not yet been removed.</p> <p>\"Unloaded\"</p> <p>BpfProgCondUnloaded indicates that the eBPF program has been unloaded from the kernel on a specific node.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.BpfProgramSpec","title":"BpfProgramSpec","text":"<p> (Appears on: BpfProgram) </p> <p> <p>BpfProgramSpec defines the desired state of BpfProgram</p> </p> Field Description <code>type</code>  string  (Optional) <p>Type specifies the bpf program type</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.BpfProgramStatus","title":"BpfProgramStatus","text":"<p> (Appears on: BpfProgram) </p> <p> <p>BpfProgramStatus defines the observed state of BpfProgram TODO Make these a fixed set of metav1.Condition.types and metav1.Condition.reasons</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  <p>Conditions houses the updates regarding the actual implementation of the bpf program on the node Known .status.conditions.type are: \u201cAvailable\u201d, \u201cProgressing\u201d, and \u201cDegraded\u201d</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.BytecodeImage","title":"BytecodeImage","text":"<p> (Appears on: BytecodeSelector) </p> <p> <p>BytecodeImage defines how to specify a bytecode container image.</p> </p> Field Description <code>url</code>  string  <p>Valid container image URL used to reference a remote bytecode image.</p> <code>imagepullpolicy</code>  PullPolicy  (Optional) <p>PullPolicy describes a policy for if/when to pull a bytecode image. Defaults to IfNotPresent.</p> <code>imagepullsecret</code>  ImagePullSecretSelector  (Optional) <p>ImagePullSecret is the name of the secret bpfman should use to get remote image repository secrets.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.BytecodeSelector","title":"BytecodeSelector","text":"<p> (Appears on: BpfProgramCommon) </p> <p> <p>BytecodeSelector defines the various ways to reference bpf bytecode objects.</p> </p> Field Description <code>image</code>  BytecodeImage  <p>Image used to specify a bytecode container image.</p> <code>path</code>  string  <p>Path is used to specify a bytecode object via filepath.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.ContainerSelector","title":"ContainerSelector","text":"<p> (Appears on: UprobeProgramSpec) </p> <p> <p>ContainerSelector identifies a set of containers. For example, this can be used to identify a set of containers in which to attach uprobes.</p> </p> Field Description <code>namespace</code>  string  (Optional) <p>Target namespaces.</p> <code>pods</code>  Kubernetes meta/v1.LabelSelector  <p>Target pods. This field must be specified, to select all pods use standard metav1.LabelSelector semantics and make it empty.</p> <code>containernames</code>  []string  (Optional) <p>Name(s) of container(s).  If none are specified, all containers in the pod are selected.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.FentryProgramSpec","title":"FentryProgramSpec","text":"<p> (Appears on: FentryProgram) </p> <p> <p>FentryProgramSpec defines the desired state of FentryProgram</p> </p> Field Description <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>func_name</code>  string  <p>Function to attach the fentry to.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.FentryProgramStatus","title":"FentryProgramStatus","text":"<p> (Appears on: FentryProgram) </p> <p> <p>FentryProgramStatus defines the observed state of FentryProgram</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  <p>Conditions houses the global cluster state for the FentryProgram. The explicit condition types are defined internally.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.FexitProgramSpec","title":"FexitProgramSpec","text":"<p> (Appears on: FexitProgram) </p> <p> <p>FexitProgramSpec defines the desired state of FexitProgram</p> </p> Field Description <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>func_name</code>  string  <p>Function to attach the fexit to.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.FexitProgramStatus","title":"FexitProgramStatus","text":"<p> (Appears on: FexitProgram) </p> <p> <p>FexitProgramStatus defines the observed state of FexitProgram</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  <p>Conditions houses the global cluster state for the FexitProgram. The explicit condition types are defined internally.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.ImagePullSecretSelector","title":"ImagePullSecretSelector","text":"<p> (Appears on: BytecodeImage) </p> <p> <p>ImagePullSecretSelector defines the name and namespace of an image pull secret.</p> </p> Field Description <code>name</code>  string  <p>Name of the secret which contains the credentials to access the image repository.</p> <code>namespace</code>  string  <p>Namespace of the secret which contains the credentials to access the image repository.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.InterfaceSelector","title":"InterfaceSelector","text":"<p> (Appears on: TcProgramSpec,  XdpProgramSpec) </p> <p> <p>InterfaceSelector defines interface to attach to.</p> </p> Field Description <code>interfaces</code>  []string  (Optional) <p>Interfaces refers to a list of network interfaces to attach the BPF program to.</p> <code>primarynodeinterface</code>  bool  (Optional) <p>Attach BPF program to the primary interface on the node. Only \u2018true\u2019 accepted.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.KprobeProgramSpec","title":"KprobeProgramSpec","text":"<p> (Appears on: KprobeProgram) </p> <p> <p>KprobeProgramSpec defines the desired state of KprobeProgram</p> </p> Field Description <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>func_name</code>  string  <p>Functions to attach the kprobe to.</p> <code>offset</code>  uint64  (Optional) <p>Offset added to the address of the function for kprobe. Not allowed for kretprobes.</p> <code>retprobe</code>  bool  (Optional) <p>Whether the program is a kretprobe.  Default is false</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.KprobeProgramStatus","title":"KprobeProgramStatus","text":"<p> (Appears on: KprobeProgram) </p> <p> <p>KprobeProgramStatus defines the observed state of KprobeProgram</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  <p>Conditions houses the global cluster state for the KprobeProgram. The explicit condition types are defined internally.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.ProgramConditionType","title":"ProgramConditionType (<code>string</code> alias)","text":"<p> <p>ProgramConditionType is a condition type to indicate the status of a BPF program at the cluster level.</p> </p> Value Description <p>\"DeleteError\"</p> <p>ProgramDeleteError indicates that the BPF program was marked for deletion, but deletion was unsuccessful.</p> <p>\"NotYetLoaded\"</p> <p>ProgramNotYetLoaded indicates that the program in question has not yet been loaded on all nodes in the cluster.</p> <p>\"ReconcileError\"</p> <p>ProgramReconcileError indicates that an unforseen situation has occurred in the controller logic, and the controller will retry.</p> <p>\"ReconcileSuccess\"</p> <p>BpfmanProgConfigReconcileSuccess indicates that the BPF program has been successfully reconciled.</p> <p>TODO: we should consider removing \u201creconciled\u201d type logic from the public API as it\u2019s an implementation detail of our use of controller runtime, but not necessarily relevant to human users or integrations.</p> <p>See: https://github.com/bpfman/bpfman/issues/430</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.PullPolicy","title":"PullPolicy (<code>string</code> alias)","text":"<p> (Appears on: BytecodeImage) </p> <p> <p>PullPolicy describes a policy for if/when to pull a container image</p> </p> Value Description <p>\"Always\"</p> <p>PullAlways means that bpfman always attempts to pull the latest bytecode image. Container will fail If the pull fails.</p> <p>\"IfNotPresent\"</p> <p>PullIfNotPresent means that bpfman pulls if the image isn\u2019t present on disk. Container will fail if the image isn\u2019t present and the pull fails.</p> <p>\"Never\"</p> <p>PullNever means that bpfman never pulls an image, but only uses a local image. Container will fail if the image isn\u2019t present</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.TcProceedOnValue","title":"TcProceedOnValue (<code>string</code> alias)","text":"<p> (Appears on: TcProgramSpec) </p> <p> </p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.TcProgramSpec","title":"TcProgramSpec","text":"<p> (Appears on: TcProgram) </p> <p> <p>TcProgramSpec defines the desired state of TcProgram</p> </p> Field Description <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>interfaceselector</code>  InterfaceSelector  <p>Selector to determine the network interface (or interfaces)</p> <code>priority</code>  int32  <p>Priority specifies the priority of the tc program in relation to other programs of the same type with the same attach point. It is a value from 0 to 1000 where lower values have higher precedence.</p> <code>direction</code>  string  <p>Direction specifies the direction of traffic the tc program should attach to for a given network device.</p> <code>proceedon</code>  []TcProceedOnValue  (Optional) <p>ProceedOn allows the user to call other tc programs in chain on this exit code. Multiple values are supported by repeating the parameter.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.TcProgramStatus","title":"TcProgramStatus","text":"<p> (Appears on: TcProgram) </p> <p> <p>TcProgramStatus defines the observed state of TcProgram</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  <p>Conditions houses the global cluster state for the TcProgram. The explicit condition types are defined internally.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.TracepointProgramSpec","title":"TracepointProgramSpec","text":"<p> (Appears on: TracepointProgram) </p> <p> <p>TracepointProgramSpec defines the desired state of TracepointProgram</p> </p> Field Description <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>names</code>  []string  <p>Names refers to the names of kernel tracepoints to attach the bpf program to.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.TracepointProgramStatus","title":"TracepointProgramStatus","text":"<p> (Appears on: TracepointProgram) </p> <p> <p>TracepointProgramStatus defines the observed state of TracepointProgram</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  <p>Conditions houses the global cluster state for the TracepointProgram. The explicit condition types are defined internally.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.UprobeProgramSpec","title":"UprobeProgramSpec","text":"<p> (Appears on: UprobeProgram) </p> <p> <p>UprobeProgramSpec defines the desired state of UprobeProgram</p> </p> Field Description <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>func_name</code>  string  (Optional) <p>Function to attach the uprobe to.</p> <code>offset</code>  uint64  (Optional) <p>Offset added to the address of the function for uprobe.</p> <code>target</code>  string  <p>Library name or the absolute path to a binary or library.</p> <code>retprobe</code>  bool  (Optional) <p>Whether the program is a uretprobe.  Default is false</p> <code>pid</code>  int32  (Optional) <p>Only execute uprobe for given process identification number (PID). If PID is not provided, uprobe executes for all PIDs.</p> <code>containers</code>  ContainerSelector  (Optional) <p>Containers identifes the set of containers in which to attach the uprobe. If Containers is not specified, the uprobe will be attached in the bpfman-agent container.  The ContainerSelector is very flexible and even allows the selection of all containers in a cluster.  If an attempt is made to attach uprobes to too many containers, it can have a negative impact on on the cluster.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.UprobeProgramStatus","title":"UprobeProgramStatus","text":"<p> (Appears on: UprobeProgram) </p> <p> <p>UprobeProgramStatus defines the observed state of UprobeProgram</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  <p>Conditions houses the global cluster state for the UprobeProgram. The explicit condition types are defined internally.</p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.XdpProceedOnValue","title":"XdpProceedOnValue (<code>string</code> alias)","text":"<p> (Appears on: XdpProgramSpec) </p> <p> </p>"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.XdpProgramSpec","title":"XdpProgramSpec","text":"<p> (Appears on: XdpProgram) </p> <p> <p>XdpProgramSpec defines the desired state of XdpProgram</p> </p> Field Description <code>BpfProgramCommon</code>  BpfProgramCommon  <p> (Members of <code>BpfProgramCommon</code> are embedded into this type.) </p> <code>interfaceselector</code>  InterfaceSelector  <p>Selector to determine the network interface (or interfaces)</p> <code>priority</code>  int32  <p>Priority specifies the priority of the bpf program in relation to other programs of the same type with the same attach point. It is a value from 0 to 1000 where lower values have higher precedence.</p> <code>proceedon</code>  []XdpProceedOnValue"},{"location":"developer-guide/api-spec/#bpfman.io/v1alpha1.XdpProgramStatus","title":"XdpProgramStatus","text":"<p> (Appears on: XdpProgram) </p> <p> <p>XdpProgramStatus defines the observed state of XdpProgram</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  <p>Conditions houses the global cluster state for the XdpProgram. The explicit condition types are defined internally.</p> <p> Generated with <code>gen-crd-api-reference-docs</code>. </p>"},{"location":"developer-guide/configuration/","title":"Configuration","text":""},{"location":"developer-guide/configuration/#bpfman-configuration-file","title":"bpfman Configuration File","text":"<p>bpfman looks for a configuration file to be present at <code>/etc/bpfman/bpfman.toml</code>. If no file is found, defaults are assumed. There is an example at <code>scripts/bpfman.toml</code>, similar to:</p> <pre><code>[interfaces]\n  [interface.eth0]\n  xdp_mode = \"hw\" # Valid xdp modes are \"hw\", \"skb\" and \"drv\". Default: \"skb\".\n\n[signing]\nallow_unsigned = true\n\n[database]\nmax_retries = 10\nmillisec_delay = 1000\n</code></pre>"},{"location":"developer-guide/configuration/#config-section-interfaces","title":"Config Section: [interfaces]","text":"<p>This section of the configuration file allows the XDP Mode for a given interface to be set. If not set, the default value of <code>skb</code> will be used. Multiple interfaces can be configured.</p> <pre><code>[interfaces]\n  [interfaces.eth0]\n  xdp_mode = \"drv\"\n  [interfaces.eth1]\n  xdp_mode = \"hw\"\n  [interfaces.eth2]\n  xdp_mode = \"skb\"\n</code></pre> <p>Valid fields:</p> <ul> <li>xdp_mode: XDP Mode for a given interface. Valid values: [\"drv\"|\"hw\"|\"skb\"]</li> </ul>"},{"location":"developer-guide/configuration/#config-section-signing","title":"Config Section: [signing]","text":"<p>This section of the configuration file allows control over whether OCI packaged eBPF bytecode as container images are required to be signed via cosign or not. By default, unsigned images are allowed. See eBPF Bytecode Image Specifications for more details on building and shipping bytecode in a container image.</p> <p>Valid fields:</p> <ul> <li>allow_unsigned: Flag indicating whether unsigned images are allowed or not.   Valid values: [\"true\"|\"false\"]</li> </ul>"},{"location":"developer-guide/configuration/#config-section-database","title":"Config Section: [database]","text":"<p><code>bpfman</code> uses an embedded database to store state and persistent data on disk which can only be accessed synchronously by a single process at a time. To avoid returning database lock errors and enhance the user experience, bpfman performs retries when opening of the database. The number of retries and the time between retries is configurable.</p> <p>Valid fields:</p> <ul> <li>max_retries: The number of times to retry opening the database on a given request.</li> <li>millisec_delay: Time in milliseconds to wait between retry attempts.</li> </ul>"},{"location":"developer-guide/debugging/","title":"Debugging using VSCode and lldb on a remote machine or VM","text":"<ol> <li>Install code-lldb vscode extension</li> <li> <p>Add a configuration to <code>.vscode/launch.json</code> like the following (customizing for a given system using the comment in the configuration file):</p> <pre><code>    {\n        \"name\": \"Remote debug bpfman\",\n        \"type\": \"lldb\",\n        \"request\": \"launch\",\n        \"program\": \"&lt;ABSOLUTE_PATH&gt;/github.com/bpfman/bpfman/target/debug/bpfman\", // Local path to latest debug binary.\n        \"initCommands\": [\n            \"platform select remote-linux\", // Execute `platform list` for a list of available remote platform plugins.\n            \"platform connect connect://&lt;IP_ADDRESS_OF_VM&gt;:8175\", // replace &lt;IP_ADDRESS_OF_VM&gt;\n            \"settings set target.inherit-env false\",\n        ],\n        \"env\": {\n            \"RUST_LOG\": \"debug\"\n        },\n        \"cargo\": {\n            \"args\": [\n                \"build\",\n                \"--bin=bpfman\",\n                \"--package=bpfman\"\n            ],\n            \"filter\": {\n                \"name\": \"bpfman\",\n                \"kind\": \"bin\"\n            }\n        },\n        \"cwd\": \"${workspaceFolder}\",\n    },\n</code></pre> </li> <li> <p>On the VM or Server install <code>lldb-server</code>:</p> <p><code>dnf</code> based OS: <pre><code>    sudo dnf install lldb\n</code></pre></p> <p><code>apt</code> based OS:</p> <pre><code>    sudo apt install lldb\n</code></pre> </li> <li> <p>Start <code>lldb-server</code> on the VM or Server (make sure to do this in the <code>~/home</code> directory)</p> <pre><code>    cd ~\n    sudo lldb-server platform --server --listen 0.0.0.0:8081\n</code></pre> </li> <li> <p>Add breakpoints as needed via the vscode GUI and then hit <code>F5</code> to start debugging!</p> </li> </ol>"},{"location":"developer-guide/develop-operator/","title":"Developing the bpfman-operator","text":"<p>This section is intended to give developer level details regarding the layout and design of the bpfman-operator. At its core the operator was implemented using the operator-sdk framework which make those docs another good resource if anything is missed here.</p>"},{"location":"developer-guide/develop-operator/#high-level-design-overview","title":"High level design overview","text":"<p>This repository houses two main processes, the <code>bpfman-agent</code> and the <code>bpfman-operator</code> along with CRD api definitions for <code>BpfProgram</code> and <code>*Program</code> Objects. The following diagram depicts how all these components work together to create a functioning operator.</p> <p></p>"},{"location":"developer-guide/develop-operator/#building-and-deploying","title":"Building and Deploying","text":"<p>For building and deploying the bpfman-operator simply see the attached <code>make help</code> output.</p> <pre><code>make help\n\nUsage:\n  make &lt;target&gt;\n\nGeneral\n  help             Display this help.\n\nLocal Dependencies\n  kustomize        Download kustomize locally if necessary.\n  controller-gen   Download controller-gen locally if necessary.\n  register-gen     Download register-gen locally if necessary.\n  informer-gen     Download informer-gen locally if necessary.\n  lister-gen       Download lister-gen locally if necessary.\n  client-gen       Download client-gen locally if necessary.\n  envtest          Download envtest-setup locally if necessary.\n  opm              Download opm locally if necessary.\n\nDevelopment\n  manifests        Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.\n  generate         Generate ALL auto-generated code.\n  generate-register  Generate register code see all `zz_generated.register.go` files.\n  generate-deepcopy  Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations see all `zz_generated.register.go` files.\n  generate-typed-clients  Generate typed client code\n  generate-typed-listers  Generate typed listers code\n  generate-typed-informers  Generate typed informers code\n  fmt              Run go fmt against code.\n  verify           Verify all the autogenerated code\n  test             Run Unit tests.\n  test-integration  Run Integration tests.\n  bundle           Generate bundle manifests and metadata, then validate generated files.\n  build-release-yamls  Generate the crd install bundle for a specific release version.\n\nBuild\n  build            Build bpfman-operator and bpfman-agent binaries.\n  build-images     Build bpfman, bpfman-agent, and bpfman-operator images.\n  push-images      Push bpfman, bpfman-agent, bpfman-operator images.\n  load-images-kind  Load bpfman, bpfman-agent, and bpfman-operator images into the running local kind devel cluster.\n  bundle-build     Build the bundle image.\n  bundle-push      Push the bundle image.\n  catalog-build    Build a catalog image.\n  catalog-push     Push a catalog image.\n\nCRD Deployment\n  install          Install CRDs into the K8s cluster specified in ~/.kube/config.\n  uninstall        Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n\nVanilla K8s Deployment\n  setup-kind       Setup Kind cluster\n  deploy           Deploy bpfman-operator to the K8s cluster specified in ~/.kube/config with the csi driver initialized.\n  undeploy         Undeploy bpfman-operator from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n  kind-reload-images  Reload locally build images into a kind cluster and restart the ds and deployment so they're picked up.\n  run-on-kind      Kind Deploy runs the bpfman-operator on a local kind cluster using local builds of bpfman, bpfman-agent, and bpfman-operator\n\nOpenshift Deployment\n  deploy-openshift  Deploy bpfman-operator to the Openshift cluster specified in ~/.kube/config.\n  undeploy-openshift  Undeploy bpfman-operator from the Openshift cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n</code></pre>"},{"location":"developer-guide/develop-operator/#project-layout","title":"Project Layout","text":"<p>The bpfman-operator project layout is guided by the recommendations from both the operator-sdk framework and the standard golang project-layout. The following is a brief description of the main directories under <code>bpfman-operator/</code> and their contents.</p> <p>NOTE: Bolded directories contain auto-generated code</p> <ul> <li><code>apis/v1alpha1/*_types.go</code>: Contains the K8s CRD api definitions (<code>*_types.go</code>) for each version.</li> <li>apis/v1alpha1/zz_generated.*.go: Contains the auto-generated register (<code>zz_generate.register.go</code>)   and deepcopy (<code>zz_generated.deepcopy.go</code>) methods.</li> <li><code>bundle/</code>: Contains the OLM bundle manifests and metadata for the operator.   More details can be found in the operator-sdk documentation.</li> <li><code>cmd/</code>: Contains the main entry-points for the bpfman-operator and bpfman-agent processes.</li> <li><code>config/</code>: Contains the configuration files for launching the bpfman-operator on a cluster.<ul> <li><code>bpfman-deployment/</code>: Contains static deployment yamls for the bpfman-daemon, this includes two containers,   one for <code>bpfman</code> and the other for the <code>bpfman-agent</code>.   This DaemonSet yaml is NOT deployed statically by kustomize, instead it's statically copied into the operator   image which is then responsible for deploying and configuring the bpfman-daemon DaemonSet.   Lastly, this directory also contains the default config used to configure the bpfman-daemon, along with the   cert-manager certificates used to encrypt communication between the bpfman-agent and bpfman.</li> <li><code>bpfman-operator-deployment/</code>: Contains the static deployment yaml for the bpfman-operator.   This is deployed statically by kustomize.</li> <li><code>crd/</code>: Contains the CRD manifests for all of the bpfman-operator APIs.<ul> <li>bases/: Is where the actual CRD definitions are stored. These definitions are auto-generated by controller-gen.</li> <li><code>patches/</code>: Contains kustomize patch files for each Program Type, which enables a conversion webhook for    the CRD and adds a directive for certmanager to inject CA into the CRD.</li> </ul> </li> <li><code>default/</code>: Contains the default deployment configuration for the bpfman-operator.</li> <li><code>manifests/</code>: Contains the bases for generating OLM manifests.</li> <li><code>openshift/</code>: Contains the Openshift specific deployment configuration for the bpfman-operator.</li> <li><code>prometheus/</code>: Contains the prometheus manifests used to deploy Prometheus to a cluster.   At the time of writing this the bpfman-operator is NOT exposing any metrics to prometheus, but this is a future goal.</li> <li>rbac/: Contains rbac yamls for getting bpfman and the bpfman-operator up and running on Kubernetes.<ul> <li>bpfman-agent/: Contains the rbac yamls for the bpfman-agent. They are automatically generated by kubebuilder via build tags in the bpfman-agent controller code.</li> <li>bpfman-operator/: Contains the rbac yamls for the bpfman-operator. They are automatically generated by kubebuilder via build tags in the bpfman-operator controller code.</li> </ul> </li> <li><code>samples/</code>: Contains sample CR definitions that can be deployed by users for each of our supported APIs.</li> <li><code>scorecard/</code>: Contains the scorecard manifests used to deploy scorecard to a cluster. At the time of writing   this the bpfman-operator is NOT running any scorecard tests.</li> <li><code>test/</code>: Contains the test manifests used to deploy the bpfman-operator to a kind cluster for integration testing.</li> </ul> </li> <li><code>controllers/</code>: Contains the controller implementations for all of the bpfman-operator APIs.   Each controller is responsible for reconciling the state of the cluster with the desired state defined by the user.   This is where the source of truth for the auto-generated RBAC can be found, keep an eye out for   <code>//+kubebuilder:rbac:groups=bpfman.io</code> comment tags.<ul> <li><code>bpfmanagent/</code>: Contains the controller implementations which reconcile user created <code>*Program</code> types to multiple   <code>BpfProgram</code> objects.</li> <li><code>bpfmanoperator/</code>: Contains the controller implementations which reconcile global <code>BpfProgram</code> object state back to   the user by ensuring the user created <code>*Program</code> objects are reporting the correct status.</li> </ul> </li> <li><code>hack/</code>: Contains any scripts+static files used by the bpfman-operator to facilitate development.</li> <li><code>internal/</code>: Contains all private library code and is used by the bpfman-operator and bpfman-agent controllers.</li> <li><code>pkg/</code>: Contains all public library code this is consumed externally and internally.<ul> <li>client/: Contains the autogenerated clientset, informers and listers for all of the bpfman-operator APIs.   These are autogenerated by the k8s.io/code-generator project,   and can be consumed by users wishing to programmatically interact with bpfman specific APIs.</li> <li><code>helpers/</code>: Contains helper functions which can be consumed by users wishing to programmatically interact with   bpfman specific APIs.</li> </ul> </li> <li><code>test/integration/</code>: Contains integration tests for the bpfman-operator.   These tests are run against a kind cluster and are responsible for testing the bpfman-operator in a real cluster   environment.   It uses the kubernetes-testing-framework project to   programmatically spin-up all of the required infrastructure for our unit tests.</li> <li><code>Makefile</code>: Contains all of the make targets used to build, test, and generate code used by the bpfman-operator.</li> </ul>"},{"location":"developer-guide/develop-operator/#rpc-protobuf-generation","title":"RPC Protobuf Generation","text":"<p>Technically part of the <code>bpfman</code> API, the RPC Protobufs are usually not coded until a bpfman feature is integrated into the <code>bpfman-operator</code> and <code>bpfman-agent</code> code. To modify the RPC Protobuf definition, edit proto/bpfman.proto. Then to generate the protobufs from the updated RPC Protobuf definitions:</p> <pre><code>cd bpfman/\ncargo xtask build-proto\n</code></pre> <p>This will generate:</p> <ul> <li>bpfman-api/src/bpfman.v1.rs: Generated Rust Protobuf source code.</li> <li>clients/gobpfman/v1/: Directory that contains the generated Go Client code for interacting   with bpfman over RPC from a Go application.</li> </ul> <p>When editing  proto/bpfman.proto, follow best practices describe in Proto Best Practices.</p> <p>Note: <code>cargo xtask build-proto</code> also pulls in  proto/csi.proto (which is in the same directory as proto/bpfman.proto). proto/csi.proto is taken from container-storage-interface/spec/csi.proto. See container-storage-interface/spec/spec.md for more details.</p>"},{"location":"developer-guide/develop-operator/#generated-files","title":"Generated Files","text":"<p>The operator-sdk framework will generate multiple categories of files (Custom Resource Definitions (CRD), RBAC ClusterRole, Webhook Configuration, typed client, listeners and informers code, etc). If any of the bpfman-operator/apis/v1alpha1/*Program_types.go files are modified, then regenerate these files using:</p> <pre><code>cd bpfman/bpfman-operator/\nmake generate\n</code></pre> <p>This command will generate all auto-generated code. There are commands to generate each sub-category if needed. See <code>make help</code> to list all the generate commands.</p>"},{"location":"developer-guide/develop-operator/#building","title":"Building","text":"<p>To run in Kubernetes, bpfman components need to be containerized. However, building container images can take longer than just building the code. During development, it may be quicker to find and fix build errors by just building the code. To build the code:</p> <pre><code>cd bpfman/bpfman-operator/\nmake build\n</code></pre> <p>To build the container images, run the following command:</p> <pre><code>cd bpfman/bpfman-operator/\nmake build-images\n</code></pre> <p>If the <code>make build</code> command is skipped above, the code will be built in the build-images command. If the <code>make build</code> command is run, the built code will be leveraged in this step. This command generates the following images:</p> <pre><code>docker images\nREPOSITORY                       TAG      IMAGE ID       CREATED          SIZE\nquay.io/bpfman/bpfman            latest   69df038ccea3   43 seconds ago   515MB\nquay.io/bpfman/bpfman-agent      latest   f6af33c5925b   2 minutes ago    464MB\nquay.io/bpfman/bpfman-operator   latest   4fe444b7abf1   2 minutes ago    141MB\n:\n</code></pre>"},{"location":"developer-guide/develop-operator/#running-locally-in-kind","title":"Running Locally in KIND","text":"<p>Deploying the bpfman-operator goes into more detail on ways to launch bpfman in a Kubernetes cluster. To run locally in a Kind cluster with an up to date build simply run:</p> <pre><code>cd bpfman/bpfman-operator/\nmake run-on-kind\n</code></pre> <p>The <code>make run-on-kind</code> will run the <code>make build-images</code> if the images do not exist or need updating.</p> <p>Then rebuild and load a fresh build run:</p> <pre><code>cd bpfman/bpfman-operator/\nmake build-images\nmake kind-reload-images\n</code></pre> <p>Which will rebuild the bpfman-operator, bpfman-agent, and bpfman images and load them into the kind cluster.</p> <p>By default, the <code>make run-on-kind</code> uses the <code>quay.io/bpfman/bpfman*</code> images described above. The container images used for <code>bpfman</code>, <code>bpfman-agent</code>, and <code>bpfman-operator</code> can also be manually configured:</p> <pre><code>BPFMAN_IMG=&lt;your/image/url&gt; BPFMAN_AGENT_IMG=&lt;your/image/url&gt; BPFMAN_OPERATOR_IMG=&lt;your/image/url&gt; make run-on-kind\n</code></pre>"},{"location":"developer-guide/develop-operator/#testing-locally","title":"Testing Locally","text":"<p>See Kubernetes Operator Tests. </p>"},{"location":"developer-guide/develop-operator/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer-guide/develop-operator/#metricshealth-port-issues","title":"Metrics/Health port issues","text":"<p>In some scenarios, the health and metric ports may are already in use by other services on the system. When this happens the bpfman-agent container fails to deploy. The ports currently default to 8175 and 8174.</p> <p>The ports are passed in through the daemonset.yaml for the <code>bpfman-daemon</code> and deployment.yaml and manager_auth_proxy_patch.yaml for the <code>bpfman-operator</code>. The easiest way to change which ports are used is to update these yaml files and rebuild the container images. The container images need to be rebuilt because the <code>bpfman-daemon</code> is deployed from the <code>bpfman-operator</code> and the associated yaml files are copied into the <code>bpfman-operator</code> image.</p> <p>If rebuild the container images is not desirable, then the ports can be changed on the fly. For the <code>bpfman-operator</code>, the ports can be updated by editing the <code>bpfman-operator</code> Deployment.</p> <pre><code>kubectl edit deployment -n bpfman bpfman-operator\n\napiVersion: apps/v1\nkind: Deployment\n:\nspec:\n  template:\n  :\n  spec:\n    containers:\n    -args:\n      - --secure-listen-address=0.0.0.0:8443\n      - --upstream=http://127.0.0.1:8174/        &lt;-- UPDATE\n      - --logtostderr=true\n      - --v=0\n      name: kube-rbac-proxy\n      :\n    - args:\n      - --health-probe-bind-address=:8175        &lt;-- UPDATE\n      - --metrics-bind-address=127.0.0.1:8174    &lt;-- UPDATE\n      - --leader-elect\n      :\n      livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 8175                           &lt;-- UPDATE\n            scheme: HTTP\n            :\n      name: bpfman-operator\n      readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: 8175                           &lt;-- UPDATE\n            scheme: HTTP\n      :\n</code></pre> <p>For the <code>bpfman-daemon</code>, the ports could be updated by editing the <code>bpfman-daemon</code> DaemonSet. However, if <code>bpfman-daemon</code> is restarted for any reason by the <code>bpfman-operator</code>, the changes will be lost. So it is recommended to update the ports for the <code>bpfman-daemon</code> via the bpfman <code>bpfman-config</code> ConfigMap.</p> <pre><code>kubectl edit configmap -n bpfman bpfman-config\n\napiVersion: v1\ndata:\n  bpfman.agent.healthprobe.addr: :8175                    &lt;-- UPDATE\n  bpfman.agent.image: quay.io/bpfman/bpfman-agent:latest\n  bpfman.agent.log.level: info\n  bpfman.agent.metric.addr: 127.0.0.1:8174                &lt;-- UPDATE\n  bpfman.image: quay.io/bpfman/bpfman:latest\n  bpfman.log.level: debug\nkind: ConfigMap\n:\n</code></pre>"},{"location":"developer-guide/documentation/","title":"Documentation","text":"<p>This section describes how to modify the related documentation around bpfman. All bpfman's documentation is written in Markdown, and leverages mkdocs to generate a static site, which is hosted on netlify.</p> <p>If this is the first time building using <code>mkdocs</code>, jump to the Development Environment Setup section for help installing the tooling.</p>"},{"location":"developer-guide/documentation/#documentation-notes","title":"Documentation Notes","text":"<p>This section describes some notes on the dos and don'ts when writing documentation.</p>"},{"location":"developer-guide/documentation/#website-management","title":"Website Management","text":"<p>The headings and layout of the website, as well as other configuration settings, are managed from the mkdocs.yml file in the project root directory.</p>"},{"location":"developer-guide/documentation/#markdown-style","title":"Markdown Style","text":"<p>When writing documentation via a Markdown file, the following format has been followed:</p> <ul> <li>Text on a given line should not exceed 100 characters, unless it's example syntax or a link   that should be broken up.</li> <li>Each new sentence should start on a new line.   That way, if text needs to be inserted, whole paragraphs don't need to be adjusted.</li> <li>Links to other markdown files are relative to the file the link is placed in.</li> </ul>"},{"location":"developer-guide/documentation/#governance-files","title":"Governance Files","text":"<p>There are a set of well known governance files that are typically placed in the root directory of most projects, like README.md, MAINTAINERS.md, CONTRIBUTING.md, etc. <code>mkdocs</code> expects all files used in the static website to be located under a common directory, <code>docs/</code> for bpfman. To reference the governance files from the static website, a directory (<code>docs/governance/</code>) was created with a file for each governance file, the only contains <code>--8&lt;--</code> and the file name. This indicates to <code>mkdocs</code> to pull the additional file from the project root directory.</p> <p>For example: docs/governance/MEETINGS.md</p> <p>NOTE: This works for the website generation, but if a Markdown file is viewed through   Github (not the website), the link is broken.   So these files should only be linked from <code>docs/index.md</code> and <code>mkdocs.yml</code>.</p>"},{"location":"developer-guide/documentation/#docsdeveloper-guideapi-specmd","title":"docs/developer-guide/api-spec.md","text":"<p>The file docs/developer-guide/api-spec.md documents the CRDs used in a Kubernetes deployment. The contents are auto-generated when PRs are pushed to Github.</p> <p>The contents can be generated locally by running the command <code>make -C bpfman-operator apidocs.html</code> from the root bpfman directory.</p>"},{"location":"developer-guide/documentation/#generate-documentation","title":"Generate Documentation","text":"<p>If you would like to test locally, build and preview the generated documentation, from the bpfman root directory, use <code>mkdocs</code> to build:</p> <pre><code>cd bpfman/\nmkdocs build\n</code></pre> <p>NOTE: If <code>mkdocs build</code> gives you an error, make sure you have the mkdocs packages listed below installed.</p> <p>To preview from a build on a local machine, start the mkdocs dev-server with the command below, then open up <code>http://127.0.0.1:8000/</code> in your browser, and you'll see the default home page being displayed:</p> <pre><code>mkdocs serve\n</code></pre> <p>To preview from a build on a remote machine, start the mkdocs dev-server with the command below, then open up <code>http://&lt;ServerIP&gt;:8000/</code> in your browser, and you'll see the default home page being displayed:</p> <pre><code>mkdocs serve -a 0.0.0.0:8000\n</code></pre>"},{"location":"developer-guide/documentation/#development-environment-setup","title":"Development Environment Setup","text":"<p>The recommended installation method is using <code>pip</code>.</p> <pre><code>pip install -r requirements.txt \n</code></pre> <p>Once installed, ensure the <code>mkdocs</code> is in your PATH:</p> <pre><code>mkdocs -V\nmkdocs, version 1.4.3 from /home/$USER/.local/lib/python3.11/site-packages/mkdocs (Python 3.11)\n</code></pre> <p>NOTE: If you have an older version of mkdocs installed, you may need to use the <code>--upgrade</code> option (e.g., <code>pip install --upgrade mkdocs</code>) to get it to work.</p>"},{"location":"developer-guide/documentation/#document-images","title":"Document Images","text":"<p>Source of images used in the example documentation can be found in bpfman Upstream Images. Request access if required.</p>"},{"location":"developer-guide/image-build/","title":"bpfman Container Images","text":"<p>Container images for the <code>bpfman</code> binaries are automatically built and pushed to <code>quay.io/bpfman</code> whenever code is merged into the <code>main</code> branch of the <code>github.com/bpfman/bpfman</code> repository under the <code>:latest</code> tag.</p>"},{"location":"developer-guide/image-build/#building-the-images-locally","title":"Building the images locally","text":""},{"location":"developer-guide/image-build/#bpfman","title":"bpfman","text":"<pre><code>docker build -f /Containerfile.bpfman . -t bpfman:local\n</code></pre>"},{"location":"developer-guide/image-build/#running-locally-in-container","title":"Running locally in container","text":""},{"location":"developer-guide/image-build/#bpfman_1","title":"bpfman","text":"<pre><code>sudo docker run --init --privileged --net=host -v /etc/bpfman/certs/:/etc/bpfman/certs/ -v /sys/fs/bpf:/sys/fs/bpf quay.io/bpfman/bpfman:latest\n</code></pre>"},{"location":"developer-guide/linux-capabilities/","title":"Linux Capabilities","text":"<p>Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled. Capabilities are a per-thread attribute. See capabilities man-page.</p> <p>When <code>bpfman</code> is run as a systemd service, the set of linux capabilities are restricted to only the required set of capabilities via the <code>bpfman.service</code> file using the <code>AmbientCapabilities</code> and <code>CapabilityBoundingSet</code> fields (see bpfman.service). All spawned threads are stripped of all capabilities, removing all sudo privileges (see <code>drop_linux_capabilities()</code> usage), leaving only the main thread with only the needed set of capabilities.</p>"},{"location":"developer-guide/linux-capabilities/#current-bpfman-linux-capabilities","title":"Current bpfman Linux Capabilities","text":"<p>Below are the current set of Linux capabilities required by bpfman to operate:</p> <ul> <li>CAP_BPF:<ul> <li>Required to load BPF programs and create BPF maps.</li> </ul> </li> <li>CAP_DAC_READ_SEARCH:<ul> <li>Required by Tracepoint programs, needed by aya to check the tracefs mount point.   For example, trying to read \"/sys/kernel/tracing\" and \"/sys/kernel/debug/tracing\".</li> </ul> </li> <li>CAP_NET_ADMIN:<ul> <li>Required for TC programs to attach/detach to/from a qdisc.</li> </ul> </li> <li>CAP_SETPCAP:<ul> <li>Required to allow bpfman to drop Linux Capabilities on spawned threads.</li> </ul> </li> <li>CAP_SYS_ADMIN: <ul> <li>Kprobe (Kprobe and Uprobe) and Tracepoint programs are considered perfmon programs and require CAP_PERFMON and CAP_SYS_ADMIN to load.</li> <li>TC and XDP programs are considered admin programs and require CAP_NET_ADMIN and CAP_SYS_ADMIN to load.</li> </ul> </li> <li>CAP_SYS_RESOURCE:<ul> <li>Required by bpfman to call <code>setrlimit()</code> on <code>RLIMIT_MEMLOCK</code>.</li> </ul> </li> </ul>"},{"location":"developer-guide/linux-capabilities/#debugging-linux-capabilities","title":"Debugging Linux Capabilities","text":"<p>As new features are added, the set of Linux capabilities required by bpfman may change over time. The following describes the steps to determine the set of capabilities required by bpfman. If there are any <code>Permission denied (os error 13)</code> type errors when starting or running bpfman as a systemd service, adjusting the linux capabilities is a good place to start.</p>"},{"location":"developer-guide/linux-capabilities/#determine-required-capabilities","title":"Determine Required Capabilities","text":"<p>The first step is to turn all capabilities on and see if that fixes the problem. This can be done without recompiling the code by editing <code>bpfman.service</code>. Comment out the finite list of granted capabilities and set to <code>~</code>,  which indicates all capabilities.</p> <pre><code>sudo vi /usr/lib/systemd/system/bpfman.service\n:\n[Service]\n:\nAmbientCapabilities=~\nCapabilityBoundingSet=~\n#AmbientCapabilities=CAP_BPF CAP_DAC_OVERRIDE CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SETPCAP CAP_SYS_ADMIN CAP_SYS_RESOURCE\n#CapabilityBoundingSet=CAP_BPF CAP_DAC_OVERRIDE CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SETPCAP CAP_SYS_ADMIN CAP_SYS_RESOURCE\n</code></pre> <p>Reload the service file and start/restart bpfman and watch the bpfman logs and see if the problem is resolved:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl start bpfman\n</code></pre> <p>If so, then the next step is to watch the set of capabilities being requested by bpfman. Run the bcc <code>capable</code> tool to watch capabilities being requested real-time and restart bpfman:</p> <pre><code>$ sudo /usr/share/bcc/tools/capable\nTIME      UID    PID    COMM             CAP  NAME                 AUDIT\n:\n16:36:00  979    75553  tokio-runtime-w  8    CAP_SETPCAP          1\n16:36:00  979    75553  tokio-runtime-w  8    CAP_SETPCAP          1\n16:36:00  979    75553  tokio-runtime-w  8    CAP_SETPCAP          1\n16:36:00  0      616    systemd-journal  19   CAP_SYS_PTRACE       1\n16:36:00  0      616    systemd-journal  19   CAP_SYS_PTRACE       1\n16:36:00  979    75550  bpfman             24   CAP_SYS_RESOURCE     1\n16:36:00  979    75550  bpfman             1    CAP_DAC_OVERRIDE     1\n16:36:00  979    75550  bpfman             21   CAP_SYS_ADMIN        1\n16:36:00  979    75550  bpfman             21   CAP_SYS_ADMIN        1\n16:36:00  0      75555  modprobe         16   CAP_SYS_MODULE       1\n16:36:00  0      628    systemd-udevd    2    CAP_DAC_READ_SEARCH  1\n16:36:00  0      75556  bpf_preload      24   CAP_SYS_RESOURCE     1\n16:36:00  0      75556  bpf_preload      39   CAP_BPF              1\n16:36:00  0      75556  bpf_preload      39   CAP_BPF              1\n16:36:00  0      75556  bpf_preload      39   CAP_BPF              1\n16:36:00  0      75556  bpf_preload      38   CAP_PERFMON          1\n16:36:00  0      75556  bpf_preload      38   CAP_PERFMON          1\n16:36:00  0      75556  bpf_preload      38   CAP_PERFMON          1\n:\n</code></pre> <p>Compare the output to list in <code>bpfman.service</code> and determine the delta.</p>"},{"location":"developer-guide/linux-capabilities/#determine-capabilities-per-thread","title":"Determine Capabilities Per Thread","text":"<p>For additional debugging, it may be helpful to know the granted capabilities on a per thread basis. As mentioned above, all spawned threads are stripped of all Linux capabilities, so if a thread is requesting a capability, that functionality should be moved off the spawned thread and onto the main thread.</p> <p>First, determine the <code>bpfman</code> process id, then determine the set of threads:</p> <pre><code>$ ps -ef | grep bpfman\n:\nbpfman       75550       1  0 16:36 ?        00:00:00 /usr/sbin/bpfman\n:\n\n$ ps -T -p 75550\n    PID    SPID TTY          TIME CMD\n  75550   75550 ?        00:00:00 bpfman\n  75550   75551 ?        00:00:00 tokio-runtime-w\n  75550   75552 ?        00:00:00 tokio-runtime-w\n  75550   75553 ?        00:00:00 tokio-runtime-w\n  75550   75554 ?        00:00:00 tokio-runtime-w\n</code></pre> <p>Then dump the capabilities of each thread:</p> <pre><code>$ grep Cap /proc/75550/status\nCapInh: 000000c001201106\nCapPrm: 000000c001201106\nCapEff: 000000c001201106\nCapBnd: 000000c001201106\nCapAmb: 000000c001201106\n\n$ grep Cap /proc/75551/status\nCapInh: 0000000000000000\nCapPrm: 0000000000000000\nCapEff: 0000000000000000\nCapBnd: 0000000000000000\nCapAmb: 0000000000000000\n\n$ grep Cap /proc/75552/status\nCapInh: 0000000000000000\nCapPrm: 0000000000000000\nCapEff: 0000000000000000\nCapBnd: 0000000000000000\nCapAmb: 0000000000000000\n\n:\n\n$ capsh --decode=000000c001201106\n0x000000c001201106=cap_dac_override,cap_dac_read_search,cap_setpcap,cap_net_admin,cap_sys_admin,cap_sys_resource,cap_perfmon,cap_bpf\n</code></pre>"},{"location":"developer-guide/linux-capabilities/#removing-cap_bpf-from-bpfman-clients","title":"Removing CAP_BPF from bpfman Clients","text":"<p>One of the advantages of using bpfman is that it is doing all the loading and unloading of eBPF programs, so it requires CAP_BPF, but clients of bpfman are just making gRPC calls to bpfman, so they do not need to be privileged or require CAP_BPF. It must be noted that this is only true for kernels 5.19 or higher. Prior to kernel 5.19, all eBPF sys calls required CAP_BPF, which are used to access maps shared between the BFP program and the userspace program. In kernel 5.19, a change went in that only requires CAP_BPF for map creation (BPF_MAP_CREATE) and loading programs (BPF_PROG_LOAD). See bpf: refine kernel.unprivileged_bpf_disabled behaviour.</p>"},{"location":"developer-guide/logging/","title":"Logging","text":"<p>This section describes how to enable logging in different <code>bpfman</code> deployments.</p>"},{"location":"developer-guide/logging/#local-privileged-bpfman-process","title":"Local Privileged Bpfman Process","text":"<p><code>bpfman</code> uses the env_logger crate to log messages to the terminal. By default, only <code>error</code> messages are logged, but that can be overwritten by setting the <code>RUST_LOG</code> environment variable. Valid values:</p> <ul> <li><code>error</code></li> <li><code>warn</code></li> <li><code>info</code></li> <li><code>debug</code></li> <li><code>trace</code></li> </ul> <p>Example:</p> <pre><code>$ sudo RUST_LOG=info /usr/local/bin/bpfman\n[2022-08-08T20:29:31Z INFO  bpfman::server] Loading static programs from /etc/bpfman/programs.d\n[2022-08-08T20:29:31Z INFO  bpfman::server::bpf] Map veth12fa8e3 to 13\n[2022-08-08T20:29:31Z INFO  bpfman::server] Listening on [::1]:50051\n[2022-08-08T20:29:31Z INFO  bpfman::server::bpf] Program added: 1 programs attached to veth12fa8e3\n[2022-08-08T20:29:31Z INFO  bpfman::server] Loaded static program pass with UUID d9fd88df-d039-4e64-9f63-19f3e08915ce\n</code></pre>"},{"location":"developer-guide/logging/#systemd-service","title":"Systemd Service","text":"<p>If <code>bpfman</code> is running as a systemd service, then <code>bpfman</code> will log to journald. As with env_logger, by default, <code>info</code> and higher messages are logged, but that can be overwritten by setting the <code>RUST_LOG</code> environment variable.</p> <p>Example:</p> <pre><code>sudo vi /usr/lib/systemd/system/bpfman.service\n[Unit]\nDescription=Run bpfman as a service\nDefaultDependencies=no\nAfter=network.target\n\n[Service]\nEnvironment=\"RUST_LOG=Info\"    &lt;==== Set Log Level Here\nExecStart=/usr/sbin/bpfman system service\nAmbientCapabilities=CAP_BPF CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SYS_ADMIN CAP_SYS_RESOURCE\nCapabilityBoundingSet=CAP_BPF CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SYS_ADMIN CAP_SYS_RESOURCE\n</code></pre> <p>Start the service:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl start bpfman.service\n</code></pre> <p>Check the logs:</p> <pre><code>$ sudo journalctl -f -u bpfman\nAug 08 16:25:04 ebpf03 systemd[1]: Started bpfman.service - Run bpfman as a service.\nAug 08 16:25:04 ebpf03 bpfman[180118]: Loading static programs from /etc/bpfman/programs.d\nAug 08 16:25:04 ebpf03 bpfman[180118]: Map veth12fa8e3 to 13\nAug 08 16:25:04 ebpf03 bpfman[180118]: Listening on [::1]:50051\nAug 08 16:25:04 ebpf03 bpfman[180118]: Program added: 1 programs attached to veth12fa8e3\nAug 08 16:25:04 ebpf03 bpfman[180118]: Loaded static program pass with UUID a3ffa14a-786d-48ad-b0cd-a4802f0f10b6\n</code></pre> <p>Stop the service:</p> <pre><code>sudo systemctl stop bpfman.service\n</code></pre>"},{"location":"developer-guide/logging/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>When <code>bpfman</code> is run in a Kubernetes deployment, there is the bpfman Daemonset that runs on every node and the bpd Operator that runs on the control plane:</p> <pre><code>kubectl get pods -A\nNAMESPACE            NAME                                                    READY   STATUS    RESTARTS   AGE\nbpfman                 bpfman-daemon-dgqzw                                       2/2     Running   0          3d22h\nbpfman                 bpfman-daemon-gqsgd                                       2/2     Running   0          3d22h\nbpfman                 bpfman-daemon-zx9xr                                       2/2     Running   0          3d22h\nbpfman                 bpfman-operator-7fbf4888c4-z8w76                          2/2     Running   0          3d22h\n:\n</code></pre>"},{"location":"developer-guide/logging/#bpfman-daemonset","title":"bpfman Daemonset","text":"<p><code>bpfman</code> and <code>bpfman-agent</code> are running in the bpfman daemonset.</p>"},{"location":"developer-guide/logging/#view-logs","title":"View Logs","text":"<p>To view the <code>bpfman</code> logs:</p> <pre><code>kubectl logs -n bpfman bpfman-daemon-dgqzw -c bpfman\n[2023-05-05T14:41:26Z INFO  bpfman] Has CAP_BPF: false\n[2023-05-05T14:41:26Z INFO  bpfman] Has CAP_SYS_ADMIN: true\n:\n</code></pre> <p>To view the <code>bpfman-agent</code> logs:</p> <pre><code>kubectl logs -n bpfman bpfman-daemon-dgqzw -c bpfman-agent\n{\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"controller-runtime.metrics\",\"msg\":\"Metrics server is starting to listen\",\"addr\":\":8174\"}\n{\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"setup\",\"msg\":\"Waiting for active connection to bpfman\"}\n{\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"setup\",\"msg\":\"starting Bpfman-Agent\"}\n:\n</code></pre>"},{"location":"developer-guide/logging/#change-log-level","title":"Change Log Level","text":"<p>To change the log level of the agent or daemon, edit the <code>bpfman-config</code> ConfigMap. The <code>bpfman-operator</code> will detect the change and restart the bpfman daemonset with the updated values.</p> <pre><code>kubectl edit configmaps -n bpfman bpfman-config\napiVersion: v1\ndata:\n  bpfman.agent.image: quay.io/bpfman/bpfman-agent:latest\n  bpfman.image: quay.io/bpfman/bpfman:latest\n  bpfman.log.level: info                     &lt;==== Set bpfman Log Level Here\n  bpfman.agent.log.level: info               &lt;==== Set bpfman agent Log Level Here\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2023-05-05T14:41:19Z\"\n  name: bpfman-config\n  namespace: bpfman\n  resourceVersion: \"700803\"\n  uid: 0cc04af4-032c-4712-b824-748b321d319b\n</code></pre> <p>Valid values for the daemon (<code>bpfman.log.level</code>) are:</p> <ul> <li><code>error</code></li> <li><code>warn</code></li> <li><code>info</code></li> <li><code>debug</code></li> <li><code>trace</code></li> </ul> <p><code>trace</code> can be very verbose. More information can be found regarding Rust's env_logger here.</p> <p>Valid values for the agent (<code>bpfman.agent.log.level</code>) are:</p> <ul> <li><code>info</code></li> <li><code>debug</code></li> <li><code>trace</code></li> </ul>"},{"location":"developer-guide/logging/#bpfman-operator","title":"bpfman Operator","text":"<p>The bpfman Operator is running as a Deployment with a ReplicaSet of one. It runs with the containers <code>bpfman-operator</code> and <code>kube-rbac-proxy</code>.</p>"},{"location":"developer-guide/logging/#view-logs_1","title":"View Logs","text":"<p>To view the <code>bpfman-operator</code> logs:</p> <pre><code>kubectl logs -n bpfman bpfman-operator-7fbf4888c4-z8w76 -c bpfman-operator\n{\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"logger\":\"controller-runtime.metrics\",\"msg\":\"Metrics server is starting to listen\",\"addr\":\"127.0.0.1:8174\"}\n{\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"logger\":\"setup\",\"msg\":\"starting manager\"}\n{\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting server\",\"kind\":\"health probe\",\"addr\":\"[::]:8175\"}\n{\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting server\",\"path\":\"/metrics\",\"kind\":\"metrics\",\"addr\":\"127.0.0.1:8174\"}\nI0509 18:37:11.262885       1 leaderelection.go:248] attempting to acquire leader lease bpfman/8730d955.bpfman.io...\nI0509 18:37:11.268918       1 leaderelection.go:258] successfully acquired lease bpfman/8730d955.bpfman.io\n{\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting EventSource\",\"controller\":\"configmap\",\"controllerGroup\":\"\",\"controllerKind\":\"ConfigMap\",\"source\":\"kind source: *v1.ConfigMap\"}\n:\n</code></pre> <p>To view the <code>kube-rbac-proxy</code> logs:</p> <pre><code>kubectl logs -n bpfman bpfman-operator-7fbf4888c4-z8w76 -c kube-rbac-proxy\nI0509 18:37:11.063386       1 main.go:186] Valid token audiences: \nI0509 18:37:11.063485       1 main.go:316] Generating self signed cert as no cert is provided\nI0509 18:37:11.955256       1 main.go:366] Starting TCP socket on 0.0.0.0:8443\nI0509 18:37:11.955849       1 main.go:373] Listening securely on 0.0.0.0:8443\n</code></pre>"},{"location":"developer-guide/logging/#change-log-level_1","title":"Change Log Level","text":"<p>To change the log level, edit the <code>bpfman-operator</code> Deployment. The change will get detected and the bpfman operator pod will get restarted with the updated log level.</p> <pre><code>kubectl edit deployment -n bpfman bpfman-operator\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"1\"\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"manager\",\"app.kubernetes.io/create&gt;\n  creationTimestamp: \"2023-05-09T18:37:08Z\"\n  generation: 1\n:\nspec:\n:\n  template:\n    metadata:\n:\n    spec:\n      containers:\n      - args:\n:\n      - args:\n        - --health-probe-bind-address=:8175\n        - --metrics-bind-address=127.0.0.1:8174\n        - --leader-elect\n        command:\n        - /bpfman-operator\n        env:\n        - name: GO_LOG\n          value: info                   &lt;==== Set Log Level Here\n        image: quay.io/bpfman/bpfman-operator:latest\n        imagePullPolicy: IfNotPresent\n:\n</code></pre> <p>Valid values are:</p> <ul> <li><code>error</code></li> <li><code>info</code></li> <li><code>debug</code></li> <li><code>trace</code></li> </ul>"},{"location":"developer-guide/operator-quick-start/","title":"Deploying the bpfman-operator","text":"<p>The <code>bpfman-operator</code> repository exists in order to deploy and manage bpfman within a Kubernetes cluster. This operator was built utilizing some great tooling provided by the operator-sdk library. A great first step in understanding some of the functionality can be to just run <code>make help</code>.</p>"},{"location":"developer-guide/operator-quick-start/#deploy-bpfman-operation","title":"Deploy bpfman Operation","text":"<p>The <code>bpfman-operator</code> is running as a Deployment with a ReplicaSet of one. It runs on the control plane and is composed of the containers <code>bpfman-operator</code> and <code>kube-rbac-proxy</code>. The operator is responsible for launching the bpfman Daemonset, which runs on every node. The bpfman Daemonset is composed of the containers <code>bpfman</code>, <code>bpfman-agent</code>, and <code>node-driver-registrar</code>.</p>"},{"location":"developer-guide/operator-quick-start/#deploy-locally-via-kind","title":"Deploy Locally via KIND","text":"<p>After reviewing the possible make targets it's quick and easy to get bpfman deployed locally on your system via a KIND cluster with:</p> <pre><code>cd bpfman/bpfman-operator\nmake run-on-kind\n</code></pre> <p>NOTE: By default, bpfman-operator deploys bpfman with CSI enabled. CSI requires Kubernetes v1.26 due to a PR (kubernetes/kubernetes#112597) that addresses a gRPC Protocol Error that was seen in the CSI client code and it doesn't appear to have been backported. It is recommended to install kind v0.20.0 or later.</p>"},{"location":"developer-guide/operator-quick-start/#deploy-to-openshift-cluster","title":"Deploy To Openshift Cluster","text":"<p>First deploy the operator with one of the following two options:</p>"},{"location":"developer-guide/operator-quick-start/#1-manually-with-kustomize","title":"1. Manually with Kustomize","text":"<p>To install manually with Kustomize and raw manifests simply run the following commands. The Openshift cluster needs to be up and running and specified in <code>~/.kube/config</code> file.</p> <pre><code>cd bpfman/bpfman-operator\nmake deploy-openshift\n</code></pre> <p>Which can then be cleaned up at a later time with:</p> <pre><code>make undeploy-openshift\n</code></pre>"},{"location":"developer-guide/operator-quick-start/#2-via-the-olm-bundle","title":"2. Via the OLM bundle","text":"<p>The other option for installing the bpfman-operator is to install it using OLM bundle.</p> <p>First setup the namespace and certificates for the operator with:</p> <pre><code>cd bpfman/bpfman-operator\noc apply -f ./hack/ocp-scc-hacks.yaml\n</code></pre> <p>Then use <code>operator-sdk</code> to install the bundle like so:</p> <pre><code>operator-sdk run bundle quay.io/bpfman/bpfman-operator-bundle:latest --namespace openshift-bpfman\n</code></pre> <p>Which can then be cleaned up at a later time with:</p> <pre><code>operator-sdk cleanup bpfman-operator\n</code></pre> <p>followed by</p> <pre><code>oc delete -f ./hack/ocp-scc-hacks.yaml\n</code></pre>"},{"location":"developer-guide/operator-quick-start/#verify-the-installation","title":"Verify the Installation","text":"<p>Independent of the method used to deploy, if the bpfman-operator came up successfully you will see the bpfman-daemon and bpfman-operator pods running without errors:</p> <pre><code>kubectl get pods -n bpfman\nNAME                             READY   STATUS    RESTARTS   AGE\nbpfman-daemon-w24pr                3/3     Running   0          130m\nbpfman-operator-78cf9c44c6-rv7f2   2/2     Running   0          132m\n</code></pre>"},{"location":"developer-guide/operator-quick-start/#deploy-an-ebpf-program-to-the-cluster","title":"Deploy an eBPF Program to the cluster","text":"<p>To test the deployment simply deploy one of the sample <code>xdpPrograms</code>:</p> <pre><code>cd bpfman/bpfman-operator/\nkubectl apply -f config/samples/bpfman.io_v1alpha1_xdp_pass_xdpprogram.yaml\n</code></pre> <p>If loading of the XDP Program to the selected nodes was successful it will be reported back to the user via the <code>xdpProgram</code>'s status field:</p> <pre><code>kubectl get xdpprogram xdp-pass-all-nodes -o yaml\napiVersion: bpfman.io/v1alpha1\nkind: XdpProgram\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"bpfman.io/v1alpha1\",\"kind\":\"XdpProgram\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/name\":\"xdpprogram\"},\"name\":\"xdp-pass-all-nodes\"},\"spec\":{\"bpffunctionname\":\"pass\",\"bytecode\":{\"image\":{\"url\":\"quay.io/bpfman-bytecode/xdp_pass:latest\"}},\"globaldata\":{\"GLOBAL_u32\":[13,12,11,10],\"GLOBAL_u8\":[1]},\"interfaceselector\":{\"primarynodeinterface\":true},\"nodeselector\":{},\"priority\":0}}\n  creationTimestamp: \"2023-11-07T19:16:39Z\"\n  finalizers:\n  - bpfman.io.operator/finalizer\n  generation: 2\n  labels:\n    app.kubernetes.io/name: xdpprogram\n  name: xdp-pass-all-nodes\n  resourceVersion: \"157187\"\n  uid: 21c71a61-4e73-44eb-9b49-07af2866d25b\nspec:\n  bpffunctionname: pass\n  bytecode:\n    image:\n      imagepullpolicy: IfNotPresent\n      url: quay.io/bpfman-bytecode/xdp_pass:latest\n  globaldata:\n    GLOBAL_u8: AQ==\n    GLOBAL_u32: DQwLCg==\n  interfaceselector:\n    primarynodeinterface: true\n  mapownerselector: {}\n  nodeselector: {}\n  priority: 0\n  proceedon:\n  - pass\n  - dispatcher_return\nstatus:\n  conditions:\n  - lastTransitionTime: \"2023-11-07T19:16:42Z\"\n    message: bpfProgramReconciliation Succeeded on all nodes\n    reason: ReconcileSuccess\n    status: \"True\"\n    type: ReconcileSuccess\n</code></pre> <p>To see information in listing form simply run:</p> <pre><code>kubectl get xdpprogram -o wide\nNAME                 BPFFUNCTIONNAME   NODESELECTOR   PRIORITY   INTERFACESELECTOR               PROCEEDON\nxdp-pass-all-nodes   pass              {}             0          {\"primarynodeinterface\":true}   [\"pass\",\"dispatcher_return\"]\n</code></pre>"},{"location":"developer-guide/operator-quick-start/#api-types-overview","title":"API Types Overview","text":"<p>See api-spec.md for a more detailed description of all the bpfman Kubernetes API types.</p>"},{"location":"developer-guide/operator-quick-start/#multiple-program-crds","title":"Multiple Program CRDs","text":"<p>The multiple <code>*Program</code> CRDs are the bpfman Kubernetes API objects most relevant to users and can be used to understand clusterwide state for an eBPF program. It's designed to express how, and where eBPF programs are to be deployed within a Kubernetes cluster. Currently bpfman supports:</p> <ul> <li><code>fentryProgram</code></li> <li><code>fexitProgram</code></li> <li><code>kprobeProgram</code></li> <li><code>tcProgram</code></li> <li><code>tracepointProgram</code></li> <li><code>uprobeProgram</code></li> <li><code>xdpProgram</code></li> </ul>"},{"location":"developer-guide/operator-quick-start/#bpfprogram-crd","title":"BpfProgram CRD","text":"<p>The <code>BpfProgram</code> CRD is used internally by the bpfman-deployment to keep track of per node bpfman state such as map pin points, and to report node specific errors back to the user. Kubernetes users/controllers are only allowed to view these objects, NOT create or edit them.</p> <p>Applications wishing to use bpfman to deploy/manage their eBPF programs in Kubernetes will make use of this object to find references to the bpfMap pin points (<code>spec.maps</code>) in order to configure their eBPF programs.</p>"},{"location":"developer-guide/release/","title":"Release Process","text":"<p>This document describes how to cut a release for the bpfman project.</p>"},{"location":"developer-guide/release/#overview","title":"Overview","text":"<p>A release for the bpfman project is comprised of the following major components:</p> <ul> <li>bpfman (Core library) and bpfman-api (Core GRPC API protobuf definitions) library crates</li> <li>bpfman (CLI), and bpfman-rpc ( gRPC server ) binary crates</li> <li>bpf-metrics-exporter and bpf-log-exporter binary crates</li> <li>Kubernetes User Facing Custom Resource Definitions (CRDs)<ul> <li><code>TcProgram</code></li> <li><code>XdpProgram</code></li> <li><code>TracepointProgram</code></li> <li><code>UprobeProgram</code></li> <li><code>KprobeProgram</code></li> <li><code>FentryProgram</code></li> <li><code>FexitProgram</code></li> </ul> </li> <li>Corresponding go pkgs in the form of <code>github.com/bpfman/bpfman</code> which includes the following:<ul> <li><code>github.com/bpfman/bpfman/clients/gobpfman/v1</code>: The go client for the bpfman GRPC API</li> <li><code>github.com/bpfman/bpfman/bpfman-operator/apis</code>: The go bindings for the   bpfman CRD API</li> <li><code>github.com/bpfman/bpfman/bpfman-operator/pkg/client</code>: The autogenerated   clientset for the bpfman CRD API</li> <li><code>github.com/bpfman/bpfman/bpfman-operator/pkg/helpers</code>: The provided bpfman CRD   API helpers.</li> </ul> </li> <li>The following core component container images with tag :<ul> <li><code>quay.io/bpfman/bpfman</code></li> <li><code>quay.io/bpfman/bpfman-operator</code></li> <li><code>quay.io/bpfman/bpfman-agent</code></li> <li><code>quay.io/bpfman/bpfman-operator-bundle</code></li> <li><code>quay.io/bpfman/xdp-dispatcher</code></li> <li><code>quay.io/bpfman/tc-dispatcher</code></li> </ul> <li>The relevant example bytecode container images with tag  from source   code located in the bpfman project:<ul> <li><code>quay.io/bpfman-bytecode/go-xdp-counter</code></li> <li><code>quay.io/bpfman-userspace/go-target</code></li> <li><code>quay.io/bpfman-bytecode/go-tc-counter</code></li> <li><code>quay.io/bpfman-bytecode/go-tracepoint-counter</code></li> <li><code>quay.io/bpfman-bytecode/xdp-pass</code></li> <li><code>quay.io/bpfman-bytecode/tc-pass</code></li> <li><code>quay.io/bpfman-bytecode/tracepoint</code></li> <li><code>quay.io/bpfman-bytecode/xdp-pass-private</code></li> <li><code>quay.io/bpfman-bytecode/go-uprobe-counter</code></li> <li><code>quay.io/bpfman-bytecode/go-kprobe-counter</code></li> <li><code>quay.io/bpfman-bytecode/uprobe</code></li> <li><code>quay.io/bpfman-bytecode/kprobe</code></li> <li><code>quay.io/bpfman-bytecode/uretprobe</code></li> <li><code>quay.io/bpfman-bytecode/kretprobe</code></li> <li><code>quay.io/bpfman-bytecode/fentry</code></li> <li><code>quay.io/bpfman-bytecode/fexit</code></li> </ul> <li>The relevant example userspace container images with tag  from source   code located in the bpfman project:<ul> <li><code>quay.io/bpfman-userspace/go-xdp-counter</code></li> <li><code>quay.io/bpfman-userspace/go-tc-counter</code></li> <li><code>quay.io/bpfman-userspace/go-tracepoint-counter</code></li> <li><code>quay.io/bpfman-userspace/go-uprobe-counter</code></li> <li><code>quay.io/bpfman-userspace/go-kprobe-counter</code></li> </ul> <li>The OLM (Operator Lifecycle Manager) for the Kubernetes Operator.<ul> <li>This includes a <code>bundle</code> directory on disk as well as the   <code>quay.io/bpfman/bpfman-operator-bundle</code> with the tag ."},{"location":"developer-guide/release/#versioning-strategy","title":"Versioning strategy","text":""},{"location":"developer-guide/release/#overview_1","title":"Overview","text":"<p>Each new release of bpfman is defined with a \"bundle version\" that represents the Git tag of a release, such as <code>v0.4.0</code>. This contains the components described above</p>"},{"location":"developer-guide/release/#kubernetes-api-versions-eg-v1alpha2-v1beta1","title":"Kubernetes API Versions (e.g. v1alpha2, v1beta1)","text":"<p>Within the bpfman-operator, API versions are primarily used to indicate the stability of a resource. For example, if a resource has not yet graduated to beta, it is still possible that it could either be removed from the API or changed in backwards incompatible ways. For more information on API versions, refer to the full Kubernetes API versioning documentation.</p>"},{"location":"developer-guide/release/#releasing-a-new-version","title":"Releasing a new version","text":""},{"location":"developer-guide/release/#writing-a-changelog","title":"Writing a Changelog","text":"<p>To simplify release notes generation, we recommend using the Kubernetes release notes generator:</p> <pre><code>go install k8s.io/release/cmd/release-notes@latest\nexport GITHUB_TOKEN=your_token_here\nrelease-notes --start-sha EXAMPLE_COMMIT --end-sha EXAMPLE_COMMIT --branch main --repo bpfman --org bpfman\n</code></pre> <p>This output will likely need to be reorganized and cleaned up a bit, but it provides a good starting point. Once you're satisfied with the changelog, create a PR. This must go through the regular PR review process and get merged into the <code>main</code> branch. Approval of the PR indicates community consensus for a new release.</p>"},{"location":"developer-guide/release/#release-steps","title":"Release Steps","text":"<p>The following steps must be done by one of the bpfman maintainers:</p> <p>For a PATCH release:</p> <ul> <li>Create a new branch in your fork named something like <code>&lt;githubuser&gt;/release-x.x.x</code>. Use the new branch   in the upcoming steps.</li> <li>Use <code>git</code> to cherry-pick all relevant PRs into your branch.</li> <li>Create a branch from the major-minor tag of interest i.e:   <code>git checkout -b release-x.x.x &lt;major.minor.patch&gt;</code></li> <li>Create a pull request of the <code>&lt;githubuser&gt;/release-x.x.x</code> branch into the <code>release-x.x</code> branch upstream.   Add a hold on this PR waiting for at least one maintainer/codeowner to provide a <code>lgtm</code>. This PR should:<ul> <li>Add a new changelog for the release</li> <li>Update the cargo.toml version for the workspace.</li> <li>Update the bpfman-operator version in it's MAKEFILE and run <code>make bundle</code> to update the bundle version.   This will generate a new <code>/bpfman-operator/bundle</code> directory which will ONLY be tracked in the   <code>release-x.x</code> branch not <code>main</code>.</li> </ul> </li> <li>Verify the CI tests pass and merge the PR into <code>release-x.x</code>.</li> <li>Create a tag using the <code>HEAD</code> of the <code>release-x.x.x</code> branch. This can be done using the <code>git</code> CLI or   Github's release page.</li> <li>The Release will be automatically created, after that is complete do the following:<ul> <li>run <code>make build-release-yamls</code> and attach the yamls for the version to the release. These will include:<ul> <li><code>bpfman-crds-install.yaml</code></li> <li><code>bpfman-operator-install.yaml</code></li> <li><code>go-xdp-counter-install.yaml</code></li> <li><code>go-tc-counter-install.yaml</code></li> <li><code>go-tracepoint-counter-install.yaml</code></li> </ul> </li> </ul> </li> <li>Update the community-operator and   community-operators-prod repositories with   the latest bundle manifests. See the following PRs as examples:<ul> <li>https://github.com/redhat-openshift-ecosystem/community-operators-prod/pull/2696</li> <li>https://github.com/k8s-operatorhub/community-operators/pull/2790</li> </ul> </li> </ul> <p>For a MAJOR or MINOR release:</p> <ul> <li>Open an update PR that:<ul> <li>Adds a new changelog for the release</li> <li>Updates the cargo.toml version for the workspace.</li> <li>Updates the bpfman-operator version in it's MAKEFILE and run <code>make bundle</code> to update the bundle version</li> <li>Add's a new <code>examples</code> config directory for the release version</li> </ul> </li> <li>Make sure CI is green and merge the update PR.</li> <li>Create a tag using the <code>HEAD</code> of the <code>main</code> branch. This can be done using the <code>git</code> CLI or   Github's release page.</li> <li>Tag the release using the commit on <code>main</code> where the changelog update merged.   This can  be done using the <code>git</code> CLI or Github's release   page.</li> <li>The Release will be automatically created, after that is complete do the following:<ul> <li>run <code>make build-release-yamls</code> and attach the yamls for the version to the release. These will include:<ul> <li><code>bpfman-crds-install.yaml</code></li> <li><code>bpfman-operator-install.yaml</code></li> <li><code>go-xdp-counter-install.yaml</code></li> <li><code>go-tc-counter-install.yaml</code></li> <li><code>go-tracepoint-counter-install.yaml</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"developer-guide/shipping-bytecode/","title":"eBPF Bytecode Image Specifications","text":""},{"location":"developer-guide/shipping-bytecode/#introduction","title":"Introduction","text":"<p>The eBPF Bytecode Image specification defines how to package eBPF bytecode as container images. The initial primary use case focuses on the containerization and deployment of eBPF programs within container orchestration systems such as Kubernetes, where it is necessary to provide a portable way to distribute bytecode to all nodes which need it.</p>"},{"location":"developer-guide/shipping-bytecode/#specifications","title":"Specifications","text":"<p>We provide two distinct spec variants here to ensure interoperatiblity with existing registries and packages which do no support the new custom media types defined here.</p> <ul> <li>custom-data-type-spec</li> <li>backwards-compatable-spec</li> </ul>"},{"location":"developer-guide/shipping-bytecode/#backwards-compatible-oci-compliant-spec","title":"Backwards compatible OCI compliant spec","text":"<p>This variant makes use of existing OCI conventions to represent eBPF Bytecode as container images.</p>"},{"location":"developer-guide/shipping-bytecode/#image-layers","title":"Image Layers","text":"<p>The container images following this variant must contain exactly one layer who's media type is one of the following:</p> <ul> <li><code>application/vnd.oci.image.layer.v1.tar+gzip</code> or the compliant <code>application/vnd.docker.image.rootfs.diff.tar.gzip</code></li> </ul> <p>Additionally the image layer must contain a valid eBPF object file (generally containing a <code>.o</code> extension) placed at the root of the layer <code>./</code>.</p>"},{"location":"developer-guide/shipping-bytecode/#image-labels","title":"Image Labels","text":"<p>To provide relevant metadata regarding the bytecode to any consumers, some relevant labels MUST be defined on the image.</p> <p>These labels are defined as follows:</p> <ul> <li> <p><code>io.ebpf.program_type</code>: The eBPF program type (i.e <code>xdp</code>,<code>tc</code>, <code>sockops</code>, ...).</p> </li> <li> <p><code>io.ebpf.filename</code>: The Filename of the bytecode stored in the image.</p> </li> <li> <p><code>io.ebpf.program_name</code>: The name of the eBPF Program represented in the bytecode.</p> </li> <li> <p><code>io.ebpf.bpf_function_name</code>: The name of the function that is the entry point for the BPF program.</p> </li> </ul>"},{"location":"developer-guide/shipping-bytecode/#building-a-backwards-compatible-oci-compliant-image","title":"Building a Backwards compatible OCI compliant image","text":"<p>An Example Containerfile can be found at <code>/packaging/container/deployment/Containerfile.bytecode</code></p> <p>To use the provided templated Containerfile simply run a <code>docker build</code> command like the following:</p> <pre><code>docker build \\\n --build-arg PROGRAM_NAME=xdp_pass \\\n --build-arg BPF_FUNCTION_NAME=pass \\\n --build-arg PROGRAM_TYPE=xdp \\\n --build-arg BYTECODE_FILENAME=pass.bpf.o \\\n --build-arg KERNEL_COMPILE_VER=$(uname -r) \\\n -f Containerfile.bytecode \\\n /home/&lt;USER&gt;/bytecode -t quay.io/&lt;USER&gt;/xdp_pass:latest\n</code></pre> <p>Where <code>/home/&lt;USER&gt;/bytecode</code> is the directory the bytecode object file is located.</p> <p>Users can also use <code>skopeo</code> to ensure the image follows the backwards compatible version of the spec:</p> <ul> <li><code>skopeo inspect</code> will show the correctly configured labels stored in the   configuration layer (<code>application/vnd.oci.image.config.v1+json</code>) of the image.</li> </ul> <pre><code>skopeo inspect docker://quay.io/astoycos/xdp_pass:latest\n{\n    \"Name\": \"quay.io/&lt;USER&gt;/xdp_pass\",\n    \"Digest\": \"sha256:db1f7dd03f9fba0913e07493238fcfaf0bf08de37b8e992cc5902775dfb9086a\",\n    \"RepoTags\": [\n        \"latest\"\n    ],\n    \"Created\": \"2022-08-14T14:27:20.147468277Z\",\n    \"DockerVersion\": \"\",\n    \"Labels\": {\n        \"io.buildah.version\": \"1.26.1\",\n        \"io.ebpf.filename\": \"pass.bpf.o\",\n        \"io.ebpf.program_name\": \"xdp_counter\",\n        \"io.ebpf.program_type\": \"xdp\",\n        \"io.ebpf.bpf_function_name\": \"pass\"\n    },\n    \"Architecture\": \"amd64\",\n    \"Os\": \"linux\",\n    \"Layers\": [\n        \"sha256:5f6dae6f567601fdad15a936d844baac1f30c31bd3df8df0c5b5429f3e048000\"\n    ],\n    \"Env\": [\n        \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n    ]\n}\n</code></pre> <ul> <li><code>skopeo inspect --raw</code> will show the correct layer type is used in the image.</li> </ul> <pre><code>skopeo inspect --raw  docker://quay.io/astoycos/xdp_pass:latest\n{\"schemaVersion\":2,\"mediaType\":\"application/vnd.oci.image.manifest.v1+json\",\"config\":{\"mediaType\":\"application/vnd.oci.image.config.v1+json\",\"digest\":\"sha256:ff4108b8405a877b2df3e06f9287c509b9d62d6c241c9a5213d81a9abee80361\",\"size\":2385},\"layers\":[{\"mediaType\":\"application/vnd.oci.image.layer.v1.tar+gzip\",\"digest\":\"sha256:5f6dae6f567601fdad15a936d844baac1f30c31bd3df8df0c5b5429f3e048000\",\"size\":1539}],\"annotations\":{\"org.opencontainers.image.base.digest\":\"sha256:86b59a6cf7046c624c47e40a5618b383d763be712df2c0e7aaf9391c2c9ef559\",\"org.opencontainers.image.base.name\":\"\"}}\n</code></pre>"},{"location":"developer-guide/shipping-bytecode/#custom-oci-compatible-spec","title":"Custom OCI compatible spec","text":"<p>This variant of the eBPF bytecode image spec uses custom OCI medium types to represent eBPF bytecode as container images. Many toolchains and registries may not support this yet.</p> <p>TODO(astoycos)</p>"},{"location":"developer-guide/testing/","title":"Testing","text":"<p>This document describes the automated testing that is done for each pull request submitted to bpfman, and also provides instructions for running them locally when doing development.</p>"},{"location":"developer-guide/testing/#unit-testing","title":"Unit Testing","text":"<p>Unit testing is executed as part of the <code>build</code> job by running the following command in the top-level bpfman directory.</p> <pre><code> cargo test\n</code></pre>"},{"location":"developer-guide/testing/#go-example-tests","title":"Go Example Tests","text":"<p>Tests are run for each of the example programs found in directory <code>examples</code></p> <p>Detailed description TBD</p>"},{"location":"developer-guide/testing/#basic-integration-tests","title":"Basic Integration Tests","text":"<p>The full set of basic integration tests are executed by running the following command in the top-level bpfman directory.</p> <pre><code>cargo xtask integration-test\n</code></pre> <p>Optionally, a subset of the integration tests can be run by adding the \"--\" and a list of one or more names at the end of the command as shown below.</p> <pre><code>cargo xtask integration-test -- test_load_unload_xdp test_proceed_on_xdp\n</code></pre> <p>The integration tests start a <code>bpfman</code> daemon process, and issue CLI commands to verify a range of functionality.  For XDP and TC programs that are installed on network interfaces, the integration test code creates a test network namespace connected to the host by a veth pair on which the programs are attached. The test code uses the IP subnet 172.37.37.1/24 for the namespace. If that address conflicts with an existing network on the host, it can be changed by setting the <code>BPFMAN_IP_PREFIX</code> environment variable to one that is available as shown below.</p> <pre><code>export BPFMAN_IP_PREFIX=\"192.168.50\"\n</code></pre> <p>If bpfman logs are needed to help debug an integration test, set <code>RUST_LOG</code> either globally or for a given test.</p> <p><pre><code>export RUST_LOG=info\n</code></pre> OR <pre><code>RUST_LOG=info cargo xtask integration-test -- test_load_unload_xdp test_proceed_on_xdp\n</code></pre></p> <p>There are two categories of integration tests: basic and e2e.  The basic tests verify basic CLI functionality such as loading, listing, and unloading programs.  The e2e tests verify more advanced functionality such as the setting of global variables, priority, and proceed-on by installing the programs, creating traffic if needed, and examining logs to confirm that things are running as expected.</p> <p>Most eBPF test programs are loaded from container images stored on quay.io. The source code for the eBPF test programs can be found in the <code>tests/integration-test/bpf</code> directory.  These programs are compiled by executing <code>cargo xtask build-ebpf --libbpf-dir &lt;libbpf dir&gt;</code></p> <p>We also load some tests from local files to test the <code>load-from-file</code> option.</p> <p>The <code>bpf</code> directory also contains a script called <code>build_push_images.sh</code> that can be used to build and push new images to quay if the code is changed. Images get pushed automatically when code gets merged, however, it's still useful to be able to push them manually sometimes. For example, when a new test case requires that both the eBPF and integration code be changed together.  It is also a useful template for new eBPF test code that needs to be pushed. However, as a word of caution, be aware that existing integration tests will start using the new programs immediately, so this should only be done if the modified program is backward compatible.</p>"},{"location":"developer-guide/testing/#kubernetes-operator-tests","title":"Kubernetes Operator Tests","text":""},{"location":"developer-guide/testing/#kubernetes-operator-unit-tests","title":"Kubernetes Operator Unit Tests","text":"<p>To run all of the unit tests defined in the bpfman-operator controller code run <code>make test</code> in the bpfman-operator directory.</p>"},{"location":"developer-guide/testing/#kubernetes-operator-integration-tests","title":"Kubernetes Operator Integration Tests","text":"<p>To run the Kubernetes Operator integration tests locally:</p> <ol> <li>Build the example test code images.</li> </ol> <pre><code>    # in bpfman/examples\n    make build-us-images\n    make build-bc-images\n</code></pre> <ol> <li>Build the bpfman images locally with the <code>int-test</code> tag.</li> </ol> <pre><code>    # in bpfman/bpfman-operator\n    BPFMAN_AGENT_IMG=quay.io/bpfman/bpfman-agent:int-test BPFMAN_IMG=quay.io/bpfman/bpfman:int-test BPFMAN_OPERATOR_IMG=quay.io/bpfman/bpfman-operator:int-test make build-images\n</code></pre> <ol> <li>Run the integration test suite.</li> </ol> <pre><code>    # in bpfman/bpfman-operator\n    BPFMAN_AGENT_IMG=quay.io/bpfman/bpfman-agent:int-test BPFMAN_IMG=quay.io/bpfman/bpfman:int-test BPFMAN_OPERATOR_IMG=quay.io/bpfman/bpfman-operator:int-test make test-integration\n</code></pre> <p>Additionally the integration test can be configured with the following environment variables:</p> <ul> <li>KEEP_TEST_CLUSTER: If set to <code>true</code> the test cluster will not be torn down   after the integration test suite completes.</li> <li>USE_EXISTING_KIND_CLUSTER: If this is set to the name of the existing kind   cluster the integration test suite will use that cluster instead of creating a   new one.</li> </ul>"},{"location":"developer-guide/xdp-overview/","title":"XDP Tutorial","text":"<p>The XDP hook point is unique in that the associated eBPF program attaches to an interface and only one eBPF program is allowed to attach to the XDP hook point for a given interface. Due to this limitation, the libxdp protocol was written. The one program that is attached to the XDP hook point is an eBPF dispatcher program. The dispatcher program contains a list of 10 stub functions. When XDP programs wish to be loaded, they are loaded as extension programs which are then called in place of one of the stub functions.</p> <p>bpfman is leveraging the libxdp protocol to allow it's users to load up to 10 XDP programs on a given interface. This tutorial will show you how to use <code>bpfman</code> to load multiple XDP programs on an interface.</p> <p>Note: The TC hook point is also associated with an interface. Within bpfman, TC is implemented in a similar fashion to XDP in that it uses a dispatcher with stub functions. TCX is a fairly new kernel feature that improves how the kernel handles multiple TC programs on a given interface. bpfman is on the process of integrating TCX support, which will replace the dispatcher logic for TC. Until then, assume TC behaves in a similar fashion to XDP.</p> <p>See Launching bpfman for more detailed instructions on building and loading bpfman. This tutorial assumes bpfman has been built and the <code>bpfman</code> CLI is in $PATH.</p>"},{"location":"developer-guide/xdp-overview/#load-xdp-program","title":"Load XDP program","text":"<p>We will load the simple <code>xdp-pass</code> program, which permits all traffic to the attached interface, <code>eno3</code> in this example. We will use the priority of 100. Find a deeper dive into CLI syntax in CLI Guide.</p> <pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp \\\n  --iface eno3 --priority 100\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6213\n Map Owner ID:  None\n Map Used By:   6213\n Priority:      100\n Iface:         eno3\n Position:      0\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6213\n Name:                             pass\n Type:                             xdp\n Loaded At:                        2023-07-17T17:48:10-0400\n Tag:                              4b9d1b2c140e87ce\n GPL Compatible:                   true\n Map IDs:                          [2724]\n BTF ID:                           2834\n Size Translated (bytes):          96\n JITed:                            true\n Size JITed (bytes):               67\n Kernel Allocated Memory (bytes):  4096\n Verified Instruction Count:       9\n</code></pre> <p><code>bpfman load image</code> returns the same data as a <code>bpfman get</code> command. From the output, the Program Id of <code>6213</code> can be found in the <code>Kernel State</code> section. This id can be used to perform a <code>bpfman get</code> to retrieve all relevant program data and a <code>bpfman unload</code> when the program needs to be unloaded.</p> <pre><code>sudo bpfman list\n Program ID  Name  Type  Load Time\n 6213        pass  xdp   2023-07-17T17:48:10-0400\n</code></pre> <p>We can recheck the details about the loaded program with the <code>bpfman get</code> command:</p> <pre><code>sudo bpfman get 6213\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6213\n Map Owner ID:  None\n Map Used By:   6213\n Priority:      100\n Iface:         eno3\n Position:      0\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6213\n Name:                             pass\n Type:                             xdp\n Loaded At:                        2023-07-17T17:48:10-0400\n Tag:                              4b9d1b2c140e87ce\n GPL Compatible:                   true\n Map IDs:                          [2724]\n BTF ID:                           2834\n Size Translated (bytes):          96\n JITed:                            true\n Size JITed (bytes):               67\n Kernel Allocated Memory (bytes):  4096\n Verified Instruction Count:       9\n</code></pre> <p>From the output above you can see the program was loaded to position 0 on our interface and thus will be executed first.</p>"},{"location":"developer-guide/xdp-overview/#loading-additional-xdp-programs","title":"Loading Additional XDP Programs","text":"<p>We will now load 2 more programs with different priorities to demonstrate how bpfman will ensure they are ordered correctly:</p> <pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp \\\n  --iface eno3 --priority 50\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6215\n Map Owner ID:  None\n Map Used By:   6215\n Priority:      50\n Iface:         eno3\n Position:      0\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6215\n Name:                             pass\n Type:                             xdp\n:\n</code></pre> <pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp \\\n  --iface eno3 --priority 200\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6217\n Map Owner ID:  None\n Map Used By:   6217\n Priority:      200\n Iface:         eno3\n Position:      2\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6217\n Name:                             pass\n Type:                             xdp\n:\n</code></pre> <p>Using <code>bpfman list</code> we can see all the programs that were loaded.</p> <pre><code>sudo bpfman list\n Program ID  Name  Type  Load Time\n 6213        pass  xdp   2023-07-17T17:48:10-0400\n 6215        pass  xdp   2023-07-17T17:52:46-0400\n 6217        pass  xdp   2023-07-17T17:53:57-0400\n</code></pre> <p>The lowest priority program is executed first, while the highest is executed last. As can be seen from the detailed output for each command below:</p> <ul> <li>Program <code>6215</code> is at position <code>0</code> with a priority of <code>50</code></li> <li>Program <code>6213</code> is at position <code>1</code> with a priority of <code>100</code></li> <li>Program <code>6217</code> is at position <code>2</code> with a priority of <code>200</code></li> </ul> <pre><code>sudo bpfman get 6213\n Bpfman State\n---------------\n Name:          pass\n:\n Priority:      100\n Iface:         eno3\n Position:      1\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6213\n Name:                             pass\n Type:                             xdp\n:\n</code></pre> <pre><code>sudo bpfman get 6215\n Bpfman State\n---------------\n Name:          pass\n:\n Priority:      50\n Iface:         eno3\n Position:      0\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6215\n Name:                             pass\n Type:                             xdp\n:\n</code></pre> <pre><code>sudo bpfman get 6217\n Bpfman State\n---------------\n Name:          pass\n:\n Priority:      200\n Iface:         eno3\n Position:      2\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6217\n Name:                             pass\n Type:                             xdp\n:\n</code></pre> <p>By default, the next program in the chain will only be executed if a given program returns <code>pass</code> (see <code>proceed-on</code> field in the <code>bpfman get</code> output above). If the next program in the chain should be called even if a different value is returned, then the program can be loaded with those additional return values using the <code>proceed-on</code> parameter (see <code>bpfman load image xdp --help</code> for list of valid values):</p> <pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp \\\n  --iface eno3 --priority 150 --proceed-on \"pass\" --proceed-on \"dispatcher_return\"\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6219\n Map Owner ID:  None\n Map Used By:   6219\n Priority:      150\n Iface:         eno3\n Position:      2\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6219\n Name:                             pass\n Type:                             xdp\n:\n</code></pre> <p>Which results in being loaded in position <code>2</code> because it was loaded at priority <code>150</code>, which is lower than the previous program at that position with a priority of <code>200</code>.</p>"},{"location":"developer-guide/xdp-overview/#delete-xdp-program","title":"Delete XDP Program","text":"<p>Let's remove the program at position 1.</p> <pre><code>sudo bpfman list\n Program ID  Name  Type  Load Time\n 6213        pass  xdp   2023-07-17T17:48:10-0400\n 6215        pass  xdp   2023-07-17T17:52:46-0400\n 6217        pass  xdp   2023-07-17T17:53:57-0400\n 6219        pass  xdp   2023-07-17T17:59:41-0400\n</code></pre> <pre><code>sudo bpfman unload 6213\n</code></pre> <p>And we can verify that it has been removed and the other programs re-ordered:</p> <pre><code>sudo bpfman list\n Program ID  Name  Type  Load Time\n 6215        pass  xdp   2023-07-17T17:52:46-0400\n 6217        pass  xdp   2023-07-17T17:53:57-0400\n 6219        pass  xdp   2023-07-17T17:59:41-0400\n</code></pre> <pre><code>bpfman get 6215\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6215\n Map Owner ID:  None\n Map Used By:   6215\n Priority:      50\n Iface:         eno3\n Position:      0\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6215\n Name:                             pass\n Type:                             xdp\n:\n</code></pre> <pre><code>bpfman get 6217\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6217\n Map Owner ID:  None\n Map Used By:   6217\n Priority:      200\n Iface:         eno3\n Position:      2\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6217\n Name:                             pass\n Type:                             xdp\n:\n</code></pre> <pre><code>bpfman get 6219\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6219\n Map Owner ID:  None\n Map Used By:   6219\n Priority:      150\n Iface:         eno3\n Position:      1\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6219\n Name:                             pass\n Type:                             xdp\n:\n</code></pre>"},{"location":"getting-started/building-bpfman/","title":"Setup and Building bpfman","text":"<p>This section describes how to build bpfman. If this is the first time building bpfman, jump to the Development Environment Setup section for help installing the tooling.</p> <p>There is also an option to run images from a given release, or from an RPM, as opposed to building locally. Jump to the Run bpfman From Release Image section for installing from a fixed release or jump to the Run bpfman From RPM section for installing from an RPM.</p>"},{"location":"getting-started/building-bpfman/#kernel-versions","title":"Kernel Versions","text":"<p>eBPF is still a relatively new technology and being actively developed. To take advantage of this constantly evolving technology, it is best to use the newest kernel version possible. If bpfman needs to be run on an older kernel, this section describes some of the kernel features bpfman relies on to work and which kernel the feature was first introduced.</p> <p>Major kernel features leveraged by bpfman:</p> <ul> <li>Program Extensions: Program Extensions allows bpfman to load multiple XDP or TC eBPF programs   on an interface, which is not natively supported in the kernel.   A <code>dispatcher</code> program is loaded as the one program on a given interface, and the user's XDP or TC   programs are loaded as extensions to the <code>dispatcher</code> program.   Introduced in Kernel 5.6.</li> <li>Pinning: Pinning allows the eBPF program to remain loaded when the loading process (bpfman) is   stopped or restarted.   Introduced in Kernel 4.11.</li> <li>BPF Perf Link: Support BPF perf link for tracing programs (Tracepoint, Uprobe and Kprobe)   which enables pinning for these program types.   Introduced in Kernel 5.15.</li> <li>Relaxed CAP_BPF Requirement: Prior to Kernel 5.19, all eBPF system calls required CAP_BPF.   This required userspace programs that wanted to access eBPF maps to have the CAP_BPF Linux capability.   With the kernel 5.19 change, CAP_BPF is only required for load and unload requests.</li> </ul> <p>bpfman tested on older kernel versions:</p> <ul> <li>Fedora 34: Kernel 5.17.6-100.fc34.x86_64<ul> <li>XDP, TC, Tracepoint, Uprobe and Kprobe programs all loaded with bpfman running on localhost   and running as systemd service.</li> </ul> </li> <li>Fedora 33: Kernel 5.14.18-100.fc33.x86_64<ul> <li>XDP and TC programs loaded with bpfman running on localhost and running as systemd service   once SELinux was disabled (see https://github.com/fedora-selinux/selinux-policy/pull/806).</li> <li>Tracepoint, Uprobe and Kprobe programs failed to load because they require the <code>BPF Perf Link</code>   support.</li> </ul> </li> <li>Fedora 32: Kernel 5.11.22-100.fc32.x86_64<ul> <li>XDP and TC programs loaded with bpfman running on localhost once SELinux was disabled   (see https://github.com/fedora-selinux/selinux-policy/pull/806).</li> <li>bpfman fails to run as a systemd service because of some capabilities issues in the   bpfman.service file.</li> <li>Tracepoint, Uprobe and Kprobe programs failed to load because they require the <code>BPF Perf Link</code>   support.</li> </ul> </li> <li>Fedora 31: Kernel 5.8.18-100.fc31.x86_64<ul> <li>bpfman was able to start on localhost, but XDP and TC programs wouldn't load because   <code>BPF_LINK_CREATE</code> call was updated in newer kernels.</li> <li>bpfman fails to run as a systemd service because of some capabilities issues in the   bpfman.service file.</li> </ul> </li> </ul>"},{"location":"getting-started/building-bpfman/#clone-the-bpfman-repo","title":"Clone the bpfman Repo","text":"<p>You can build and run bpfman from anywhere. However, if you plan to make changes to the bpfman operator, specifically run <code>make generate</code>, it will need to be under your <code>GOPATH</code> because Kubernetes Code-generator does not work outside of <code>GOPATH</code> Issue 86753. Assuming your <code>GOPATH</code> is set to the typical <code>$HOME/go</code>, your repo should live in <code>$HOME/go/src/github.com/bpfman/bpfman</code></p> <pre><code>mkdir -p $HOME/go/src/github.com/bpfman\ncd $HOME/go/src/github.com/bpfman\ngit clone git@github.com:bpfman/bpfman.git\n</code></pre>"},{"location":"getting-started/building-bpfman/#building-bpfman","title":"Building bpfman","text":"<p>To just test with the latest bpfman, containerized image are stored in <code>quay.io/bpfman</code> (see bpfman Container Images). To build with local changes, use the following commands.</p> <p>If you are building bpfman for the first time OR the eBPF code has changed:</p> <pre><code>cargo xtask build-ebpf --libbpf-dir /path/to/libbpf\n</code></pre> <p>If protobuf files have changed (see RPC Protobuf Generation):</p> <pre><code>cargo xtask build-proto\n</code></pre> <p>To build bpfman:</p> <pre><code>cargo build\n</code></pre>"},{"location":"getting-started/building-bpfman/#building-cli-tab-completion-files","title":"Building CLI TAB completion files","text":"<p>Optionally, to build the CLI TAB completion files, run the following command:</p> <pre><code>cargo xtask build-completion\n</code></pre> <p>Files are generated for different shells:</p> <pre><code>ls .output/completions/\n_bpfman  bpfman.bash  bpfman.elv  bpfman.fish  _bpfman.ps1\n</code></pre>"},{"location":"getting-started/building-bpfman/#bash","title":"bash","text":"<p>For <code>bash</code>, this generates a file that can be used by the linux <code>bash-completion</code> utility (see Install bash-completion for installation instructions).</p> <p>If the files are generated, they are installed automatically when using the install script (i.e. <code>sudo ./scripts/setup.sh install</code> - See Run as a systemd Service). To install the files manually, copy the file associated with a given shell to <code>/usr/share/bash-completion/completions/</code>. For example:</p> <pre><code>sudo cp .output/completions/bpfman.bash /usr/share/bash-completion/completions/.\n\nbpfman g&lt;TAB&gt;\n</code></pre>"},{"location":"getting-started/building-bpfman/#other-shells","title":"Other shells","text":"<p>Files are generated other shells (Elvish, Fish, PowerShell and zsh). For these shells, generated file must be manually installed.</p>"},{"location":"getting-started/building-bpfman/#building-cli-manpages","title":"Building CLI Manpages","text":"<p>Optionally, to build the CLI Manpage files, run the following command:</p> <pre><code>cargo xtask build-man-page\n</code></pre> <p>If the files are generated, they are installed automatically when using the install script (i.e. <code>sudo ./scripts/setup.sh install</code> - See Run as a systemd Service). To install the files manually, copy the generated files to <code>/usr/local/share/man/man1/</code>. For example:</p> <pre><code>sudo cp .output/manpage/bpfman*.1 /usr/local/share/man/man1/.\n</code></pre> <p>Once installed, use <code>man</code> to view the pages.</p> <pre><code>man bpfman list\n</code></pre> <p>NOTE: <code>bpfman</code> commands with subcommands (specifically <code>bpfman load</code>) have <code>-</code> in the manpage subcommand generation. So use <code>bpfman load-file</code>, <code>bpfman load-image</code>, <code>bpfman load-image-xdp</code>, etc. to display the subcommand manpage files.</p>"},{"location":"getting-started/building-bpfman/#development-environment-setup","title":"Development Environment Setup","text":"<p>To build bpfman, the following packages must be installed.</p>"},{"location":"getting-started/building-bpfman/#install-rust-toolchain","title":"Install Rust Toolchain","text":"<p>For further detailed instructions, see Rust Stable &amp; Rust Nightly.</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource \"$HOME/.cargo/env\"\nrustup toolchain install nightly -c rustfmt,clippy,rust-src\n</code></pre>"},{"location":"getting-started/building-bpfman/#install-llvm","title":"Install LLVM","text":"<p>LLVM 11 or later must be installed. Linux package managers should provide a recent enough release.</p> <p><code>dnf</code> based OS:</p> <pre><code>sudo dnf install llvm-devel clang-devel elfutils-libelf-devel\n</code></pre> <p><code>apt</code> based OS:</p> <pre><code>sudo apt install clang lldb lld libelf-dev gcc-multilib\n</code></pre>"},{"location":"getting-started/building-bpfman/#install-protobuf-compiler","title":"Install Protobuf Compiler","text":"<p>For further detailed instructions, see protoc.</p> <p><code>dnf</code> based OS:</p> <pre><code>sudo dnf install protobuf-compiler\n</code></pre> <p><code>apt</code> based OS:</p> <pre><code>sudo apt install protobuf-compiler\n</code></pre>"},{"location":"getting-started/building-bpfman/#install-go-protobuf-compiler-extensions","title":"Install GO protobuf Compiler Extensions","text":"<p>See Quick Start Guide for gRPC in Go for installation instructions.</p>"},{"location":"getting-started/building-bpfman/#local-libbpf","title":"Local libbpf","text":"<p>Checkout a local copy of libbpf.</p> <pre><code>git clone https://github.com/libbpf/libbpf --branch v0.8.0\n</code></pre>"},{"location":"getting-started/building-bpfman/#install-perl","title":"Install perl","text":"<p>Install <code>perl</code>:</p> <p><code>dnf</code> based OS:</p> <pre><code>sudo dnf install perl\n</code></pre> <p><code>apt</code> based OS:</p> <pre><code>sudo apt install perl\n</code></pre>"},{"location":"getting-started/building-bpfman/#install-docker","title":"Install docker","text":"<p>To build the <code>bpfman-agent</code> and <code>bpfman-operator</code> using the provided Makefile and the <code>make build-images</code> command, <code>docker</code> needs to be installed. There are several existing guides:</p> <ul> <li>Fedora: https://developer.fedoraproject.org/tools/docker/docker-installation.html</li> <li>Linux: https://docs.docker.com/engine/install/</li> </ul>"},{"location":"getting-started/building-bpfman/#install-kind","title":"Install Kind","text":"<p>Optionally, to test <code>bpfman</code> running in Kubernetes, the easiest method and the one documented throughout the <code>bpfman</code> documentation is to run a Kubernetes Kind cluster. See kind for documentation and installation instructions. <code>kind</code> also requires <code>docker</code> to be installed.</p> <p>NOTE: By default, bpfman-operator deploys bpfman with CSI enabled. CSI requires Kubernetes v1.26 due to a PR (kubernetes/kubernetes#112597) that addresses a gRPC Protocol Error that was seen in the CSI client code and it doesn't appear to have been backported. It is recommended to install kind v0.20.0 or later.</p> <p>If the following error is seen, it means there is an older version of Kubernetes running and it needs to be upgraded.</p> <pre><code>kubectl get pods -A\nNAMESPACE   NAME                               READY   STATUS             RESTARTS      AGE\nbpfman      bpfman-daemon-2hnhx                2/3     CrashLoopBackOff   4 (38s ago)   2m20s\nbpfman      bpfman-operator-6b6cf97857-jbvv4   2/2     Running            0             2m22s\n:\n\nkubectl logs -n bpfman bpfman-daemon-2hnhx -c node-driver-registrar\n:\nE0202 15:33:12.342704       1 main.go:101] Received NotifyRegistrationStatus call: &amp;RegistrationStatus{PluginRegistered:false,Error:RegisterPlugin error -- plugin registration failed with err: rpc error: code = Internal desc = stream terminated by RST_STREAM with error code: PROTOCOL_ERROR,}\nE0202 15:33:12.342723       1 main.go:103] Registration process failed with error: RegisterPlugin error -- plugin registration failed with err: rpc error: code = Internal desc = stream terminated by RST_STREAM with error code: PROTOCOL_ERROR, restarting registration container.\n</code></pre>"},{"location":"getting-started/building-bpfman/#install-bash-completion","title":"Install bash-completion","text":"<p><code>bpfman</code> uses the Rust crate <code>clap</code> for the CLI implementation. <code>clap</code> has an optional Rust crate <code>clap_complete</code>. For <code>bash</code> shell, it leverages <code>bash-completion</code> for CLI Command  completion. So in order for CLI  completion to work in a <code>bash</code> shell, <code>bash-completion</code> must be installed. This feature is optional. <p>For the CLI  completion to work after installation, <code>/etc/profile.d/bash_completion.sh</code> must be sourced in the running sessions. New login sessions should pick it up automatically. <p><code>dnf</code> based OS:</p> <pre><code>sudo dnf install bash-completion\nsource /etc/profile.d/bash_completion.sh\n</code></pre> <p><code>apt</code> based OS:</p> <pre><code>sudo apt install bash-completion\nsource /etc/profile.d/bash_completion.sh\n</code></pre>"},{"location":"getting-started/building-bpfman/#install-yaml-formatter","title":"Install Yaml Formatter","text":"<p>As part of CI, the Yaml files are validated with a Yaml formatter. Optionally, to verify locally, install the YAML Language Support by Red Hat VsCode Extension, or to format in bulk, install <code>prettier</code>.</p> <p>To install <code>prettier</code>:</p> <pre><code>npm install -g prettier\n</code></pre> <p>Then to flag which files are violating the formatting guide, run:</p> <pre><code>prettier -l \"*.yaml\"\n</code></pre> <p>And to write changes in place, run:</p> <pre><code> prettier -f \"*.yaml\"\n</code></pre>"},{"location":"getting-started/building-bpfman/#install-toml-formatter","title":"Install toml Formatter","text":"<p>As part of CI, the toml files are validated with a toml formatter. Optionally, to verify locally, install <code>taplo</code>.</p> <pre><code>cargo install taplo-cli\n</code></pre> <p>And to verify locally:</p> <pre><code>taplo fmt --check\n</code></pre>"},{"location":"getting-started/cli-guide/","title":"CLI Guide","text":"<p><code>bpfman</code> offers several CLI commands to interact with the <code>bpfman</code> daemon. The CLI allows you to <code>load</code>, <code>unload</code>, <code>get</code> and <code>list</code> eBPF programs.</p>"},{"location":"getting-started/cli-guide/#notes-for-this-guide","title":"Notes For This Guide","text":"<p>As described in other sections, <code>bpfman</code> can be run as either a privileged process or a systemd service. If run as a privileged process, <code>bpfman</code> will most likely be run from your local development branch and will require <code>sudo</code>. Example:</p> <pre><code>sudo ./target/debug/bpfman list\n</code></pre> <p>If run as a systemd service, <code>bpfman</code> will most likely be installed in your $PATH, and will also require <code>sudo</code>. Example:</p> <pre><code>sudo bpfman list\n</code></pre> <p>The examples here use <code>sudo bpfman</code> in place of <code>sudo ./target/debug/bpfman</code> for readability, use as your system is deployed.</p> <p>eBPF object files used in the examples are taken from the examples and integration-test directories from the <code>bpfman</code> repository.</p>"},{"location":"getting-started/cli-guide/#basic-syntax","title":"Basic Syntax","text":"<p>Below are the commands supported by <code>bpfman</code>.</p> <pre><code>sudo bpfman --help\nAn eBPF manager focusing on simplifying the deployment and administration of eBPF programs.\n\nUsage: bpfman &lt;COMMAND&gt;\n\nCommands:\n  load    Load an eBPF program on the system\n  unload  Unload an eBPF program using the Program Id\n  list    List all eBPF programs loaded via bpfman\n  get     Get an eBPF program using the Program Id\n  image   eBPF Bytecode Image related commands\n  help    Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help\n          Print help (see a summary with '-h')\n</code></pre>"},{"location":"getting-started/cli-guide/#bpfman-load","title":"bpfman load","text":"<p>The <code>bpfman load file</code> and <code>bpfman load image</code> commands are used to load eBPF programs. The <code>bpfman load file</code> command is used to load a locally built eBPF program. The <code>bpfman load image</code> command is used to load an eBPF program packaged in a OCI container image from a given registry. Each program type (i.e. <code>&lt;COMMAND&gt;</code>) has it's own set of attributes specific to the program type, and those attributes MUST come after the program type is entered. There are a common set of attributes, and those MUST come before the program type is entered.</p> <pre><code>sudo bpfman load file --help\nLoad an eBPF program from a local .o file\n\nUsage: bpfman load file [OPTIONS] --path &lt;PATH&gt; --name &lt;NAME&gt; &lt;COMMAND&gt;\n\nCommands:\n  xdp         Install an eBPF program on the XDP hook point for a given interface\n  tc          Install an eBPF program on the TC hook point for a given interface\n  tracepoint  Install an eBPF program on a Tracepoint\n  kprobe      Install a kprobe or kretprobe eBPF probe\n  uprobe      Install a uprobe or uretprobe eBPF probe\n  fentry      Install a fentry eBPF probe\n  fexit       Install a fexit eBPF probe\n  help        Print this message or the help of the given subcommand(s)\n\nOptions:\n  -p, --path &lt;PATH&gt;\n          Required: Location of local bytecode file\n          Example: --path /run/bpfman/examples/go-xdp-counter/bpf_bpfel.o\n\n  -n, --name &lt;NAME&gt;\n          Required: The name of the function that is the entry point for the BPF program\n\n  -g, --global &lt;GLOBAL&gt;...\n          Optional: Global variables to be set when program is loaded.\n          Format: &lt;NAME&gt;=&lt;Hex Value&gt;\n\n          This is a very low level primitive. The caller is responsible for formatting\n          the byte string appropriately considering such things as size, endianness,\n          alignment and packing of data structures.\n\n  -m, --metadata &lt;METADATA&gt;\n          Optional: Specify Key/Value metadata to be attached to a program when it\n          is loaded by bpfman.\n          Format: &lt;KEY&gt;=&lt;VALUE&gt;\n\n          This can later be used to `list` a certain subset of programs which contain\n          the specified metadata.\n          Example: --metadata owner=acme\n\n      --map-owner-id &lt;MAP_OWNER_ID&gt;\n          Optional: Program Id of loaded eBPF program this eBPF program will share a map with.\n          Only used when multiple eBPF programs need to share a map.\n          Example: --map-owner-id 63178\n\n  -h, --help\n          Print help (see a summary with '-h')\n</code></pre> <p>and</p> <pre><code>sudo bpfman load image --help\nLoad an eBPF program packaged in a OCI container image from a given registry\n\nUsage: bpfman load image [OPTIONS] --image-url &lt;IMAGE_URL&gt; &lt;COMMAND&gt;\n\nCommands:\n  xdp         Install an eBPF program on the XDP hook point for a given interface\n  tc          Install an eBPF program on the TC hook point for a given interface\n  tracepoint  Install an eBPF program on a Tracepoint\n  kprobe      Install a kprobe or kretprobe eBPF probe\n  uprobe      Install a uprobe or uretprobe eBPF probe\n  fentry      Install a fentry eBPF probe\n  fexit       Install a fexit eBPF probe\n  help        Print this message or the help of the given subcommand(s)\n\nOptions:\n  -i, --image-url &lt;IMAGE_URL&gt;\n          Required: Container Image URL.\n          Example: --image-url quay.io/bpfman-bytecode/xdp_pass:latest\n\n  -r, --registry-auth &lt;REGISTRY_AUTH&gt;\n          Optional: Registry auth for authenticating with the specified image registry.\n          This should be base64 encoded from the '&lt;username&gt;:&lt;password&gt;' string just like\n          it's stored in the docker/podman host config.\n          Example: --registry_auth \"YnjrcKw63PhDcQodiU9hYxQ2\"\n\n  -p, --pull-policy &lt;PULL_POLICY&gt;\n          Optional: Pull policy for remote images.\n\n          [possible values: Always, IfNotPresent, Never]\n\n          [default: IfNotPresent]\n\n  -n, --name &lt;NAME&gt;\n          Optional: The name of the function that is the entry point for the BPF program.\n          If not provided, the program name defined as part of the bytecode image will be used.\n\n          [default: ]\n\n  -g, --global &lt;GLOBAL&gt;...\n          Optional: Global variables to be set when program is loaded.\n          Format: &lt;NAME&gt;=&lt;Hex Value&gt;\n\n          This is a very low level primitive. The caller is responsible for formatting\n          the byte string appropriately considering such things as size, endianness,\n          alignment and packing of data structures.\n\n  -m, --metadata &lt;METADATA&gt;\n          Optional: Specify Key/Value metadata to be attached to a program when it\n          is loaded by bpfman.\n          Format: &lt;KEY&gt;=&lt;VALUE&gt;\n\n          This can later be used to list a certain subset of programs which contain\n          the specified metadata.\n          Example: --metadata owner=acme\n\n      --map-owner-id &lt;MAP_OWNER_ID&gt;\n          Optional: Program Id of loaded eBPF program this eBPF program will share a map with.\n          Only used when multiple eBPF programs need to share a map.\n          Example: --map-owner-id 63178\n\n  -h, --help\n          Print help (see a summary with '-h')\n</code></pre> <p>When using either load command, <code>--path</code>, <code>--image-url</code>, <code>--registry-auth</code>, <code>--pull-policy</code>, <code>--name</code>,  <code>--global</code>, <code>--metadata</code> and <code>--map-owner-id</code> must be entered before the <code>&lt;COMMAND&gt;</code> (<code>xdp</code>, <code>tc</code>,  <code>tracepoint</code>, etc) is entered. Then each <code>&lt;COMMAND&gt;</code> has its own custom parameters (same for both <code>bpfman load file</code> and <code>bpfman load image</code>):</p> <pre><code>sudo bpfman load file xdp --help\nInstall an eBPF program on the XDP hook point for a given interface\n\nUsage: bpfman load file --path &lt;PATH&gt; --name &lt;NAME&gt; xdp [OPTIONS] --iface &lt;IFACE&gt; --priority &lt;PRIORITY&gt;\n\nOptions:\n  -i, --iface &lt;IFACE&gt;\n          Required: Interface to load program on\n\n  -p, --priority &lt;PRIORITY&gt;\n          Required: Priority to run program in chain. Lower value runs first\n\n      --proceed-on &lt;PROCEED_ON&gt;...\n          Optional: Proceed to call other programs in chain on this exit code.\n          Multiple values supported by repeating the parameter.\n          Example: --proceed-on \"pass\" --proceed-on \"drop\"\n\n          [possible values: aborted, drop, pass, tx, redirect, dispatcher_return]\n\n          [default: pass, dispatcher_return]\n\n  -h, --help\n          Print help (see a summary with '-h')\n</code></pre> <p>Example loading from local file (<code>--path</code> is the fully qualified path):</p> <pre><code>sudo bpfman load file --path $HOME/src/bpfman/tests/integration-test/bpf/.output/xdp_pass.bpf.o --name \"pass\" xdp --iface vethb2795c7 --priority 100\n</code></pre> <p>Example from image in remote repository (Note: <code>--name</code> is built into the image and is not required):</p> <pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethb2795c7 --priority 100\n</code></pre> <p>The <code>tc</code> command is similar to <code>xdp</code>, but it also requires the <code>direction</code> option and the <code>proceed-on</code> values are different.</p> <pre><code>sudo bpfman load file tc -h\nInstall an eBPF program on the TC hook point for a given interface\n\nUsage: bpfman load file --path &lt;PATH&gt; --name &lt;NAME&gt; tc [OPTIONS] --direction &lt;DIRECTION&gt; --iface &lt;IFACE&gt; --priority &lt;PRIORITY&gt;\n\nOptions:\n  -d, --direction &lt;DIRECTION&gt;\n          Required: Direction to apply program.\n\n          [possible values: ingress, egress]\n\n  -i, --iface &lt;IFACE&gt;\n          Required: Interface to load program on\n\n  -p, --priority &lt;PRIORITY&gt;\n          Required: Priority to run program in chain. Lower value runs first\n\n      --proceed-on &lt;PROCEED_ON&gt;...\n          Optional: Proceed to call other programs in chain on this exit code.\n          Multiple values supported by repeating the parameter.\n          Example: --proceed-on \"ok\" --proceed-on \"pipe\"\n\n          [possible values: unspec, ok, reclassify, shot, pipe, stolen, queued,\n                            repeat, redirect, trap, dispatcher_return]\n\n          [default: ok, pipe, dispatcher_return]\n\n  -h, --help\n          Print help (see a summary with '-h')\n</code></pre> <p>The following is an example of the <code>tc</code> command using short option names:</p> <pre><code>sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/tc_pass.bpf.o -n \"pass\" tc -d ingress -i mynet1 -p 40\n</code></pre> <p>For the <code>tc_pass.bpf.o</code> program loaded with the command above, the name would be set as shown in the following snippet:</p> <pre><code>SEC(\"classifier/pass\")\nint accept(struct __sk_buff *skb)\n{\n    :\n}\n</code></pre>"},{"location":"getting-started/cli-guide/#additional-load-examples","title":"Additional Load Examples","text":"<p>Below are some additional examples of <code>bpfman load</code> commands:</p>"},{"location":"getting-started/cli-guide/#fentry","title":"Fentry","text":"<pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/fentry:latest fentry -f do_unlinkat\n</code></pre>"},{"location":"getting-started/cli-guide/#fexit","title":"Fexit","text":"<pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/fexit:latest fexit -f do_unlinkat\n</code></pre>"},{"location":"getting-started/cli-guide/#kprobe","title":"Kprobe","text":"<pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/kprobe:latest kprobe -f try_to_wake_up\n</code></pre>"},{"location":"getting-started/cli-guide/#kretprobe","title":"Kretprobe","text":"<pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/kretprobe:latest kprobe -f try_to_wake_up -r\n</code></pre>"},{"location":"getting-started/cli-guide/#tc","title":"TC","text":"<pre><code>sudo bpfman load file --path $HOME/src/bpfman/examples/go-tc-counter/bpf_bpfel.o --name \"stats\"\" tc --direction ingress --iface vethb2795c7 --priority 110\n</code></pre>"},{"location":"getting-started/cli-guide/#uprobe","title":"Uprobe","text":"<pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/uprobe:latest uprobe -f \"malloc\" -t \"libc\"\n</code></pre>"},{"location":"getting-started/cli-guide/#uretprobe","title":"Uretprobe","text":"<pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/uretprobe:latest uprobe -f \"malloc\" -t \"libc\" -r\n</code></pre>"},{"location":"getting-started/cli-guide/#xdp","title":"XDP","text":"<pre><code>sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o --name \"xdp_stats\" xdp --iface vethb2795c7 --priority 35\n</code></pre>"},{"location":"getting-started/cli-guide/#setting-global-variables-in-ebpf-programs","title":"Setting Global Variables in eBPF Programs","text":"<p>Global variables can be set for any eBPF program type when loading as follows:</p> <pre><code>sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/tc_pass.bpf.o -g GLOBAL_u8=01020304 GLOBAL_u32=0A0B0C0D -n \"pass\" tc -d ingress -i mynet1 -p 40\n</code></pre> <p>Note, that when setting global variables, the eBPF program being loaded must have global variables named with the strings given, and the size of the value provided must match the size of the given variable.  For example, the above command can be used to update the following global variables in an eBPF program.</p> <pre><code>volatile const __u32 GLOBAL_u8 = 0;\nvolatile const __u32 GLOBAL_u32 = 0;\n</code></pre>"},{"location":"getting-started/cli-guide/#modifying-the-proceed-on-behavior","title":"Modifying the Proceed-On Behavior","text":"<p>The <code>proceed-on</code> setting applies to <code>xdp</code> and <code>tc</code> programs. For both of these program types, an ordered list of eBPF programs is maintained per attach point. The <code>proceed-on</code> setting determines whether processing will \"proceed\" to the next eBPF program in the list, or terminate processing and return, based on the program's return value. For example, the default <code>proceed-on</code> configuration for an <code>xdp</code> program can be modified as follows:</p> <pre><code>sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/xdp_pass.bpf.o -n \"pass\" xdp -i mynet1 -p 30 --proceed-on drop pass dispatcher_return\n</code></pre>"},{"location":"getting-started/cli-guide/#sharing-maps-between-ebpf-programs","title":"Sharing Maps Between eBPF Programs","text":"<p>WARNING Currently for the map sharing feature to work the LIBBPF_PIN_BY_NAME flag MUST be set in the shared bpf map definitions. Please see this aya issue for future work that will change this requirement.</p> <p>To share maps between eBPF programs, first load the eBPF program that owns the maps. One eBPF program must own the maps.</p> <pre><code>sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o -n \"xdp_stats\" xdp --iface vethb2795c7 --priority 100\n6371\n</code></pre> <p>Next, load additional eBPF programs that will share the existing maps by passing the program id of the eBPF program that owns the maps using the <code>--map-owner-id</code> parameter:</p> <pre><code>sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o -n \"xdp_stats\" --map-owner-id 6371 xdp --iface vethff657c7 --priority 100\n6373\n</code></pre> <p>Use the <code>bpfman get &lt;PROGRAM_ID&gt;</code> command to display the configuration:</p> <pre><code>sudo bpfman list\n Program ID  Name       Type  Load Time\n 6371        xdp_stats  xdp   2023-07-18T16:50:46-0400\n 6373        xdp_stats  xdp   2023-07-18T16:51:06-0400\n</code></pre> <pre><code>sudo bpfman get 6371\n Bpfman State\n---------------\n Name:          xdp_stats\n Path:          /home/&lt;$USER&gt;/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6371\n Map Owner ID:  None\n Map Used By:   6371\n                6373\n Priority:      50\n Iface:         vethff657c7\n Position:      1\n Proceed On:    pass, dispatcher_return\n:\n</code></pre> <pre><code>sudo bpfman get 6373\n Bpfman State\n---------------\n Name:          xdp_stats\n Path:          /home/&lt;$USER&gt;/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6371\n Map Owner ID:  6371\n Map Used By:   6371\n                6373\n Priority:      50\n Iface:         vethff657c7\n Position:      0\n Proceed On:    pass, dispatcher_return\n:\n</code></pre> <p>As the output shows, the first program (<code>6371</code>) owns the map, with <code>Map Owner ID</code> of <code>None</code> and the <code>Map Pin Path</code> (<code>/run/bpfman/fs/maps/6371</code>) that includes its own ID.</p> <p>The second program (<code>6373</code>) references the first program via the <code>Map Owner ID</code> set to <code>6371</code> and the <code>Map Pin Path</code> (<code>/run/bpfman/fs/maps/6371</code>) set to same directory as the first program, which includes the first program's ID. The output for both commands shows the map is being used by both programs via the <code>Map Used By</code> with values of <code>6371</code> and <code>6373</code>.</p> <p>The eBPF programs can be unloaded any order, the <code>Map Pin Path</code> will not be deleted until all the programs referencing the maps are unloaded:</p> <pre><code>sudo bpfman unload 6371\nsudo bpfman unload 6373\n</code></pre>"},{"location":"getting-started/cli-guide/#bpfman-list","title":"bpfman list","text":"<p>The <code>bpfman list</code> command lists all the bpfman loaded eBPF programs:</p> <pre><code>sudo bpfman list\n Program ID  Name              Type        Load Time\n 6201        pass              xdp         2023-07-17T17:17:53-0400\n 6202        sys_enter_openat  tracepoint  2023-07-17T17:19:09-0400\n 6204        stats             tc          2023-07-17T17:20:14-0400\n</code></pre> <p>To see all eBPF programs loaded on the system, include the <code>--all</code> option.</p> <pre><code>sudo bpfman list --all\n Program ID  Name              Type           Load Time\n 52          restrict_filesy   lsm            2023-05-03T12:53:34-0400\n 166         dump_bpf_map      tracing        2023-05-03T12:53:52-0400\n 167         dump_bpf_prog     tracing        2023-05-03T12:53:52-0400\n 455                           cgroup_device  2023-05-03T12:58:26-0400\n :\n 6194                          cgroup_device  2023-07-17T17:15:23-0400\n 6201        pass              xdp            2023-07-17T17:17:53-0400\n 6202        sys_enter_openat  tracepoint     2023-07-17T17:19:09-0400\n 6203        dispatcher        tc             2023-07-17T17:20:14-0400\n 6204        stats             tc             2023-07-17T17:20:14-0400\n 6207        xdp               xdp            2023-07-17T17:27:13-0400\n 6210        test_fentry       tracing        2023-07-17T17:28:34-0400\n 6212        test_fexit        tracing        2023-07-17T17:29:02-0400\n 6223        my_uprobe         probe          2023-07-17T17:31:45-0400\n 6225        my_kretprobe      probe          2023-07-17T17:32:27-0400\n 6928        my_kprobe         probe          2023-07-17T17:33:49-0400\n</code></pre> <p>To filter on a given program type, include the <code>--program-type</code> parameter:</p> <pre><code>sudo bpfman list --all --program-type tc\n Program ID  Name        Type  Load Time\n 6203        dispatcher  tc    2023-07-17T17:20:14-0400\n 6204        stats       tc    2023-07-17T17:20:14-0400\n</code></pre> <p>Note: The list filters by the Kernel Program Type. <code>kprobe</code>, <code>kretprobe</code>, <code>uprobe</code> and <code>uretprobe</code> all map to the <code>probe</code> Kernel Program Type. <code>fentry</code> and <code>fexit</code> both map to the <code>tracing</code> Kernel Program Type.</p>"},{"location":"getting-started/cli-guide/#bpfman-get","title":"bpfman get","text":"<p>To retrieve detailed information for a loaded eBPF program, use the <code>bpfman get &lt;PROGRAM_ID&gt;</code> command. If the eBPF program was loaded via bpfman, then there will be a <code>Bpfman State</code> section with bpfman related attributes and a <code>Kernel State</code> section with kernel information. If the eBPF program was loaded outside of bpfman, then the <code>Bpfman State</code> section will be empty and <code>Kernel State</code> section will be populated.</p> <pre><code>sudo bpfman get 6204\n Bpfman State\n---------------\n Name:          stats\n Image URL:     quay.io/bpfman-bytecode/go-tc-counter:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6204\n Map Owner ID:  None\n Map Used By:   6204\n Priority:      100\n Iface:         vethff657c7\n Position:      0\n Direction:     eg\n Proceed On:    pipe, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6204\n Name:                             stats\n Type:                             tc\n Loaded At:                        2023-07-17T17:20:14-0400\n Tag:                              ead94553702a3742\n GPL Compatible:                   true\n Map IDs:                          [2705]\n BTF ID:                           2821\n Size Translated (bytes):          176\n JITed:                            true\n Size JITed (bytes):               116\n Kernel Allocated Memory (bytes):  4096\n Verified Instruction Count:       24\n</code></pre> <pre><code>sudo bpfman get 6190\n Bpfman State\n---------------\nNONE\n\n Kernel State\n----------------------------------\nProgram ID:                        6190\nName:                              None\nType:                              cgroup_skb\nLoaded At:                         2023-07-17T17:15:23-0400\nTag:                               6deef7357e7b4530\nGPL Compatible:                    true\nMap IDs:                           []\nBTF ID:                            0\nSize Translated (bytes):           64\nJITed:                             true\nSize JITed (bytes):                55\nKernel Allocated Memory (bytes):   4096\nVerified Instruction Count:        8\n</code></pre>"},{"location":"getting-started/cli-guide/#bpfman-unload","title":"bpfman unload","text":"<p>The <code>bpfman unload</code> command takes the program id from the load or list command as a parameter, and unloads the requested eBPF program:</p> <pre><code>sudo bpfman unload 6204\n</code></pre> <pre><code>sudo bpfman list\n Program ID  Name              Type        Load Time\n 6201        pass              xdp         2023-07-17T17:17:53-0400\n 6202        sys_enter_openat  tracepoint  2023-07-17T17:19:09-0400\n</code></pre>"},{"location":"getting-started/cli-guide/#bpfman-image-pull","title":"bpfman image pull","text":"<p>The <code>bpfman image pull</code> command pulls a given bytecode image for future use by a load command.</p> <pre><code>sudo bpfman image pull --help\nPull an eBPF bytecode image from a remote registry\n\nUsage: bpfman image pull [OPTIONS] --image-url &lt;IMAGE_URL&gt;\n\nOptions:\n  -i, --image-url &lt;IMAGE_URL&gt;\n          Required: Container Image URL.\n          Example: --image-url quay.io/bpfman-bytecode/xdp_pass:latest\n\n  -r, --registry-auth &lt;REGISTRY_AUTH&gt;\n          Optional: Registry auth for authenticating with the specified image registry.\n          This should be base64 encoded from the '&lt;username&gt;:&lt;password&gt;' string just like\n          it's stored in the docker/podman host config.\n          Example: --registry_auth \"YnjrcKw63PhDcQodiU9hYxQ2\"\n\n  -p, --pull-policy &lt;PULL_POLICY&gt;\n          Optional: Pull policy for remote images.\n\n          [possible values: Always, IfNotPresent, Never]\n\n          [default: IfNotPresent]\n\n  -h, --help\n          Print help (see a summary with '-h')\n</code></pre> <p>Example usage:</p> <pre><code>sudo bpfman image pull --image-url quay.io/bpfman-bytecode/xdp_pass:latest\nSuccessfully downloaded bytecode\n</code></pre> <p>Then when loaded, the local image will be used:</p> <pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest --pull-policy IfNotPresent xdp --iface vethff657c7 --priority 100\n Bpfman State                                           \n ---------------\n Name:          pass                                  \n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest \n Pull Policy:   IfNotPresent                          \n Global:        None                                  \n Metadata:      None                                  \n Map Pin Path:  /run/bpfman/fs/maps/406681              \n Map Owner ID:  None                                  \n Maps Used By:  None                                  \n Priority:      100                                   \n Iface:         vethff657c7                           \n Position:      2                                     \n Proceed On:    pass, dispatcher_return               \n\n Kernel State                                               \n ----------------------------------\n Program ID:                       406681                   \n Name:                             pass                     \n Type:                             xdp                      \n Loaded At:                        1917-01-27T01:37:06-0500 \n Tag:                              4b9d1b2c140e87ce         \n GPL Compatible:                   true                     \n Map IDs:                          [736646]                 \n BTF ID:                           555560                   \n Size Translated (bytes):          96                       \n JITted:                           true                     \n Size JITted:                      67                       \n Kernel Allocated Memory (bytes):  4096                     \n Verified Instruction Count:       9                        \n</code></pre>"},{"location":"getting-started/example-bpf-k8s/","title":"Deploying Example eBPF Programs On Kubernetes","text":"<p>This section will describe launching eBPF enabled applications on a Kubernetes cluster. The approach is slightly different when running on a Kubernetes cluster.</p> <p>This section assumes there is already a Kubernetes cluster running and <code>bpfman</code> is running in the cluster. See Deploying the bpfman-operator for details on deploying bpfman on a Kubernetes cluster, but the quickest solution is to run a Kubernetes KIND Cluster:</p> <pre><code>cd bpfman/bpfman-operator/\nmake run-on-kind\n</code></pre>"},{"location":"getting-started/example-bpf-k8s/#loading-ebpf-programs-on-kubernetes","title":"Loading eBPF Programs On Kubernetes","text":"<p>Instead of using the userspace program or CLI to load the eBPF bytecode as done in previous sections, the bytecode will be loaded by creating a Kubernetes CRD object. There is a CRD object for each eBPF program type bpfman supports.</p> <ul> <li>FentryProgram CRD: Fentry Sample yaml</li> <li>FexitProgram CRD: Fexit Sample yaml</li> <li>KprobeProgram CRD: Kprobe Examples yaml</li> <li>TcProgram CRD: TcProgram Examples yaml</li> <li>TracepointProgram CRD: Tracepoint Examples yaml</li> <li>UprobeProgram CRD: Uprobe Examples yaml</li> <li>XdpProgram CRD: XdpProgram Examples yaml</li> </ul> <p>Sample bytecode yaml with XdpProgram CRD: <pre><code>cat examples/config/base/go-xdp-counter/bytecode.yaml\napiVersion: bpfman.io/v1alpha1\nkind: XdpProgram\nmetadata:\n  labels:\n    app.kubernetes.io/name: xdpprogram\n  name: go-xdp-counter-example\nspec:\n  name: xdp_stats\n  # Select all nodes\n  nodeselector: {}\n  interfaceselector:\n    primarynodeinterface: true\n  priority: 55\n  bytecode:\n    image:\n      url: quay.io/bpfman-bytecode/go-xdp-counter:latest\n</code></pre></p> <p>Note that all the sample yaml files are configured with the bytecode running on all nodes (<code>nodeselector: {}</code>). This can be configured to run on specific nodes, but the DaemonSet yaml for the userspace program, which is described below, should have an equivalent change.</p> <p>Assume the following command is run:</p> <pre><code>kubectl apply -f examples/config/base/go-xdp-counter/bytecode.yaml\n  xdpprogram.bpfman.io/go-xdp-counter-example created\n</code></pre> <p>The diagram below shows <code>go-xdp-counter</code> example, but the other examples operate in a similar fashion.</p> <p></p> <p>Following the diagram for XDP example (Blue numbers):</p> <ol> <li>The user creates a <code>XdpProgram</code> object with the parameters associated with the eBPF bytecode, like interface, priority and BFP bytecode image. The name of the <code>XdpProgram</code> object in this example is <code>go-xdp-counter-example</code>. The <code>XdpProgram</code> is applied using <code>kubectl</code>, but in a more practical deployment, the <code>XdpProgram</code> would be applied by the application or a controller.</li> <li><code>bpfman-agent</code>, running on each node, is watching for all changes to <code>XdpProgram</code> objects. When it sees a <code>XdpProgram</code> object created or modified, it makes sure a <code>BpfProgram</code> object for that node exists. The name of the <code>BpfProgram</code> object is the <code>XdpProgram</code> object name with the node name and interface or attach point appended. On a KIND Cluster, it would be similar to <code>go-xdp-counter-example-bpfman-deployment-control-plane-eth0</code>.</li> <li><code>bpfman-agent</code> then determines if it should be running on the given node, loads or unloads as needed by making gRPC calls the <code>bpfman-rpc</code>, which calls into the <code>bpfman</code> Library. <code>bpfman</code> behaves the same as described in the running locally example.</li> <li><code>bpfman-agent</code> finally updates the status of the <code>BpfProgram</code> object.</li> <li><code>bpfman-operator</code> watches all <code>BpfProgram</code> objects, and updates the status of the <code>XdpProgram</code> object indicating if the eBPF program has been applied to all the desired nodes or not.</li> </ol> <p>To retrieve information on the <code>XdpProgram</code> objects:</p> <pre><code>kubectl get xdpprograms\nNAME                     BPFFUNCTIONNAME   NODESELECTOR   STATUS\ngo-xdp-counter-example   xdp_stats         {}             ReconcileSuccess\n\n\nkubectl get xdpprograms go-xdp-counter-example -o yaml\napiVersion: bpfman.io/v1alpha1\nkind: XdpProgram\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"bpfman.io/v1alpha1\",\"kind\":\"XdpProgram\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/name\":\"xdpprogram\"},\"name\":\"go-xdp-counter-example\"},\"spec\":{\"bpffunctionname\":\"xdp_stats\",\"bytecode\":{\"image\":{\"url\":\"quay.io/bpfman-bytecode/go-xdp-counter:latest\"}},\"interfaceselector\":{\"primarynodeinterface\":true},\"nodeselector\":{},\"priority\":55}}\n  creationTimestamp: \"2023-11-06T21:05:15Z\"\n  finalizers:\n  - bpfman.io.operator/finalizer\n  generation: 2\n  labels:\n    app.kubernetes.io/name: xdpprogram\n  name: go-xdp-counter-example\n  resourceVersion: \"3103\"\n  uid: edd45e2e-a40b-4668-ac76-c1f1eb63a23b\nspec:\n  bpffunctionname: xdp_stats\n  bytecode:\n    image:\n      imagepullpolicy: IfNotPresent\n      url: quay.io/bpfman-bytecode/go-xdp-counter:latest\n  interfaceselector:\n    primarynodeinterface: true\n  mapownerselector: {}\n  nodeselector: {}\n  priority: 55\n  proceedon:\n  - pass\n  - dispatcher_return\nstatus:\n  conditions:\n  - lastTransitionTime: \"2023-11-06T21:05:21Z\"\n    message: bpfProgramReconciliation Succeeded on all nodes\n    reason: ReconcileSuccess\n    status: \"True\"\n    type: ReconcileSuccess\n</code></pre> <p>To retrieve information on the <code>BpfProgram</code> objects:</p> <pre><code>kubectl get bpfprograms\nNAME                                                          TYPE      STATUS         AGE\n:\ngo-xdp-counter-example-bpfman-deployment-control-plane-eth0   xdp       bpfmanLoaded   11m\n\n\nkubectl get bpfprograms go-xdp-counter-example-bpfman-deployment-control-plane-eth0 -o yaml\napiVersion: bpfman.io/v1alpha1\nkind: BpfProgram\nmetadata:\n  annotations:\n    bpfman.io.xdpprogramcontroller/interface: eth0\n    bpfman.io/ProgramId: \"4801\"\n  creationTimestamp: \"2023-11-06T21:05:15Z\"\n  finalizers:\n  - bpfman.io.xdpprogramcontroller/finalizer\n  generation: 1\n  labels:\n    bpfman.io/ownedByProgram: go-xdp-counter-example\n    kubernetes.io/hostname: bpfman-deployment-control-plane\n  name: go-xdp-counter-example-bpfman-deployment-control-plane-eth0\n  ownerReferences:\n  - apiVersion: bpfman.io/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: XdpProgram\n    name: go-xdp-counter-example\n    uid: edd45e2e-a40b-4668-ac76-c1f1eb63a23b\n  resourceVersion: \"3102\"\n  uid: f7ffd156-168b-4dc8-be38-18c42626a631\nspec:\n  type: xdp\nstatus:\n  conditions:\n  - lastTransitionTime: \"2023-11-06T21:05:21Z\"\n    message: Successfully loaded bpfProgram\n    reason: bpfmanLoaded\n    status: \"True\"\n    type: Loaded\n</code></pre>"},{"location":"getting-started/example-bpf-k8s/#deploying-an-ebpf-enabled-application-on-kubernetes","title":"Deploying an eBPF enabled application On Kubernetes","text":"<p>Here, a userspace container is deployed to consume the map data generated by the eBPF counter program. bpfman provides a Container Storage Interface (CSI) driver for exposing eBPF maps into a userspace container. To avoid having to mount a host directory that contains the map pinned file into the container and forcing the container to have permissions to access that host directory, the CSI driver mounts the map at a specified location in the container. All the examples use CSI, here is go-xdp-counter/deployment.yaml for reference:</p> <pre><code>cd bpfman/examples/\ncat config/base/go-xdp-counter/deployment.yaml\n:\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: go-xdp-counter-ds\n  namespace: go-xdp-counter\n  labels:\n    k8s-app: go-xdp-counter\nspec:\n  :\n  template:\n    :\n    spec:\n       :\n      containers:\n      - name: go-xdp-counter\n        :\n        volumeMounts:\n        - name: go-xdp-counter-maps                        &lt;==== 2) VolumeMount in container\n          mountPath: /run/xdp/maps                         &lt;==== 2a) Mount path in the container\n          readOnly: true\n      volumes:\n      - name: go-xdp-counter-maps                          &lt;==== 1) Volume describing the map\n        csi:\n          driver: csi.bpfman.io                             &lt;==== 1a) bpfman CSI Driver\n          volumeAttributes:\n            csi.bpfman.io/program: go-xdp-counter-example   &lt;==== 1b) eBPF Program owning the map\n            csi.bpfman.io/maps: xdp_stats_map               &lt;==== 1c) Map to be exposed to the container\n</code></pre>"},{"location":"getting-started/example-bpf-k8s/#loading-a-userspace-container-image","title":"Loading A Userspace Container Image","text":"<p>The userspace programs have been pre-built and can be found here:</p> <ul> <li>quay.io/bpfman-userspace/go-kprobe-counter:latest</li> <li>quay.io/bpfman-userspace/go-tc-counter:latest</li> <li>quay.io/bpfman-userspace/go-tracepoint-counter:latest</li> <li>quay.io/bpfman-userspace/go-uprobe-counter:latest</li> <li>quay.io/bpfman-userspace/go-xdp-counter:latest</li> </ul> <p>The example yaml files below are loading from these image.</p> <ul> <li>go-kprobe-counter/deployment.yaml</li> <li>go-tc-counter/deployment.yaml</li> <li>go-tracepoint-counter/deployment.yaml</li> <li>go-uprobe-counter/deployment.yaml</li> <li>go-xdp-counter/deployment.yaml</li> </ul> <p>The userspace program in a Kubernetes Deployment doesn't interacts directly with <code>bpfman</code> like it did in the local host deployment. Instead, the userspace program running on each node, if needed, reads the <code>BpfProgram</code> object from the KubeApiServer to gather additional information about the loaded eBPF program. To interact with the KubeApiServer, RBAC must be setup properly to access the <code>BpfProgram</code> object. The <code>bpfman-operator</code> defined the yaml for several ClusterRoles that can be used to access the different <code>bpfman</code> related CRD objects with different access rights. The example userspace containers will use the <code>bpfprogram-viewer-role</code>, which allows Read-Only access to the <code>BpfProgram</code> object. This ClusterRole is created automatically by the <code>bpfman-operator</code>.</p> <p>The remaining objects (NameSpace, ServiceAccount, ClusterRoleBinding and examples DaemonSet) can be created for each program type as follows:</p> <pre><code>cd bpfman/\nkubectl create -f examples/config/base/go-xdp-counter/deployment.yaml\n</code></pre> <p>This creates the <code>go-xdp-counter</code> userspace pod, but the other examples operate in a similar fashion.</p> <p></p> <p>Following the diagram for the XDP example (Green numbers):</p> <ol> <li>The userspace program queries the KubeApiServer for a specific <code>BpfProgram</code> object.</li> <li>The userspace program verifies the <code>BpfProgram</code> has been loaded and uses the map to periodically read the counter values.</li> </ol> <p>To see if the userspace programs are working, view the logs:</p> <pre><code>kubectl get pods -A\nNAMESPACE               NAME                              READY   STATUS    RESTARTS   AGE\nbpfman                  bpfman-daemon-jsgdh               3/3     Running   0          11m\nbpfman                  bpfman-operator-6c5c8887f7-qk28x  2/2     Running   0          12m\ngo-xdp-counter          go-xdp-counter-ds-2hs6g           1/1     Running   0          6m12s\n:\n\nkubectl logs -n go-xdp-counter go-xdp-counter-ds-2hs6g\n2023/11/06 20:27:16 2429 packets received\n2023/11/06 20:27:16 1328474 bytes received\n\n2023/11/06 20:27:19 2429 packets received\n2023/11/06 20:27:19 1328474 bytes received\n\n2023/11/06 20:27:22 2430 packets received\n2023/11/06 20:27:22 1328552 bytes received\n:\n</code></pre> <p>To cleanup:</p> <pre><code>kubectl delete -f examples/config/base/go-xdp-counter/deployment.yaml\nkubectl delete -f examples/config/base/go-xdp-counter/bytecode.yaml\n</code></pre>"},{"location":"getting-started/example-bpf-k8s/#automated-deployment","title":"Automated Deployment","text":"<p>The steps above are automated in the <code>Makefile</code> in the examples directory. Run <code>make deploy</code> to load each of the example bytecode and userspace yaml files, then <code>make undeploy</code> to unload them.</p> <pre><code>cd bpfman/examples/\nmake deploy\n  for target in deploy-tc deploy-tracepoint deploy-xdp deploy-xdp-ms deploy-kprobe deploy-target deploy-uprobe ; do \\\n      make $target  || true; \\\n  done\n  make[1]: Entering directory '/home/bmcfall/go/src/github.com/bpfman/bpfman/examples'\n  sed 's@URL_BC@quay.io/bpfman-bytecode/go-tc-counter:latest@' config/default/go-tc-counter/patch.yaml.env &gt; config/default/go-tc-counter/patch.yaml\n  cd config/default/go-tc-counter &amp;&amp; /home/bmcfall/go/src/github.com/bpfman/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tc-counter=quay.io/bpfman-userspace/go-tc-counter:latest\n  namespace/go-tc-counter created\n  serviceaccount/bpfman-app-go-tc-counter created\n  daemonset.apps/go-tc-counter-ds created\n  tcprogram.bpfman.io/go-tc-counter-example created\n  :\n  sed 's@URL_BC@quay.io/bpfman-bytecode/go-uprobe-counter:latest@' config/default/go-uprobe-counter/patch.yaml.env &gt; config/default/go-uprobe-counter/patch.yaml\n  cd config/default/go-uprobe-counter &amp;&amp; /home/bmcfall/go/src/github.com/bpfman/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-uprobe-counter=quay.io/bpfman-userspace/go-uprobe-counter:latest\n  namespace/go-uprobe-counter created\n  serviceaccount/bpfman-app-go-uprobe-counter created\n  daemonset.apps/go-uprobe-counter-ds created\n  uprobeprogram.bpfman.io/go-uprobe-counter-example created\n  make[1]: Leaving directory '/home/bmcfall/go/src/github.com/bpfman/bpfman/examples'\n\n# Test Away ...\n\nkubectl get pods -A\nNAMESPACE               NAME                                                      READY   STATUS    RESTARTS   AGE\nbpfman                  bpfman-daemon-md2c5                                       3/3     Running   0          2d17h\nbpfman                  bpfman-operator-7f67bc7c57-95zf7                          2/2     Running   0          2d17h\ngo-kprobe-counter       go-kprobe-counter-ds-8dkls                                1/1     Running   0          2m14s\ngo-target               go-target-ds-nbdf5                                        1/1     Running   0          2m14s\ngo-tc-counter           go-tc-counter-ds-7mtcw                                    1/1     Running   0          2m19s\ngo-tracepoint-counter   go-tracepoint-counter-ds-bcbs7                            1/1     Running   0          2m18s\ngo-uprobe-counter       go-uprobe-counter-ds-j26hc                                1/1     Running   0          2m13s\ngo-xdp-counter          go-xdp-counter-ds-nls6s                                   1/1     Running   0          2m17s\n\nkubectl get bpfprograms\nNAME                                                                                                TYPE         STATUS         AGE\ngo-kprobe-counter-example-bpfman-deployment-control-plane-try-to-wake-up                            kprobe       bpfmanLoaded   2m41s\ngo-tc-counter-example-bpfman-deployment-control-plane-eth0                                          tc           bpfmanLoaded   2m46s\ngo-tracepoint-counter-example-bpfman-deployment-control-plane-syscalls-sys-enter-kill               tracepoint   bpfmanLoaded   2m35s\ngo-uprobe-counter-example-bpfman-deployment-control-plane--go-target-go-target-ds-nbdf5-go-target   uprobe       bpfmanLoaded   2m29s\ngo-xdp-counter-example-bpfman-deployment-control-plane-eth0                                         xdp          bpfmanLoaded   2m24s\ngo-xdp-counter-sharing-map-example-bpfman-deployment-control-plane-eth0                             xdp          bpfmanLoaded   2m21s\n\nmake undeploy\n  for target in undeploy-tc undeploy-tracepoint undeploy-xdp undeploy-xdp-ms undeploy-kprobe undeploy-uprobe undeploy-target ; do \\\n      make $target  || true; \\\n  done\n  make[1]: Entering directory '/home/bmcfall/go/src/github.com/bpfman/bpfman/examples'\n  sed 's@URL_BC@quay.io/bpfman-bytecode/go-tc-counter:latest@' config/default/go-tc-counter/patch.yaml.env &gt; config/default/go-tc-counter/patch.yaml\n  cd config/default/go-tc-counter &amp;&amp; /home/bmcfall/go/src/github.com/bpfman/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tc-counter=quay.io/bpfman-userspace/go-tc-counter:latest\n  namespace \"go-tc-counter\" deleted\n  serviceaccount \"bpfman-app-go-tc-counter\" deleted\n  daemonset.apps \"go-tc-counter-ds\" deleted\n  tcprogram.bpfman.io \"go-tc-counter-example\" deleted\n  :\n  kubectl delete -f config/base/go-target/deployment.yaml\n  namespace \"go-target\" deleted\n  serviceaccount \"bpfman-app-go-target\" deleted\n  daemonset.apps \"go-target-ds\" deleted\n  make[1]: Leaving directory '/home/bmcfall/go/src/github.com/bpfman/bpfman/examples'\n</code></pre> <p>Individual examples can be loaded and unloaded as well, for example <code>make deploy-xdp</code> and <code>make undeploy-xdp</code>. To see the full set of available commands, run <code>make help</code>:</p> <pre><code>make help\n\nUsage:\n  make &lt;target&gt;\n  make deploy TAG=v0.2.0\n  make deploy-xdp IMAGE_XDP_US=quay.io/user1/go-xdp-counter-userspace:test\n\nGeneral\n  help             Display this help.\n\nLocal Dependencies\n  kustomize        Download kustomize locally if necessary.\n\nDevelopment\n  fmt              Run go fmt against code.\n  verify           Verify all the autogenerated code\n\nBuild\n  build            Build all the userspace example code.\n  generate         Run `go generate` to build the bytecode for each of the examples.\n  build-us-images  Build all example userspace images\n  build-bc-images  Build bytecode example userspace images\n  push-us-images   Push all example userspace images\n  push-bc-images   Push all example bytecode images\n  load-us-images-kind  Build and load all example userspace images into kind\n\nDeployment Variables (not commands)\n  TAG              Used to set all images to a fixed tag. Example: make deploy TAG=v0.2.0\n  IMAGE_TC_BC      TC Bytecode image. Example: make deploy-tc IMAGE_TC_BC=quay.io/user1/go-tc-counter-bytecode:test\n  IMAGE_TC_US      TC Userspace image. Example: make deploy-tc IMAGE_TC_US=quay.io/user1/go-tc-counter-userspace:test\n  IMAGE_TP_BC      Tracepoint Bytecode image. Example: make deploy-tracepoint IMAGE_TP_BC=quay.io/user1/go-tracepoint-counter-bytecode:test\n  IMAGE_TP_US      Tracepoint Userspace image. Example: make deploy-tracepoint IMAGE_TP_US=quay.io/user1/go-tracepoint-counter-userspace:test\n  IMAGE_XDP_BC     XDP Bytecode image. Example: make deploy-xdp IMAGE_XDP_BC=quay.io/user1/go-xdp-counter-bytecode:test\n  IMAGE_XDP_US     XDP Userspace image. Example: make deploy-xdp IMAGE_XDP_US=quay.io/user1/go-xdp-counter-userspace:test\n  IMAGE_KP_BC      Kprobe Bytecode image. Example: make deploy-kprobe IMAGE_KP_BC=quay.io/user1/go-kprobe-counter-bytecode:test\n  IMAGE_KP_US      Kprobe Userspace image. Example: make deploy-kprobe IMAGE_KP_US=quay.io/user1/go-kprobe-counter-userspace:test\n  IMAGE_UP_BC      Uprobe Bytecode image. Example: make deploy-uprobe IMAGE_UP_BC=quay.io/user1/go-uprobe-counter-bytecode:test\n  IMAGE_UP_US      Uprobe Userspace image. Example: make deploy-uprobe IMAGE_UP_US=quay.io/user1/go-uprobe-counter-userspace:test\n  IMAGE_GT_US      Uprobe Userspace target. Example: make deploy-target IMAGE_GT_US=quay.io/user1/go-target-userspace:test\n  KIND_CLUSTER_NAME  Name of the deployed cluster to load example images to, defaults to `bpfman-deployment`\n  ignore-not-found  For any undeploy command, set to true to ignore resource not found errors during deletion. Example: make undeploy ignore-not-found=true\n\nDeployment\n  deploy-tc        Deploy go-tc-counter to the cluster specified in ~/.kube/config.\n  undeploy-tc      Undeploy go-tc-counter from the cluster specified in ~/.kube/config.\n  deploy-tracepoint  Deploy go-tracepoint-counter to the cluster specified in ~/.kube/config.\n  undeploy-tracepoint  Undeploy go-tracepoint-counter from the cluster specified in ~/.kube/config.\n  deploy-xdp       Deploy go-xdp-counter to the cluster specified in ~/.kube/config.\n  undeploy-xdp     Undeploy go-xdp-counter from the cluster specified in ~/.kube/config.\n  deploy-xdp-ms    Deploy go-xdp-counter-sharing-map (shares map with go-xdp-counter) to the cluster specified in ~/.kube/config.\n  undeploy-xdp-ms  Undeploy go-xdp-counter-sharing-map from the cluster specified in ~/.kube/config.\n  deploy-kprobe    Deploy go-kprobe-counter to the cluster specified in ~/.kube/config.\n  undeploy-kprobe  Undeploy go-kprobe-counter from the cluster specified in ~/.kube/config.\n  deploy-uprobe    Deploy go-uprobe-counter to the cluster specified in ~/.kube/config.\n  undeploy-uprobe  Undeploy go-uprobe-counter from the cluster specified in ~/.kube/config.\n  deploy-target    Deploy go-target to the cluster specified in ~/.kube/config.\n  undeploy-target  Undeploy go-target from the cluster specified in ~/.kube/config.\n  deploy           Deploy all examples to the cluster specified in ~/.kube/config.\n  undeploy         Undeploy all examples to the cluster specified in ~/.kube/config.\n</code></pre>"},{"location":"getting-started/example-bpf-k8s/#building-a-userspace-container-image","title":"Building A Userspace Container Image","text":"<p>To build the userspace examples in a container instead of using the pre-built ones, from the bpfman examples code source directory, run the following build command:</p> <pre><code>cd bpfman/examples\nmake \\\n  IMAGE_KP_US=quay.io/$USER/go-kprobe-counter:latest \\\n  IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\\n  IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\\n  IMAGE_UP_US=quay.io/$USER/go-uprobe-counter:latest \\\n  IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\\n  build-us-images\n</code></pre> <p>Then EITHER push images to a remote repository:</p> <pre><code>docker login quay.io\ncd bpfman/examples\nmake \\\n  IMAGE_KP_US=quay.io/$USER/go-kprobe-counter:latest \\\n  IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\\n  IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\\n  IMAGE_UP_US=quay.io/$USER/go-uprobe-counter:latest \\\n  IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\\n  push-us-images\n</code></pre> <p>OR load the images directly to a specified kind cluster:</p> <pre><code>cd bpfman/examples\nmake \\\n  IMAGE_KP_US=quay.io/$USER/go-kprobe-counter:latest \\\n  IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\\n  IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\\n  IMAGE_UP_US=quay.io/$USER/go-uprobe-counter:latest \\\n  IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\\n  KIND_CLUSTER_NAME=bpfman-deployment \\\n  load-us-images-kind\n</code></pre> <p>Lastly, update the yaml to use the private images or override the yaml files using the Makefile:</p> <pre><code>cd bpfman/examples/\n\nmake deploy-kprobe IMAGE_XDP_US=quay.io/$USER/go-kprobe-counter:latest\nmake undeploy-kprobe\n\nmake deploy-tc IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest\nmake undeploy-tc\n\nmake deploy-tracepoint IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest\nmake undeploy-tracepoint\n\nmake deploy-uprobe IMAGE_XDP_US=quay.io/$USER/go-uprobe-counter:latest\nmake undeploy-uprobe\n\nmake deploy-xdp IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest\nmake undeploy-xdp\n</code></pre>"},{"location":"getting-started/example-bpf-local/","title":"Deploying Example eBPF Programs On Local Host","text":"<p>This section describes running bpfman and the example eBPF programs on a local host.</p>"},{"location":"getting-started/example-bpf-local/#example-overview","title":"Example Overview","text":"<p>Assume the following command is run:</p> <pre><code>cd bpfman/examples/go-xdp-counter/\nsudo ./go-xdp-counter -iface eno3\n</code></pre> <p>The diagram below shows <code>go-xdp-counter</code> example, but the other examples operate in a similar fashion.</p> <p></p> <p>Following the diagram (Purple numbers):</p> <ol> <li>When <code>go-xdp-counter</code> userspace is started, it will send a gRPC request over unix    socket to <code>bpfman-rpc</code> requesting <code>bpfman</code> to load the <code>go-xdp-counter</code> eBPF bytecode located    on disk at <code>bpfman/examples/go-xdp-counter/bpf_bpfel.o</code> at a priority of 50 and on interface <code>eno3</code>.    These values are configurable as we will see later, but for now we will use the defaults    (except interface, which is required to be entered).</li> <li><code>bpfman</code> will load it's <code>dispatcher</code> eBPF program, which links to the <code>go-xdp-counter</code> eBPF program    and return a kernel Program ID referencing the running program.</li> <li><code>bpfman list</code> can be used to show that the eBPF program was loaded.</li> <li>Once the <code>go-xdp-counter</code> eBPF bytecode is loaded, the eBPF program will write packet counts    and byte counts to a shared map.</li> <li><code>go-xdp-counter</code> userspace program periodically reads counters from the shared map and logs    the value.</li> </ol> <p>Below are the steps to run the example program described above and then some additional examples that use the <code>bpfman</code> CLI to load and unload other eBPF programs. See Launching bpfman for more detailed instructions on building and loading bpfman. This tutorial assumes bpfman has been built, <code>bpfman-rpc</code> is running, and the <code>bpfman</code> CLI is in $PATH.</p>"},{"location":"getting-started/example-bpf-local/#running-example-programs","title":"Running Example Programs","text":"<p>Example eBPF Programs describes how the example programs work, how to build them, and how to run the different examples. Build the <code>go-xdp-counter</code> program before continuing.</p> <p>To run the <code>go-xdp-counter</code> program, determine the host interface to attach the eBPF program to and then start the go program. In this example, <code>eno3</code> will be used, as shown in the diagram at the top of the page.  The output should show the count and total bytes of packets as they pass through the interface as shown below:</p> <pre><code>sudo ./go-xdp-counter --iface eno3\n2023/07/17 17:43:58 Using Input: Interface=eno3 Priority=50 Source=/home/&lt;$USER&gt;/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o\n2023/07/17 17:43:58 Program registered with id 6211\n2023/07/17 17:44:01 4 packets received\n2023/07/17 17:44:01 580 bytes received\n\n2023/07/17 17:44:04 4 packets received\n2023/07/17 17:44:04 580 bytes received\n\n2023/07/17 17:44:07 8 packets received\n2023/07/17 17:44:07 1160 bytes received\n\n:\n</code></pre> <p>In another terminal, use the CLI to show the <code>go-xdp-counter</code> eBPF bytecode was loaded.</p> <pre><code>sudo bpfman list\n Program ID  Name       Type  Load Time\n 6211        xdp_stats  xdp   2023-07-17T17:43:58-0400\n</code></pre> <p>Finally, press <code>&lt;CTRL&gt;+c</code> when finished with <code>go-xdp-counter</code>.</p> <pre><code>:\n\n2023/07/17 17:44:34 28 packets received\n2023/07/17 17:44:34 4060 bytes received\n\n^C2023/07/17 17:44:35 Exiting...\n2023/07/17 17:44:35 Unloading Program: 6211\n</code></pre>"},{"location":"getting-started/example-bpf-local/#using-cli-to-manage-ebpf-programs","title":"Using CLI to Manage eBPF Programs","text":"<p>bpfman provides a CLI to interact with the <code>bpfman</code> Library. Find a deeper dive into CLI syntax in CLI Guide. We will load the simple <code>xdp-pass</code> program, which allows all traffic to pass through the attached interface, <code>eno3</code> in this example. The source code, xdp_pass.bpf.c, is located in the integration-test directory and there is also a prebuilt image: quay.io/bpfman-bytecode/xdp_pass:latest.</p> <pre><code>sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface eno3 --priority 100\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6213\n Map Owner ID:  None\n Map Used By:   6213\n Priority:      100\n Iface:         eno3\n Position:      0\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6213\n Name:                             pass\n Type:                             xdp\n Loaded At:                        2023-07-17T17:48:10-0400\n Tag:                              4b9d1b2c140e87ce\n GPL Compatible:                   true\n Map IDs:                          [2724]\n BTF ID:                           2834\n Size Translated (bytes):          96\n JITed:                            true\n Size JITed (bytes):               67\n Kernel Allocated Memory (bytes):  4096\n Verified Instruction Count:       9\n</code></pre> <p><code>bpfman load image</code> returns the same data as the <code>bpfman get</code> command. From the output, the Program Id of <code>6213</code> can be found in the <code>Kernel State</code> section. The Program Id can be used to perform a <code>bpfman get</code> to retrieve all relevant program data and a <code>bpfman unload</code> when the program needs to be unloaded.</p> <pre><code>sudo bpfman list\n Program ID  Name  Type  Load Time\n 6213        pass  xdp   2023-07-17T17:48:10-0400\n</code></pre> <p>We can recheck the details about the loaded program with the <code>bpfman get</code> command:</p> <pre><code>sudo bpfman get 6213\n Bpfman State\n---------------\n Name:          pass\n Image URL:     quay.io/bpfman-bytecode/xdp_pass:latest\n Pull Policy:   IfNotPresent\n Global:        None\n Metadata:      None\n Map Pin Path:  /run/bpfman/fs/maps/6213\n Map Owner ID:  None\n Map Used By:   6213\n Priority:      100\n Iface:         eno3\n Position:      0\n Proceed On:    pass, dispatcher_return\n\n Kernel State\n----------------------------------\n Program ID:                       6213\n Name:                             pass\n Type:                             xdp\n Loaded At:                        2023-07-17T17:48:10-0400\n Tag:                              4b9d1b2c140e87ce\n GPL Compatible:                   true\n Map IDs:                          [2724]\n BTF ID:                           2834\n Size Translated (bytes):          96\n JITed:                            true\n Size JITed (bytes):               67\n Kernel Allocated Memory (bytes):  4096\n Verified Instruction Count:       9\n</code></pre> <p>Then unload the program:</p> <pre><code>sudo bpfman unload 6213\n</code></pre>"},{"location":"getting-started/example-bpf/","title":"Example eBPF Programs","text":"<p>Example applications that use the <code>bpfman-go</code> bindings can be found in the examples/ directory. Current examples include:</p> <ul> <li>examples/go-kprobe-counter/</li> <li>examples/go-tc-counter/</li> <li>examples/go-tracepoint-counter/</li> <li>examples/go-uprobe-counter/<ul> <li>examples/go-target/</li> </ul> </li> <li>examples/go-xdp-counter/</li> </ul>"},{"location":"getting-started/example-bpf/#example-code-breakdown","title":"Example Code Breakdown","text":"<p>These examples and the associated documentation are intended to provide the basics on how to deploy and manage an eBPF program using bpfman. Each of the examples contain an eBPF Program written in C (kprobe_counter.c, tc_counter.c, tracepoint_counter.c uprobe_counter.c, and xdp_counter.c) that is compiled into eBPF bytecode (bpf_bpfel.o). Each time the eBPF program is called, it increments the packet and byte counts in a map that is accessible by the userspace portion.</p> <p>Each of the examples also have a userspace portion written in GO. The userspace code is leveraging the cilium/ebpf library to manage the maps shared with the eBPF program. The example eBPF programs are very similar in functionality, and only vary where in the Linux networking stack they are inserted. The userspace program then polls the eBPF map every 3 seconds and logs the current counts.</p> <p>The examples were written to either run locally on a host or run in a container in a Kubernetes deployment. The userspace code flow is slightly different depending on the deployment, so input parameters dictate the deployment method.</p>"},{"location":"getting-started/example-bpf/#examples-in-local-deployment","title":"Examples in Local Deployment","text":"<p>When run locally, the userspace program makes gRPC calls to <code>bpfman-rpc</code> requesting <code>bpfman</code> to load the eBPF program at the requested hook point (XDP hook point, TC hook point, Tracepoint, etc). Data sent in the RPC request is either defaulted or passed in via input parameters. To make the examples as simple as possible to run, all input data is defaulted (except the interface TC and XDP programs need to attach to) but can be overwritten if desired. All example programs have the following common parameters (kprobe does not have any command specific parameters):</p> <pre><code>cd bpfman/examples/go-kprobe-counter/\n\n./go-kprobe-counter --help\nUsage of ./go-kprobe-counter:\n  -crd\n        Flag to indicate all attributes should be pulled from the BpfProgram CRD.\n        Used in Kubernetes deployments and is mutually exclusive with all other\n        parameters.\n  -file string\n        File path of bytecode source. \"file\" and \"image\"/\"id\" are mutually exclusive.\n        Example: -file /home/$USER/src/bpfman/examples/go-kprobe-counter/bpf_bpfel.o\n  -id uint\n        Optional Program ID of bytecode that has already been loaded. \"id\" and\n        \"file\"/\"image\" are mutually exclusive.\n        Example: -id 28341\n  -image string\n        Image repository URL of bytecode source. \"image\" and \"file\"/\"id\" are\n        mutually exclusive.\n        Example: -image quay.io/bpfman-bytecode/go-kprobe-counter:latest\n  -map_owner_id int\n        Program Id of loaded eBPF program this eBPF program will share a map with.\n        Example: -map_owner_id 9785\n</code></pre> <p>The location of the eBPF bytecode can be provided four different ways:</p> <ul> <li>Defaulted: If nothing is passed in, the code scans the local directory for   a <code>bpf_bpfel.o</code> file. If found, that is used. If not, it errors out.</li> <li>file: Fully qualified path of the bytecode object file.</li> <li>image: Image repository URL of bytecode source.</li> <li>id: Kernel program Id of a bytecode that has already been loaded. This   program could have been loaded using <code>bpftool</code>, or <code>bpfman</code>. </li> </ul> <p>If two userspace programs need to share the same map, map_owner_id is the Program ID of the first loaded program that has the map the second program wants to share.</p> <p>The examples require <code>sudo</code> to run because they require access the Unix socket <code>bpfman-rpc</code> is listening on. Deploying Example eBPF Programs On Local Host steps through launching <code>bpfman</code> locally and running some of the examples. </p>"},{"location":"getting-started/example-bpf/#examples-in-kubernetes-deployment","title":"Examples in Kubernetes Deployment","text":"<p>When run in a Kubernetes deployment, all the input data is passed to Kubernetes through yaml files. To indicate to the userspace code that it is in a Kubernetes deployment and not to try to load the eBPF bytecode, the example is launched in the container with the crd flag. Example: <code>./go-kprobe-counter -crd</code></p> <p>For these examples, the bytecode is loaded via one yaml file which creates a *Program CRD Object (KprobeProgram, TcProgram, TracepointProgram, etc.) and the userspace pod is loaded via another yaml file. In a more realistic deployment, the userspace pod may have the logic to send the *Program CRD Object create request to the KubeAPI Server, but the two yaml files are load manually for simplicity in the example code. The examples directory contain yaml files to load each example, leveraging Kustomize to modify the yaml to load the latest images from Quay.io, to load custom images or released based images. It is recommended to use the commands built into the Makefile, which run kustomize, to apply and remove the yaml files to a Kubernetes cluster. Use <code>make help</code> to see all the make options. For example:</p> <pre><code>cd bpfman/examples/\n\n# Deploy then undeploy all the examples\nmake deploy\nmake undeploy\n\nOR\n\n# Deploy then undeploy just the TC example\nmake deploy-tc\nmake undeploy-tc\n</code></pre> <p>Deploying Example eBPF Programs On Kubernetes steps through deploying bpfman to multiple nodes in a Kubernetes cluster and loading the examples.</p>"},{"location":"getting-started/example-bpf/#building-example-code","title":"Building Example Code","text":"<p>All the examples can be built locally as well as packaged in a container for Kubernetes deployment.</p>"},{"location":"getting-started/example-bpf/#building-locally","title":"Building Locally","text":"<p>To build directly on a system, make sure all the prerequisites are met, then build.</p>"},{"location":"getting-started/example-bpf/#prerequisites","title":"Prerequisites","text":"<p>This assumes bpfman is already installed and running on the system. If not, see Setup and Building bpfman.</p> <ol> <li>All requirements defined by the <code>cilium/ebpf</code> package</li> <li> <p>libbpf development package to get the required eBPF c headers</p> <p>Fedora: <code>sudo dnf install libbpf-devel</code></p> <p>Ubuntu: <code>sudo apt-get install libbpf-dev</code></p> </li> <li> <p>Cilium's <code>bpf2go</code> binary</p> <p><code>go install github.com/cilium/ebpf/cmd/bpf2go@v0.11.0</code></p> </li> </ol>"},{"location":"getting-started/example-bpf/#build","title":"Build","text":"<p>To build all the C based eBPF counter bytecode, run:</p> <pre><code>cd bpfman/examples/\nmake generate\n</code></pre> <p>To build all the Userspace GO Client examples, run:</p> <pre><code>cd bpfman/examples/\nmake build\n</code></pre> <p>To build only a single example:</p> <pre><code>cd bpfman/examples/go-tc-counter/\ngo generate\ngo build\n</code></pre> <pre><code>cd bpfman/examples/go-tracepoint-counter/\ngo generate\ngo build\n</code></pre> <p>Other program types are the same.</p>"},{"location":"getting-started/example-bpf/#building-ebpf-bytecode-container-image","title":"Building eBPF Bytecode Container Image","text":"<p>eBPF Bytecode Image Specifications provides detailed instructions on building and shipping bytecode in a container image. Pre-built eBPF container images for the examples can be loaded from:</p> <ul> <li><code>quay.io/bpfman-bytecode/go-kprobe-counter:latest</code></li> <li><code>quay.io/bpfman-bytecode/go-tc-counter:latest</code></li> <li><code>quay.io/bpfman-bytecode/go-tracepoint-counter:latest</code></li> <li><code>quay.io/bpfman-bytecode/go-uprobe-counter:latest</code></li> <li><code>quay.io/bpfman-bytecode/go-xdp-counter:latest</code></li> </ul> <p>To build the example eBPF bytecode container images, run the build commands below (the <code>go generate</code> requires the Prerequisites described above):</p> <pre><code>cd bpfman/examples/go-xdp-counter/\ngo generate\n\ndocker build \\\n  --build-arg PROGRAM_NAME=go-xdp-counter \\\n  --build-arg BPF_FUNCTION_NAME=xdp_stats \\\n  --build-arg PROGRAM_TYPE=xdp \\\n  --build-arg BYTECODE_FILENAME=bpf_bpfel.o \\\n  --build-arg KERNEL_COMPILE_VER=$(uname -r) \\\n  -f ../../Containerfile.bytecode . -t quay.io/$USER/go-xdp-counter-bytecode:latest\n</code></pre> <p>and</p> <pre><code>cd bpfman/examples/go-tc-counter/\ngo generate\n\ndocker build \\\n  --build-arg PROGRAM_NAME=go-tc-counter \\\n  --build-arg BPF_FUNCTION_NAME=stats \\\n  --build-arg PROGRAM_TYPE=tc \\\n  --build-arg BYTECODE_FILENAME=bpf_bpfel.o \\\n  --build-arg KERNEL_COMPILE_VER=$(uname -r) \\\n  -f ../../Containerfile.bytecode . -t quay.io/$USER/go-tc-counter-bytecode:latest\n</code></pre> <p>Other program types are the same.</p> <p><code>bpfman</code> currently does not provide a method for pre-loading bytecode images (see issue #603), so push the bytecode image to a remote repository. For example:</p> <pre><code>docker login quay.io\ndocker push quay.io/$USER/go-xdp-counter-bytecode:latest\ndocker push quay.io/$USER/go-tc-counter-bytecode:latest\n</code></pre> <p>Then run with the privately built bytecode container image:</p> <pre><code>sudo ./go-tc-counter -iface ens3 -direction ingress -image quay.io/$USER/go-tc-counter-bytecode:latest\n2022/12/02 16:38:44 Using Input: Interface=ens3 Priority=50 Source=quay.io/$USER/go-tc-counter-bytecode:latest\n2022/12/02 16:38:45 Program registered with id 6225\n2022/12/02 16:38:48 4 packets received\n2022/12/02 16:38:48 580 bytes received\n\n2022/12/02 16:38:51 4 packets received\n2022/12/02 16:38:51 580 bytes received\n\n^C2022/12/02 16:38:51 Exiting...\n2022/12/02 16:38:51 Unloading Program: 6225\n</code></pre>"},{"location":"getting-started/example-bpf/#running-examples","title":"Running Examples","text":"<pre><code>cd bpfman/examples/go-xdp-counter/\nsudo ./go-xdp-counter -iface &lt;INTERNET INTERFACE NAME&gt;\n</code></pre> <p>or (NOTE: TC programs also require a direction, ingress or egress)</p> <pre><code>cd bpfman/examples/go-tc-counter/\nsudo ./go-tc-counter -direction ingress -iface &lt;INTERNET INTERFACE NAME&gt;\n</code></pre> <p>or</p> <pre><code>cd bpfman/examples/go-tracepoint-counter/\nsudo ./go-tracepoint-counter\n</code></pre> <p>bpfman can load eBPF bytecode from a container image built following the spec described in eBPF Bytecode Image Specifications.</p> <p>To use the container image, pass the URL to the userspace program:</p> <pre><code>sudo ./go-xdp-counter -iface ens3 -image quay.io/bpfman-bytecode/go-xdp-counter:latest\n2022/12/02 16:28:32 Using Input: Interface=ens3 Priority=50 Source=quay.io/bpfman-bytecode/go-xdp-counter:latest\n2022/12/02 16:28:34 Program registered with id 6223\n2022/12/02 16:28:37 4 packets received\n2022/12/02 16:28:37 580 bytes received\n\n2022/12/02 16:28:40 4 packets received\n2022/12/02 16:28:40 580 bytes received\n\n^C2022/12/02 16:28:42 Exiting...\n2022/12/02 16:28:42 Unloading Program: 6223\n</code></pre>"},{"location":"getting-started/launching-bpfman/","title":"Launching bpfman","text":"<p>The most basic way to deploy bpfman is to run it directly on a host system. First <code>bpfman</code> needs to be built and then started.</p>"},{"location":"getting-started/launching-bpfman/#build-bpfman","title":"Build bpfman","text":"<p>Perform the following steps to build <code>bpfman</code>. If this is your first time using bpfman, follow the instructions in Setup and Building bpfman to setup the prerequisites for building. To avoid installing the dependencies and having to build bpfman, consider running bpfman from a packaged release (see Run bpfman From Release Image) or installing the bpfman RPM (see Run bpfman From RPM).</p> <pre><code>cd bpfman/\ncargo build\n</code></pre>"},{"location":"getting-started/launching-bpfman/#start-bpfman-rpc","title":"Start bpfman-rpc","text":"<p>When running bpfman, the RPC Server <code>bpfman-rpc</code> can be run as a long running process or a systemd service. Examples run the same, independent of how bpfman is deployed.</p>"},{"location":"getting-started/launching-bpfman/#run-as-a-long-lived-process","title":"Run as a Long Lived Process","text":"<p>While learning and experimenting with <code>bpfman</code>, it may be useful to run <code>bpfman</code> in the foreground (which requires a second terminal to run the <code>bpfman</code> CLI commands). When run in this fashion, logs are dumped directly to the terminal. For more details on how logging is handled in bpfman, see Logging.</p> <pre><code>sudo RUST_LOG=info ./target/debug/bpfman-rpc --timeout=0\n[INFO  bpfman::utils] Has CAP_BPF: true\n[INFO  bpfman::utils] Has CAP_SYS_ADMIN: true\n[WARN  bpfman::utils] Unable to read config file, using defaults\n[INFO  bpfman_rpc::serve] Using no inactivity timer\n[INFO  bpfman_rpc::serve] Using default Unix socket\n[INFO  bpfman_rpc::serve] Listening on /run/bpfman-sock/bpfman.sock\n</code></pre> <p>When a build is run for bpfman, built binaries can be found in <code>./target/debug/</code>. So when launching <code>bpfman-rpc</code> and calling <code>bpfman</code> CLI commands, the binary must be in the $PATH or referenced directly:</p> <pre><code>sudo ./target/debug/bpfman list\n</code></pre> <p>For readability, the remaining sample commands will assume the <code>bpfman</code> CLI binary is in the $PATH, so <code>./target/debug/</code> will be dropped.</p>"},{"location":"getting-started/launching-bpfman/#run-as-a-systemd-service","title":"Run as a systemd Service","text":"<p>Run the following command to copy the <code>bpfman</code> CLI and <code>bpfman-rpc</code> binaries to <code>/usr/sbin/</code> and copy <code>bpfman.socket</code> and <code>bpfman.service</code> files to <code>/usr/lib/systemd/system/</code>. This option will also enable and start the systemd services:</p> <pre><code>sudo ./scripts/setup.sh install\n</code></pre> <p><code>bpfman</code> CLI is now in $PATH, so <code>./targer/debug/</code> is not needed:</p> <pre><code>sudo bpfman list\n</code></pre> <p>To view logs, use <code>journalctl</code>:</p> <pre><code>sudo journalctl -f -u bpfman.service -u bpfman.socket\nMar 27 09:13:54 server-calvin systemd[1]: Listening on bpfman.socket - bpfman API Socket.\n  &lt;RUN \"sudo ./go-kprobe-counter\"&gt;\nMar 27 09:15:43 server-calvin systemd[1]: Started bpfman.service - Run bpfman as a service.\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Has CAP_BPF: true\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Has CAP_SYS_ADMIN: true\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Unable to read config file, using defaults\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Using a Unix socket from systemd\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Using inactivity timer of 15 seconds\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Listening on /run/bpfman-sock/bpfman.sock\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Unable to read config file, using defaults\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Unable to read config file, using defaults\nMar 27 09:15:43 server-calvin bpfman-rpc[2548091]: Starting Cosign Verifier, downloading data from Sigstore TUF repository\nMar 27 09:15:45 server-calvin bpfman-rpc[2548091]: Loading program bytecode from file: /home/&lt;USER&gt;/src/bpfman/examples/go-kprobe-counter/bpf_bpfel.o\nMar 27 09:15:45 server-calvin bpfman-rpc[2548091]: Added probe program with name: kprobe_counter and id: 7568\nMar 27 09:15:48 server-calvin bpfman-rpc[2548091]: Unable to read config file, using defaults\nMar 27 09:15:48 server-calvin bpfman-rpc[2548091]: Removing program with id: 7568\nMar 27 09:15:58 server-calvin bpfman-rpc[2548091]: Shutdown Unix Handler /run/bpfman-sock/bpfman.sock\nMar 27 09:15:58 server-calvin systemd[1]: bpfman.service: Deactivated successfully.\n</code></pre>"},{"location":"getting-started/launching-bpfman/#additional-notes","title":"Additional Notes","text":"<p>To update the configuration settings associated with running <code>bpfman</code> as a service, edit the service configuration files:</p> <pre><code>sudo vi /usr/lib/systemd/system/bpfman.socket\nsudo vi /usr/lib/systemd/system/bpfman.service\nsudo systemctl daemon-reload\n</code></pre> <p>If <code>bpfman</code> CLI or <code>bpfman-rpc</code> is rebuilt, the following command can be run to install the update binaries without tearing down <code>bpfman</code>. The services are automatically restarted.</p> <pre><code>sudo ./scripts/setup.sh reinstall\n</code></pre> <p>To unwind all the changes, stop <code>bpfman</code> and remove all related files from the system, run the following script:</p> <pre><code>sudo ./scripts/setup.sh uninstall\n</code></pre>"},{"location":"getting-started/launching-bpfman/#preferred-method-to-start-bpfman","title":"Preferred Method to Start bpfman","text":"<p>In order to call into the <code>bpfman</code> Library, the calling process must be privileged. In order to load and unload eBPF, the kernel requires a set of powerful capabilities. Long lived privileged processes are more vulnerable to attack than short lived processes. When <code>bpfman-rpc</code> is run as a systemd service, it is leveraging socket activation. This means that it loads a <code>bpfman.socket</code> and <code>bpfman.service</code> file. The socket service is the long lived process, which doesn't have any special permissions. The service that runs <code>bpfman-rpc</code> is only started when there is a request on the socket, and then <code>bpfman-rpc</code> stops itself after an inactivity timeout.</p> <p>For security reasons, it is recommended to run <code>bpfman-rpc</code> as a systemd service when running on a local host. For local development, some may find it useful to run <code>bpfman-rpc</code> as a long lived process.</p> <p>When run as a systemd service, the set of linux capabilities are limited to only the required set. If permission errors are encountered, see Linux Capabilities for help debugging.</p>"},{"location":"getting-started/overview/","title":"bpfman Overview","text":"<p>Core bpfman is a library written in Rust and published as a Crate via crates.io. The <code>bpfman</code> library leverages the <code>aya</code> library to manage eBPF programs. Applications written in Rust can import the <code>bpfman</code> library and call the bpfman APIs directly. An example of a Rust based application leveraging the <code>bpfman</code> library is the <code>bpfman</code> CLI, which is a Rust based binary used to provision bpfman from a Linux command prompt (see CLI Guide).</p> <p>For applications written in other languages, bpfman provides <code>bpfman-rpc</code>, a Rust based bpfman RPC server binary. Non-Rust applications can send a RPC message to the server, which translate the RPC request into a bpfman library call. The long term solution is to leverage the Rust Foreign Function Interface (FFI) feature, which enables a different (foreign) programming language to call Rust functions, but that is not supported at the moment.</p> <p></p> <p>The <code>bpfman-rpc</code> server can run in one of two modes. It can be run as a long running process or as a systemd service that uses socket activation to start <code>bpfman-rpc</code> only when there is a RPC message to process. More details are provided in Deploying Example eBPF Programs On Local Host.</p> <p>When deploying <code>bpfman</code> in a Kubernetes deployment, <code>bpfman-agent</code>, <code>bpfman-rpc</code>, and the <code>bpfman</code> library are packaged in a container. When the container starts, <code>bpfman-rpc</code> is started as a long running process. <code>bpfman-agent</code> listens to the KubeAPI Server and send RPC requests to <code>bpfman-rpc</code>, which in turn calls the <code>bpfman</code> library to manage eBPF programs on a given node.</p> <p></p> <p>More details provided in Deploying Example eBPF Programs On Kubernetes.</p>"},{"location":"getting-started/running-release/","title":"Run bpfman From Release Image","text":"<p>This section describes how to deploy <code>bpfman</code> from a given release. See Releases for the set of bpfman releases.</p> <p>Note: Instructions for interacting with bpfman change from release to release, so reference release specific documentation. For example:</p> <p>https://bpfman.io/v0.4.0/getting-started/running-release/</p> <p>Jump to the Setup and Building bpfman section for help building from the latest code or building from a release branch.</p> <p>Start bpfman-rpc contains more details on the different modes to run <code>bpfman</code> in on the host. Use Run using an rpm for deploying a released version of <code>bpfman</code> from an rpm as a systemd service and then use Deploying Example eBPF Programs On Local Host for further information on how to test and interact with <code>bpfman</code>.</p> <p>Deploying the bpfman-operator contains more details on deploying <code>bpfman</code> in a Kubernetes deployment and Deploying Example eBPF Programs On Kubernetes contains more details on interacting with <code>bpfman</code> running in a Kubernetes deployment. Use Deploying Release Version of the bpfman-operator below for deploying released version of <code>bpfman</code> in Kubernetes and then use the links above for further information on how to test and interact with <code>bpfman</code>.</p>"},{"location":"getting-started/running-release/#run-as-a-long-lived-process","title":"Run as a Long Lived Process","text":"<pre><code>export BPFMAN_REL=0.4.0\nmkdir -p $HOME/src/bpfman-${BPFMAN_REL}/; cd $HOME/src/bpfman-${BPFMAN_REL}/\nwget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfman-linux-x86_64.tar.gz\ntar -xzvf bpfman-linux-x86_64.tar.gz; rm bpfman-linux-x86_64.tar.gz\n\n$ tree\n.\n\u251c\u2500\u2500 bpf-log-exporter\n\u251c\u2500\u2500 bpfman\n\u251c\u2500\u2500 bpfman-ns\n\u251c\u2500\u2500 bpfman-rpc\n\u2514\u2500\u2500 bpf-metrics-exporter\n</code></pre> <p>To deploy <code>bpfman-rpc</code>:</p> <pre><code>sudo RUST_LOG=info ./bpfman-rpc --timeout=0\n[INFO  bpfman::utils] Has CAP_BPF: true\n[INFO  bpfman::utils] Has CAP_SYS_ADMIN: true\n[WARN  bpfman::utils] Unable to read config file, using defaults\n[INFO  bpfman_rpc::serve] Using no inactivity timer\n[INFO  bpfman_rpc::serve] Using default Unix socket\n[INFO  bpfman_rpc::serve] Listening on /run/bpfman-sock/bpfman.sock\n:\n</code></pre> <p>To use the CLI:</p> <pre><code>sudo ./bpfman list\n Program ID  Name  Type  Load Time\n</code></pre> <p>Continue in Deploying Example eBPF Programs On Local Host if desired.</p>"},{"location":"getting-started/running-release/#deploying-release-version-of-the-bpfman-operator","title":"Deploying Release Version of the bpfman-operator","text":"<p>The quickest solution for running <code>bpfman</code> in a Kubernetes deployment is to run a Kubernetes KIND Cluster:</p> <pre><code>kind create cluster --name=test-bpfman\n</code></pre> <p>Next, deploy the bpfman CRDs:</p> <pre><code>export BPFMAN_REL=0.4.0\nkubectl apply -f  https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfman-crds-install.yaml\n</code></pre> <p>Next, deploy the <code>bpfman-operator</code>, which will also deploy the <code>bpfman-daemon</code>, which contains <code>bpfman-rpc</code>, <code>bpfman</code> Library and <code>bpfman-agent</code>:</p> <pre><code>kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfman-operator-install-v${BPFMAN_REL}.yaml\n</code></pre> <p>Finally, deploy an example eBPF program.</p> <pre><code>kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/go-xdp-counter-install-v${BPFMAN_REL}.yaml\n</code></pre> <p>There are other example programs in the Releases page.</p> <p>Continue in Deploying the bpfman-operator or Deploying Example eBPF Programs On Kubernetes if desired. Keep in mind that prior to v0.4.0, <code>bpfman</code> was released as <code>bpfd</code>. So follow the release specific documentation.</p> <p>Use the following command to teardown the cluster:</p> <pre><code>kind delete cluster -n test-bpfman\n</code></pre>"},{"location":"getting-started/running-rpm/","title":"Run bpfman From RPM","text":"<p>This section describes how to deploy <code>bpfman</code> from an RPM. RPMs are generated each time a Pull Request is merged in github for Fedora 38, 39 and Rawhide (see Install Prebuilt RPM below). RPMs can also be built locally from a Fedora server (see Build RPM Locally below).</p>"},{"location":"getting-started/running-rpm/#install-prebuilt-rpm","title":"Install Prebuilt RPM","text":"<p>This section describes how to install an RPM built automatically by the Packit Service. The Packit Service builds RPMs for each Pull Request merged.</p>"},{"location":"getting-started/running-rpm/#packit-service-prerequisites","title":"Packit Service Prerequisites","text":"<p>To install an RPM generated by the Packit Service, the following packages need to be installed:</p> <p><code>dnf</code> based OS:</p> <pre><code>sudo dnf install -y dnf-plugins-core\nsudo dnf copr enable @ebpf-sig/bpfman-next\n</code></pre>"},{"location":"getting-started/running-rpm/#install-rpm-from-packit-service","title":"Install RPM From Packit Service","text":"<p>To load an RPM from a specific commit, find the commit from bpfman commits, and click on the green check showing a given Pull Request was verified. At the bottom of the list of checks are the RPM builds, click on the <code>details</code>, and follow the Packit Dashboard link to the <code>Copr Build Results</code>. Then install the given RPM:</p> <pre><code>sudo dnf install -y bpfman-0.4.0~dev-1.20240117143006587102.main.191.gda44a71.fc38.x86_64\n</code></pre> <p><code>bpfman</code> is now installed but not running. To start <code>bpfman</code>:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable bpfman.socket\nsudo systemctl start bpfman.socket\n</code></pre> <p>Verify <code>bpfman</code> is installed and running:</p> <pre><code>$ sudo systemctl status bpfman.socket\n\u25cf bpfman.socket - bpfman API Socket\n     Loaded: loaded (/usr/lib/systemd/system/bpfman.socket; enabled; preset: disabled)\n     Active: active (listening) since Thu 2024-01-18 21:19:29 EST; 5s ago\n   Triggers: \u25cf bpfman.service\n     Listen: /run/bpfman-sock/bpfman.sock (Stream)\n     CGroup: /system.slice/bpfman.socket\n:\n\n$ sudo systemctl status bpfman.service\n\u25cb bpfman.service - Run bpfman as a service\n     Loaded: loaded (/usr/lib/systemd/system/bpfman.service; static)\n    Drop-In: /usr/lib/systemd/system/service.d\n             \u2514\u250010-timeout-abort.conf\n     Active: inactive (dead)\nTriggeredBy: \u25cf bpfman.socket\n:\n\n$ sudo bpfman list\n Program ID  Name  Type  Load Time\n</code></pre>"},{"location":"getting-started/running-rpm/#uninstall-given-rpm","title":"Uninstall Given RPM","text":"<p>To determine the RPM that is currently loaded:</p> <pre><code>$ sudo rpm -qa | grep bpfman\nbpfman-0.4.0~dev-1.20240117143006587102.main.191.gda44a71.fc39.x86_64\n</code></pre> <p>To uninstall the RPM:</p> <pre><code>sudo dnf erase -y bpfman-0.4.0~dev-1.20240117143006587102.main.191.gda44a71.fc39.x86_64\n\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"getting-started/running-rpm/#build-rpm-locally","title":"Build RPM Locally","text":"<p>This section describes how to build and install an RPM locally.</p>"},{"location":"getting-started/running-rpm/#local-build-prerequisites","title":"Local Build Prerequisites","text":"<p>To build locally, the following packages need to be installed:</p> <p><code>dnf</code> based OS:</p> <pre><code>sudo dnf install packit\nsudo dnf install cargo-rpm-macros\n</code></pre> <p>NOTE: <code>cargo-rpm-macros</code> needs to be version 25 or higher. It appears this is only available on Fedora 37, 38, 39 and Rawhide at the moment.</p>"},{"location":"getting-started/running-rpm/#build-locally","title":"Build Locally","text":"<p>To build locally, run the following command:</p> <pre><code>packit build locally\n</code></pre> <p>This will generate several RPMs in a <code>x86_64/</code> directory:</p> <pre><code>$ ls x86_64/\nbpfman-0.4.0~dev-1.20240118212420167308.&lt;USERNAME&gt;.rpm.socket.192.gb2ea1b9.fc39.x86_64.rpm\nbpfman-debuginfo-0.4.0~dev-1.20240118212420167308.&lt;USERNAME&gt;.rpm.socket.192.gb2ea1b9.fc39.x86_64.rpm\nbpfman-debugsource-0.4.0~dev-1.20240118212420167308.&lt;USERNAME&gt;.rpm.socket.192.gb2ea1b9.fc39.x86_64.rpm\n</code></pre>"},{"location":"getting-started/running-rpm/#install-local-build","title":"Install Local Build","text":"<p>Install the RPM:</p> <pre><code>sudo rpm -i x86_64/bpfman-0.4.0~dev-1.20240118212420167308.&lt;USERNAME&gt;.rpm.socket.192.gb2ea1b9.fc39.x86_64.rpm\n</code></pre> <p><code>bpfman</code> is now installed but not running. To start <code>bpfman</code>:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable bpfman.socket\nsudo systemctl start bpfman.socket\n</code></pre> <p>Verify <code>bpfman</code> is installed and running:</p> <pre><code>$ sudo systemctl status bpfman.socket\n\u25cf bpfman.socket - bpfman API Socket\n     Loaded: loaded (/usr/lib/systemd/system/bpfman.socket; enabled; preset: disabled)\n     Active: active (listening) since Thu 2024-01-18 21:19:29 EST; 5s ago\n   Triggers: \u25cf bpfman.service\n     Listen: /run/bpfman-sock/bpfman.sock (Stream)\n     CGroup: /system.slice/bpfman.socket\n:\n\n$ sudo systemctl status bpfman.service\n\u25cb bpfman.service - Run bpfman as a service\n     Loaded: loaded (/usr/lib/systemd/system/bpfman.service; static)\n    Drop-In: /usr/lib/systemd/system/service.d\n             \u2514\u250010-timeout-abort.conf\n     Active: inactive (dead)\nTriggeredBy: \u25cf bpfman.socket\n:\n\n$ sudo bpfman list\n Program ID  Name  Type  Load Time\n</code></pre>"},{"location":"getting-started/running-rpm/#uninstall-local-build","title":"Uninstall Local Build","text":"<p>To determine the RPM that is currently loaded:</p> <pre><code>$ sudo rpm -qa | grep bpfman\nbpfman-0.4.0~dev-1.20240118212420167308.&lt;USERNAME&gt;.rpm.socket.192.gb2ea1b9.fc39.x86_64\n</code></pre> <p>To uninstall the RPM:</p> <pre><code>sudo rpm -e bpfman-0.4.0~dev-1.20240118212420167308.&lt;USERNAME&gt;.rpm.socket.192.gb2ea1b9.fc39.x86_64\n\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"getting-started/troubleshooting/","title":"Troubleshooting","text":"<p>This section provides a list of common issues and solutions when working with <code>bpfman</code>.</p>"},{"location":"getting-started/troubleshooting/#xdp","title":"XDP","text":""},{"location":"getting-started/troubleshooting/#xdp-program-fails-to-load","title":"XDP Program Fails to Load","text":"<p>When attempting to load an XDP program and the program fails to load:</p> <pre><code>$ sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface veth92cd99b --priority 100\nError: status: Aborted, message: \"An error occurred. dispatcher attach failed on interface veth92cd99b: `bpf_link_create` failed\", details: [], metadata: MetadataMap { headers: {\"content-type\": \"application/grpc\", \"date\": \"Tue, 28 Nov 2023 13:37:02 GMT\", \"content-length\": \"0\"} }\n</code></pre> <p>The log may look something like this:</p> <pre><code>Nov 28 08:36:58 ebpf03 bpfman[2081732]: The bytecode image: quay.io/bpfman-bytecode/xdp_pass:latest is signed\nNov 28 08:36:59 ebpf03 bpfman[2081732]: Loading program bytecode from container image: quay.io/bpfman-bytecode/xdp_pass:latest\nNov 28 08:37:01 ebpf03 bpfman[2081732]: The bytecode image: quay.io/bpfman/xdp-dispatcher:v2 is signed\nNov 28 08:37:02 ebpf03 bpfman[2081732]: BPFMAN load error: Error(\n                                            \"dispatcher attach failed on interface veth92cd99b: `bpf_link_create` failed\",\n                                        )\n</code></pre> <p>The issue may be the there is already an external XDP program loaded on the given interface. bpfman allows multiple XDP programs on an interface by loading a <code>dispatcher</code> program which is the XDP program and additional programs are loaded as extensions to the <code>dispatcher</code>. Use <code>bpftool</code> to determine if any programs are already loaded on an interface:</p> <pre><code>$ sudo bpftool net list dev veth92cd99b\nxdp:\nveth92cd99b(32) generic id 8733\n\ntc:\nveth92cd99b(32) clsact/ingress tc_dispatcher id 8922\n\nflow_dissector:\n</code></pre>"},{"location":"governance/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"governance/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"governance/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"governance/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement directly. Maintainers are identified in the MAINTAINERS.md file and their contact information is on their GitHub profile page. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"governance/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"governance/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"governance/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"governance/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"governance/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"governance/CONTRIBUTING/","title":"Contributing Guide","text":"<ul> <li>Ways to Contribute</li> <li>Find an Issue</li> <li>Ask for Help</li> <li>Pull Request Lifecycle</li> <li>Development Environment Setup</li> <li>Signoff Your Commits</li> <li>Pull Request Checklist</li> </ul> <p>Welcome! We are glad that you want to contribute to our project! \ud83d\udc96</p> <p>As you get started, you are in the best position to give us feedback on areas of our project that we need help with including:</p> <ul> <li>Problems found during setting up a new developer environment</li> <li>Gaps in our Quickstart Guide or documentation</li> <li>Bugs in our automation scripts</li> </ul> <p>If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!</p>"},{"location":"governance/CONTRIBUTING/#ways-to-contribute","title":"Ways to Contribute","text":"<p>We welcome many different types of contributions including:</p> <ul> <li>New features</li> <li>Builds, CI/CD</li> <li>Bug fixes</li> <li>Documentation</li> <li>Issue Triage</li> <li>Answering questions on Slack/Mailing List</li> <li>Web design</li> <li>Communications / Social Media / Blog Posts</li> <li>Release management</li> </ul> <p>Not everything happens through a GitHub pull request. Please come to our meetings or contact us and let's discuss how we can work together.</p>"},{"location":"governance/CONTRIBUTING/#come-to-meetings","title":"Come to Meetings","text":"<p>Absolutely everyone is welcome to come to any of our meetings. You never need an invite to join us. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough!</p> <p>You can find out more about our meetings here. You don\u2019t have to turn on your video. The first time you come, introducing yourself is more than enough. Over time, we hope that you feel comfortable voicing your opinions, giving feedback on others\u2019 ideas, and even sharing your own ideas, and experiences.</p>"},{"location":"governance/CONTRIBUTING/#find-an-issue","title":"Find an Issue","text":"<p>We have good first issues for new contributors and help wanted issues suitable for any contributor. good first issue has extra information to help you make your first contribution. help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request.</p> <p>Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us on Slack and we will be happy to help.</p> <p>Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine.</p>"},{"location":"governance/CONTRIBUTING/#ask-for-help","title":"Ask for Help","text":"<p>The best way to reach us with a question when contributing is to ask on:</p> <ul> <li>The original github issue</li> <li>Our Slack channel</li> </ul>"},{"location":"governance/CONTRIBUTING/#pull-request-lifecycle","title":"Pull Request Lifecycle","text":"<p>Pull requests are managed by Mergify.</p> <p>Our process is currently as follows:</p> <ol> <li>When you open a PR a maintainer will automatically be assigned for review</li> <li>Make sure that your PR is passing CI - if you need help with failing checks please feel free to ask!</li> <li>Once it is passing all CI checks, a maintainer will review your PR and you may be asked to make changes.</li> <li>When you have received at least one approval from a maintainer, your PR will be merged automatically.</li> </ol> <p>In some cases, other changes may conflict with your PR. If this happens, you will get notified by a comment in the issue that your PR requires a rebase, and the <code>needs-rebase</code> label will be applied. Once a rebase has been performed, this label will be automatically removed.</p>"},{"location":"governance/CONTRIBUTING/#development-environment-setup","title":"Development Environment Setup","text":"<p>See Setup and Building bpfman</p>"},{"location":"governance/CONTRIBUTING/#signoff-your-commits","title":"Signoff Your Commits","text":""},{"location":"governance/CONTRIBUTING/#dco","title":"DCO","text":"<p>Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project.</p> <p>You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit.</p> <pre><code>This is my commit message\n\nSigned-off-by: Your Name &lt;your.name@example.com&gt;\n</code></pre> <p>Git has a <code>-s</code> command line option to do this automatically:</p> <pre><code>git commit -s -m 'This is my commit message'\n</code></pre> <p>If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running</p> <pre><code>git commit --amend -s\n</code></pre>"},{"location":"governance/CONTRIBUTING/#logical-grouping-of-commits","title":"Logical Grouping of Commits","text":"<p>It is a recommended best practice to keep your changes as logically grouped as possible within individual commits. If while you're developing you prefer doing a number of commits that are \"checkpoints\" and don't represent a single logical change, please squash those together before asking for a review. When addressing review comments, please perform an interactive rebase and edit commits directly rather than adding new commits with messages like \"Fix review comments\".</p>"},{"location":"governance/CONTRIBUTING/#commit-message-guidelines","title":"Commit message guidelines","text":"<p>A good commit message should describe what changed and why.</p> <ol> <li> <p>The first line should:</p> </li> <li> <p>contain a short description of the change (preferably 50 characters or less,     and no more than 72 characters)</p> </li> <li>be entirely in lowercase with the exception of proper nouns, acronyms, and     the words that refer to code, like function/variable names</li> <li>be prefixed with the name of the sub crate being changed</li> </ol> <p>Examples:</p> <ul> <li>bpfman: validate program section names</li> <li> <p>bpf: add dispatcher program test slot</p> </li> <li> <p>Keep the second line blank.</p> </li> <li>Wrap all other lines at 72 columns (except for long URLs).</li> <li>If your patch fixes an open issue, you can add a reference to it at the end    of the log. Use the <code>Fixes: #</code> prefix and the issue number. For other    references use <code>Refs: #</code>. <code>Refs</code> may include multiple issues, separated by a    comma.</li> </ul> <p>Examples:</p> <ul> <li><code>Fixes: #1337</code></li> <li><code>Refs: #1234</code></li> </ul> <p>Sample complete commit message:</p> <pre><code>subcrate: explain the commit in one line\n\nBody of commit message is a few lines of text, explaining things\nin more detail, possibly giving some background about the issue\nbeing fixed, etc.\n\nThe body of the commit message can be several paragraphs, and\nplease do proper word-wrap and keep columns shorter than about\n72 characters or so. That way, `git log` will show things\nnicely even when it is indented.\n\nFixes: #1337\nRefs: #453, #154\n</code></pre>"},{"location":"governance/CONTRIBUTING/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>When you submit your pull request, or you push new commits to it, our automated systems will run some checks on your new code. We require that your pull request passes these checks, but we also have more criteria than just that before we can accept and merge it. We recommend that you check the following things locally before you submit your code:</p> <ul> <li>Verify that Rust code has been formatted and that all clippy lints have been fixed:</li> <li>Verify that Go code has been formatted and linted</li> <li>Verify that Yaml files have been formatted (see   Install Yaml Formatter)</li> <li> <p>Verify that Bash scripts have been linted using <code>shellcheck</code></p> <pre><code>cd bpfman/\ncargo xtask lint\n</code></pre> </li> <li> <p>Verify that unit tests are passing locally (see   Unit Testing):</p> <pre><code>cd bpfman/\ncargo xtask unit-test\n</code></pre> </li> <li> <p>Verify any changes to the bpfman API have been \"blessed\".   After running the below command, any changes to any of the files in   <code>bpfman/xtask/public-api/*.txt</code> indicate changes to the bpfman API.   Verify that these changes were intentional.   CI uses the latest nightly Rust toolchain, so make sure the public-apis   are verified against latest.</p> <pre><code>cd bpfman/\nrustup update nightly\ncargo +nightly xtask public-api --bless\n</code></pre> </li> <li> <p>Verify that integration tests are passing locally (see   Basic Integration Tests):</p> <pre><code>cd bpfman/\ncargo xtask integration-test\n</code></pre> </li> <li> <p>If developing the bpfman-operator, verify that bpfman-operator unit and integration tests   are passing locally:</p> <p>See Kubernetes Operator Tests.</p> </li> </ul>"},{"location":"governance/GOVERNANCE/","title":"bpfman Project Governance","text":"<p>The bpfman project is dedicated to creating an easy way to run eBPF programs on a single host and in clusters. This governance explains how the project is run.</p> <ul> <li>Values</li> <li>Maintainers</li> <li>Becoming a Maintainer</li> <li>Meetings</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifications</li> </ul>"},{"location":"governance/GOVERNANCE/#values","title":"Values","text":"<p>The bpfman project and its leadership embrace the following values:</p> <ul> <li> <p>Openness: Communication and decision-making happens in the open and is discoverable for future   reference. As much as possible, all discussions and work take place in public   forums and open repositories.</p> </li> <li> <p>Fairness: All stakeholders have the opportunity to provide feedback and submit   contributions, which will be considered on their merits.</p> </li> <li> <p>Community over Product or Company: Sustaining and growing our community takes   priority over shipping code or sponsors' organizational goals.  Each   contributor participates in the project as an individual.</p> </li> <li> <p>Inclusivity: We innovate through different perspectives and skill sets, which   can only be accomplished in a welcoming and respectful environment.</p> </li> <li> <p>Participation: Responsibilities within the project are earned through   participation, and there is a clear path up the contributor ladder into leadership   positions.</p> </li> </ul>"},{"location":"governance/GOVERNANCE/#maintainers","title":"Maintainers","text":"<p>bpfman Maintainers have write access to the project GitHub repository. They can merge their patches or patches from others. The list of current maintainers can be found at MAINTAINERS.md.  Maintainers collectively manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the bpfman project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which is the governing body for the project.</p>"},{"location":"governance/GOVERNANCE/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:</li> <li>participate in discussions, contributions, code and documentation reviews, for 6 months or more,</li> <li>perform reviews for 10 non-trivial pull requests,</li> <li>contribute 10 non-trivial pull requests and have them merged,</li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.</li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by opening a Pull Request on GitHub to update the MAINTAINERS.md file. A simple majority vote of existing Maintainers approves the application. Maintainer nominations will be evaluated without prejudice to employers or demographics.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights.</p>"},{"location":"governance/GOVERNANCE/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failing to fulfill their Maintainer responsibilities, violating the Code of Conduct, or for other reasons. Inactivity is defined as a period of very low or no activity in the project for a year or more, with no definite schedule to return to full Maintainer activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus status. Emeritus Maintainers will still be consulted on some project matters and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"governance/GOVERNANCE/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public developer meeting, detailed in the meetings document.</p> <p>Maintainers will also have closed meetings to discuss security reports or Code of Conduct violations. Such meetings should be scheduled by any Maintainer on receipt of a security issue or CoC report. All current Maintainers must be invited to such closed meetings, except for any Maintainer who is accused of a CoC violation.</p>"},{"location":"governance/GOVERNANCE/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private maintainer Slack channel.</p>"},{"location":"governance/GOVERNANCE/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves.  If this responsibility is delegated, the Maintainers will appoint a team of at least two contributors to handle it.  The Maintainers will review who is assigned to this at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security holes and breaches according to the security policy.</p>"},{"location":"governance/GOVERNANCE/#voting","title":"Voting","text":"<p>While most business in bpfman is conducted by \"lazy consensus\", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the private developer slack channel for security or conduct matters. Votes may also be taken at the developer meeting.  Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed, except where otherwise noted.  Two-thirds majority votes mean at least two-thirds of all existing maintainers.</p>"},{"location":"governance/GOVERNANCE/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by a 2/3 vote of the Maintainers.</p>"},{"location":"governance/MAINTAINERS/","title":"Maintainers","text":"<p>See CONTRIBUTING.md for general contribution guidelines. See GOVERNANCE.md for governance guidelines and maintainer responsibilities. See CODEOWNERS for a detailed list of owners for the various source directories.</p> Name Employer Responsibilities Dave Tucker Red Hat Catch all Andrew Stoycos Red Hat bpfman-operator, bpfman-agent Andre Fredette Red Hat All things tc-bpf Billy McFall Red Hat All things systemd"},{"location":"governance/MEETINGS/","title":"bpfman Community Meetings","text":""},{"location":"governance/MEETINGS/#meeting-time","title":"Meeting time","text":"<p>We meet every Thursday at 10:00 AM Eastern Time. The meetings last up to 1 hour.</p>"},{"location":"governance/MEETINGS/#meeting-location","title":"Meeting location","text":"<p>Video call link: https://meet.google.com/ggz-zkmp-pxx Or dial: (US) +1 98ttp4-221-0859 PIN: 613 588 790# More phone numbers: https://tel.meet/ggz-zkmp-pxx?pin=3270510926446</p>"},{"location":"governance/MEETINGS/#meeting-agenda-and-minutes","title":"Meeting agenda and minutes","text":"<p>Meeting agenda</p>"},{"location":"governance/REVIEWING/","title":"Reviewing Guide","text":"<p>This document covers who may review pull requests for this project, and guides how to perform code reviews that meet our community standards and code of conduct. All reviewers must read this document and agree to follow the project review guidelines. Reviewers who do not follow these guidelines may have their privileges revoked.</p>"},{"location":"governance/REVIEWING/#the-reviewer-role","title":"The Reviewer Role","text":"<p>Only maintainers are REQUIRED to review pull requests. Other contributors may opt to review pull requests, but any LGTM from a non-maintainer won't count towards the required number of Approved Reviews in the Mergify policy.</p>"},{"location":"governance/REVIEWING/#values","title":"Values","text":"<p>All reviewers must abide by the Code of Conduct and are also protected by it. A reviewer should not tolerate poor behavior and is encouraged to report any behavior that violates the Code of Conduct. All of our values listed above are distilled from our Code of Conduct.</p> <p>Below are concrete examples of how it applies to code review specifically:</p>"},{"location":"governance/REVIEWING/#inclusion","title":"Inclusion","text":"<p>Be welcoming and inclusive. You should proactively ensure that the author is successful. While any particular pull request may not ultimately be merged, overall we want people to have a great experience and be willing to contribute again. Answer the questions they didn't know to ask or offer concrete help when they appear stuck.</p>"},{"location":"governance/REVIEWING/#sustainability","title":"Sustainability","text":"<p>Avoid burnout by enforcing healthy boundaries. Here are some examples of how a reviewer is encouraged to act to take care of themselves:</p> <ul> <li>Authors should meet baseline expectations when submitting a pull request, such as writing tests.</li> <li>If your availability changes, you can step down from a pull request and have someone else assigned.</li> <li>If interactions with an author are not following the code of conduct, close the PR and raise it with your Code of Conduct committee or point of contact. It's not your job to coax people into behaving.</li> </ul>"},{"location":"governance/REVIEWING/#trust","title":"Trust","text":"<p>Be trustworthy. During a review, your actions both build and help maintain the trust that the community has placed in this project. Below are examples of ways that we build trust:</p> <ul> <li>Transparency - If a pull request won't be merged, clearly say why and close it. If a pull request won't be reviewed for a while, let the author know so they can set expectations and understand why it's blocked.</li> <li>Integrity - Put the project's best interests ahead of personal relationships or company affiliations when deciding if a change should be merged.</li> <li>Stability - Only merge when the change won't negatively impact project stability. It can be tempting to merge a pull request that doesn't meet our quality standards, for example when the review has been delayed, or because we are trying to deliver new features quickly, but regressions can significantly hurt trust in our project.</li> </ul>"},{"location":"governance/REVIEWING/#process","title":"Process","text":"<ul> <li>Reviewers are automatically assigned based on the CODEOWNERS file.</li> <li>Reviewers should wait for automated checks to pass before reviewing</li> <li>At least 1 approved review is required from a maintainer before a pull request can be merged</li> <li>All CI checks must pass</li> <li>If a PR is stuck for some reason it is down to the reviewer to determine the best course of action:</li> <li>PRs may be closed if they are no longer relevant</li> <li>A maintainer may choose to carry a PR forward on their own, but they should ALWAYS include the original author's commits</li> <li>A maintainer may choose to open additional PRs to help lay a foundation on which the stuck PR can be unstuck. They may either rebase the stuck PR themselves or leave this to the author</li> <li>Maintainers should not merge their pull requests without a review</li> <li>Maintainers should let the Mergify bot merge PRs and not merge PRs directly</li> <li>In times of need, i.e. to fix pressing security issues, the Maintainers may, at their discretion, merge PRs without review. They should at least add a comment to the PR explaining why they did so.</li> </ul>"},{"location":"governance/REVIEWING/#checklist","title":"Checklist","text":"<p>Below are a set of common questions that apply to all pull requests:</p> <ul> <li>[ ] Is this PR targeting the correct branch?</li> <li>[ ] Does the commit message provide an adequate description of the change?</li> <li>[ ] Does the affected code have corresponding tests?</li> <li>[ ] Are the changes documented, not just with inline documentation, but also with conceptual documentation such as an overview of a new feature, or task-based documentation like a tutorial? Consider if this change should be announced on your project blog.</li> <li>[ ] Does this introduce breaking changes that would require an announcement or bumping of the major version?</li> <li>[ ] Does this PR introduce any new dependencies?</li> </ul>"},{"location":"governance/REVIEWING/#reading-list","title":"Reading List","text":"<p>Reviewers are encouraged to read the following articles for help with common reviewer tasks:</p> <ul> <li>The Art of Closing: How to close an unfinished or rejected pull request</li> <li>Kindness and Code Reviews: Improving the Way We Give Feedback</li> <li>Code Review Guidelines for Humans: Examples of good and back feedback</li> </ul>"},{"location":"governance/SECURITY/","title":"Security Policy","text":""},{"location":"governance/SECURITY/#supported-versions","title":"Supported Versions","text":"<p>No released versions of bpfman and bpfman-agent or bpfman-operator will receive regular security updates until a mainline release has been performed. A reported and fixed vulnerability will be included in the next minor release, which depending on the severity of the vulnerability may be immediate.</p>"},{"location":"governance/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>To report a vulnerability, please use the Private Vulnerability Reporting Feature on GitHub. We will endevour to respond within 48hrs of reporting. If a vulnerability is reported but considered low priority it may be converted into an issue and handled on the public issue tracker. Should a vulnerability be considered severe we will endeavour to patch it within 48hrs of acceptance, and may ask for you to collaborate with us on a temporary private fork of the repository.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/community-meeting/","title":"Community Meeting","text":""},{"location":"blog/category/2024/","title":"2024","text":""}]}